---
title: "Einführung & Überblick"
title-slide-attributes:
  # data-background-image: ../../background_title-dimmed_red_light.png
  data-background-color: "#04316a"
subtitle: "Digital behavioral data"
author: 
  - name: Christoph Adrian 
    url: https://twitter.com/chrdrn
    affiliation: Lehrstuhl für Kommunikationswissenschaft
    affiliation-url: https://www.kowi.rw.fau.de/person/christoph-adrian/
date: 03 11 2022
date-format: "DD.MM.YYYY"
format:
  revealjs:
    theme: ../../slidetheme.scss
    template-partials:
      - title-slide.html
    slide-number: true
    chalkboard:
      buttons: false
    preview-links: auto
    logo: ../../logo.png
    footer: "[Digital Behavioral Data](https://chrdrn.github.io/digital-behavioral-data/)"
comments:
  hypothesis: 
    theme: clean
filters:
  - roughnotation
execute:
  echo: true
csl: ../../apa.csl
---

## Seminarplan {.smaller}

| Sitzung             | Datum                        | Thema                                           | Referent\*Innen                                    |
|------------------|------------------|------------------|-------------------|
| ~~1~~               | ~~26.10.2022~~               | ~~Kick-Off Session~~                            | ~~Christoph Adrian~~                               |
| [**2**]{.smallcaps} | [**02.11.2022**]{.smallcaps} | [**DBD: Einführung und Überblick**]{.smallcaps} | [**Christoph Adrian**]{.smallcaps}                 |
| 3                   | 09.11.2022                   | DBD: Datenerhebung                              | Christoph Adrian                                   |
| 4                   | 16.11.2022                   | API-Access (I): *Twitter*                       | Falk                                               |
| 5                   | 23.11.2022                   | API-Access (II): *YouTube*                      | Denisov                                            |
| 6                   | 30.11.2022                   | API-Access (II): *Reddit*                       | Landauer                                           |
| 7                   | 07.12.2022                   | Webscraping: *TikTok*                           | Brand & Kocher                                     |
| 8                   | 14.12.2022                   | ESM: *m-path*                                   | Dörr                                               |
|                     |                              | *WEIHNACHTSPAUSE*                               |                                                    |
| 9                   | 12.01.2023                   | Data Donations                                  |                                                    |
| 10                  | 19.01.2023                   | Mock-Up-Virtual Environments                    |                                                    |
| 11                  | 26.01.2023                   | Open Science                                    |                                                    |
| 12                  | 02.02.2023                   | ***Guest Lecture: Linking DBD & Survey data***  | [Johannes Breuer](https://www.johannesbreuer.com/) |
| 13                  | 09.02.2023                   | Semesterabschluss & Evaluation                  | Christoph Adrian                                   |

# Agenda

1.  Organisation und Koordination

2.  A short (re-)introduction to DBD

3.  Herausforderungen von DBD

<!--# TODO: Folie überarbeiten -->

# Organisation & Koordination {background-color="#E6002E"}

Fragen, MS Teams & alternativer Seminarplan

## Kursmaterialien etc.

#### Kurze Einführung in Teams

![](images/ms_teams.png){fig-align="center"}

## Vorschlag: Alternativer Seminarplan {.smaller}

| Sitzung             | Datum                        | Thema                                           | Referent\*Innen                                    |
|------------------|------------------|------------------|-------------------|
| ~~1~~               | ~~26.10.2022~~               | ~~Kick-Off Session~~                            | ~~Christoph Adrian~~                               |
| [**2**]{.smallcaps} | [**02.11.2022**]{.smallcaps} | [**DBD: Einführung und Überblick**]{.smallcaps} | [**Christoph Adrian**]{.smallcaps}                 |
| 3                   | 09.11.2022                   | DBD: Datenerhebung                              | Christoph Adrian                                   |
| 4                   | 16.11.2022                   | API-Access (I): *Twitter*                       | Falk                                               |
| 5                   | 23.11.2022                   | API-Access (II): *YouTube*                      | Denisov                                            |
| 6                   | 30.11.2022                   | API-Access (II): *Reddit*                       | Landauer                                           |
| 7                   | 07.12.2022                   | Webscraping: *TikTok*                           | Brand & Kocher                                     |
| ***8***             | ***14.12.2022***             | ***Exkurs: DBD Analyse mit R***                 | ***Christoph Adrian***                             |
|                     |                              | *WEIHNACHTSPAUSE*                               |                                                    |
| ***9***             | ***12.01.2023***             | ***ESM: m-path***                               | ***Dörr***                                         |
| ***10***            | ***19.01.2023***             | ***TBD***                                       | ***Hofmann & Wierzbicki***                         |
| 11                  | 26.01.2023                   | Puffer                                          |                                                    |
| 12                  | 02.02.2023                   | ***Guest Lecture: Linking DBD & Survey data***  | [Johannes Breuer](https://www.johannesbreuer.com/) |
| 13                  | 09.02.2023                   | Semesterabschluss & Evaluation                  | Christoph Adrian                                   |

# A short (re-)introduction {background-color="#E6002E"}

Was sind *digital behavior data*?

Und was können wir mit Ihnen untersuchen?

## DBD -- Was ist das eigentlich?

#### Rückblick auf Definition nach @weller2021

-   ... fasst eine **Vielzahl von möglichen Datenquellen** zusammen, die verschiedene Arten von **Aktivitäten aufzeichnen** (*häufig sogar "nur" als Nebenprodukt***)**

-   ... können dabei helfen, **Meinungen, Verhalten und Merkmale der menschlichen Nutzung digitaler Technologien** zu erkennen

<br>

::: fragment
#### **Im Kontext dieses Seminars:**

-   Schwerpunkt: **Nutzung und Inhalte** von **soziale Medien**
-   **Computational Social Science** \[CSS\] **Verfahren**, z.B. zur Erhebung, Verarbeitung, Auswertung und Präsentation
:::

## Ohne CSS keine DBD

#### Zur Bedeutung von Computational Social Science

::: {.callout-tip appearance="minimal"}
## Definition von CSS

We define CSS as the development and application of computational methods to complex, typically large-scale, human (sometimes simulated) behavioral data." @lazer2020
:::

**hilft dabei ...**

-   echte digitale Phänomene zu untersuchen

-   digitale Verhaltensdaten zu sammeln und vorzuverarbeiten

-   neue Methoden zur Analyse von großen Datensätzen anzuwenden

::: notes
CSS = neues Teilgebiet der Sozialwissenschaften oder neuer "Werkzeugkasten" zur Ergänzung der traditionellen sozialwissenschaftlichen Ansätze
:::

## Und was können wir damit untersuchen?

#### Kategorisierung verschiedener Verhaltensweisen

::: columns
::: {.column width="70%"}
![](images/dbd_matrix.png){fig-align="center"}
:::

::: {.column width="30%"}
<br>

##### **Einschränkungen**

-   Kategorisierung ist **Momentaufnahme** und nicht überschneidungsfrei

-   **Selektive Nutzung** von bestimmten digitalen Geräten
:::
:::

::: notes
-   Kategorien: Digital/Analog individual/social behavior

-   Einige inhärent digitale Verhalten (z.B. Web Searches) bei zunehmender Digitalisierung von analogen Verhalten (z.B. Collaborative Work)

-   Fehlen digitaler Spurendaten in all diesen Quadranten für bestimmte Personen und bestimmte Verhaltensweisen durch selektive Nutzung digitaler Geräte.
:::

<!--# TODO: Animieren? Bilde slidet raus, Einschränkungen erscheinen -->

## Verfügbarkeit als Pluspunkt

#### Wertvolle Quelle bei sensiblen und unvorhersehbaren Themen

-   Mining von Meinungen aus bestehenden digitalen Kommunikationsströmen (z.B. Inhalte sozialer Medien) als wertvolle Quelle
    -   teilweise zeitnaher als die Erstellung einer Umfrage
    -   zusätzlicher Nutzen als "Archiv" bei unvorhersehbaren Ereignissen
-   Einsatz bei Themen bzw. Untersuchungen,
    -   für die es schwierig ist, Studienteilnehmer\*innen zu rekrutieren
    -   bei denen Beobachtungen vorteilhafter sind als Befragungen

<!--# TODO: Hier Einsatz von Markierungen (revealjs) -->

## Mehr Daten durch technologischen Fortschritt

#### Beispiel: Wachsenden Anzahl eingebauter Smartphone-Sensoren

![Graphik aus @struminskaya2020](images/dbd_smartphone_development.jpeg){alt="Aus @struminskaya2020" fig-align="center"}

<!--# TODO: Bedeutung/Funktion der einzelnen Sensoren -->

## Ein Plädoyer für DBD

#### Kurze Zusammenfassung

-   Digitale Geräte oder Sensoren können sich besser an bestimmte Fakten besser "erinnern" als das menschliche Gedächtnis.

-   Sensoren sind oft bereits in alltägliche Technik eingebaut Technologie eingebaut und produzieren digitale Verhaltensdaten als ein "Nebenprodukt".

-   Unaufdringliche Erfassung als potentieller Vorteil bzw. Entlastung für Teilnehmer\*Innen

-   Kombination mit Umfragedaten möglich (und bereichernd!)

# Herausforderungen von DBD

Der Umgang mit Biases, methodischen Tücken und ethischen Einschränkungen

## Wenn der Vorteil zum Nachteil wird

#### Ambivalenz der Unaufdringlichkeit @keusch2021

-   Unterscheidung zwischen aufdringlichen (z.B. spezielle Research-App & Befragungen) & unaufdringlichen (z.B. Cookie & APIs) erhobenen Daten

-   Bewertung und Erwartung an Datensammlung ist abhängig vom Kontext (Amazon vs. Researchgate)

-   Dilema: Einerseits bereitwillige (oft unwissende) Abgabe der Daten an Konzerne, andererseits häufig Bedenken bezüglich Datenschutz & Privatsphäre bei wissenschaftlichen Studien

::: notes
Q: Gründe für Ablehnung: Nutzenorientierung?
:::

## The End of Theory {.smaller}

#### Zur Wichtigkeit von konzipierte Messungen & Designs

::: columns
::: {.column width="50%"}
> "Who knows why people do what they do? The point is they do it, and we can track and measure it with unprecedented fidelity. With enough data, the numbers speak for themselves." [@anderson2008]
:::

::: {.column width="50%"}
> "Size alone does not necessarily make the data better" [@boyd2007]

> "there are a lot of small data problems that occur in big data \[which\] don't disappear because you've got lots of the stuff. They get worse" [@harford2014]
:::
:::

##### Problem

-   (Big) Data zunehmend Grundlage für politische Maßnahmen, die Gestaltung von Produkten und Dienstleistungen und für die automatisierte Entscheidungsfindung

<!--# Animieren mit fragments -->

## We need to talk about [biases]{.smallcaps}

#### Spezifische und allgemeine Herausfoderungen

::: columns
::: {.column width="50%"}
**Herausforderungen in Bezug auf DBD-Forschung**

-   fehlender Konsens über ein Vokabular oder eine Taxonomie

-   häufig nur impliziter der Bezug
:::

::: {.column width="50%"}
**Generelle Herausforderung:** `bias` **ist einweit gefasster Begriff**

-   *conformation bias* und andere kognitive Voreingenommenheiten (Croskerry, 2002)

-   systemische, diskriminierende Ergebnisse (Friedman und Nissenbaum, 1996)

-   systemische Schäden (Barocas et al., 2017)
:::
:::

::: notes
term "bias" is used in its more statistical sense to refer to biases in social data and social data analyses

Herausforderungen in Bezug auf DBD-Forschung

-   fehlender Konsenz über ein Vokabular oder eine Taxonomie von möglichen biases sowie methodischen Probleme und Fallstricke

-   häufig nur impliziter der Bezug zu potentiellen Quellen und/oder Auswirkungen von Verzerrung

Generelle Herausforderung

-   "bias" ist einweit gefasster Begriff ist, der in vielen Disziplinen untersucht wurde und verschiedene Phänomene abbildet

    -   *onformation bias* und andere kognitive Voreingenommenheiten (Croskerry, 2002)

    -   systemische, diskriminierende Ergebnisse (Friedman und Nissenbaum, 1996)

    -   systemische Schäden (Barocas et al., 2017)
:::

<!--# Was bedeuten hier "systematische Schäde"-->

## Know your bias!

#### Ein Framework zur Minimierung von Bias [@olteanu2019]

![](images/bias_framework_without_legend.png){fig-align="center"}

::: notes
Description:

-   Social data analysis starts with certain goals (section 2.1), such as understanding or influencing phenomena specific to social platforms (Type I) and/or phenomena beyond social platforms (Type II).

-   These goals require that research satisfies certain validity criteria, described earlier (section 2.2).

-   These criteria, in turn, can be compromised by a series of general biases and issues (section 3).

-   These challenges may depend on the characteristics of each data platform (section 4)---which are often not under the control of the researcher---and on the research designs choices made along a data processing pipeline (from sections 5 to 8)--which are often under the researcher control.

Pfeile zeigen an, wie sich Komponenten in unserem Rahmenwerk direkt auf andere auswirken

-   Erreichen bestimmter Ziele (Type I & II) bei der Analyse von Sozialdaten (Abschnitt 2.1) voraussetzt, dass die Forschung bestimmte Validitätskriterien erfüllt (Abschnitt 2.2),

-   die durch Verzerrungen und andere Probleme mit Sozialdaten beeinträchtigt werden können (Abschnitt 3)

-   Diese Verzerrungen und Probleme können an der Quelle der Daten auftreten (Abschnitt 4), oder sie können im Verlauf der Datenanalyse eingeführt werden (Abschnitte 5-8).
:::

## Worauf wirkt die Verzerrung?

#### Beispiele für Forschung von Typ I & II [@olteanu2019]

**Typ I: understand/influence phenomena [specific]{.underline} to social platforms**

-   Dynamik der Verbreitung von "Memes"
-   Steigerung der Attraktivität bzw. besonders Features
-   Verbesserung der Suchfunktion oder des Empfehlungssystems

**Typ II: understand/influence phenomena [beyond]{.underline} social platforms**

-   Beschreibung des Einflusses sozialer Medien auf eine politische Wahl.

-   Nutzung sozialer Daten zur Verfolgung der Entwicklung ansteckender Krankheiten durch Analyse der von Social-Media-Nutzern online gemeldeten Symptomen

## Eine Frage der Datenqualität

Verschiedene Kriterien

## Im Fokus: Population Bias {.smaller}

::: {.callout-important appearance="minimal"}
Systematic distortions in demographics or other user characteristics between a population of users represented in a dataset or on a platform and some target population.
:::

##### Auswirkungen:

-   kann die (Stichproben)-Repräsentativität beeinträchtigen\
    ➥ ⚠️externe Validität
-   besonders problematisch für Forschungsarbeiten des Typs II
-   kann sich auch auf die Leistung von Algorithmen auswirken, die Rückschlüsse auf die Nutzer ziehen\
    ➥ ⚠️interne Validität (Typ-I & Typ II)

##### Allgemeine Probleme:

-   Unterschiedliche Demographien (z.B. Geschlechts-, Alters- & Bildungsgruppen) neigen zu unterschiedlichen sozialen Plattformen und nutzen deren Mechanismen unterschiedlich

-   Proxies für Eigenschaften oder demografische Kriterien der Nutzenden sind unterschiedlich verlässlich

::: notes
Häufig ist der Zusammenhang von Untersuchungspopulation (z.B. in Deutschland lebende Erwachsene) zu Zielpopulation (Erwachsene auf Twitter, die angeben, in DE zu leben) unbekannt.

Beispiel "Auswirkungen"

-   Algorithmen: Schätzung der Geo-Location im Stadt-Land-Spektrum (z.B Johnson et al., 2017)

Beispiele "Probleme":

-   Signifikant mehr Twitter Nutzer (Mislove et al. ,2011), bei Pinterest tendenziell Frauen überrepräsentiert (Ottoni et al., 2013)

-   Unterschiedliche Twitter-Nutzung in DE (Fokus auf Hashtags) und KOR (Fokus auf Konversationen) (Hong et al., 2011).

-   Angabe von Geo-Location oder Profilinformationen in sozialen Netzwerken
:::

##  {.smaller}

## Im Fokus: Behavioral Bias {.smaller}

::: {.callout-important appearance="minimal"}
Systematic distortions in user behavior across platforms or contexts, or across users represented in different datasets.
:::

##### Allgemeine Probleme:

-   beeinflussen die Art und Weise, wie Nutzer miteinander interagieren

-   Auftreten von Selbstselektion und Reaktionsverzerrungen

    -   Response Bias

-   Verzerrungen beim Konsumieren von Inhalten beeinflussen die Art und Weise, wie Nutzer Inhalte finden und mit ihnen interagieren, aufgrund von Unterschieden in ihren Interessen, ihrem Fachwissen und ihrem Informationsbedarfes

::: notes
Unterschiede in Bezug auf Nutzerpersönlichkeiten (Hughes et al., 2012), die Verbreitung von Nachrichten (Lerman und Ghosh, 2010) oder den Austausch von Inhalten (Ottoni et al., 2014)

##### Auswirkungen:

-   Ergebnisse einer Studie von der gewählten Plattform oder dem Kontext abhängig\
    ➤ ⚠️externe Validität

-   nur Teilweise von population bias abhängig

-   bei (expliziten oder impliziten) Annahmen über die Verhaltensmuster der Nutzenden\
    ➤ ⚠️potentielle Effekte auf Untersuchung von Typ-I & Typ II (z.B. Untersuchung der Bedürfnisse oder Interessen der Nutzenden)

Beispiel "Probleme":

-   Gesonderte Diskussion von Verhalten, die sich auf die Erstellung von Inhalten durch die Nutzer auswirken ("content production bias") und solche, die sich auf die Verknüpfungsmuster zwischen Nutzern auswirken ("linking bias").

-   Drei weitere häufige Klassen von Verhaltensverzerrungen betreffen die Interaktionen zwischen Nutzern, die Interaktionen zwischen Nutzern und Inhalten und die Verzerrungen, die dazu führen, dass Nutzer in eine Studienpopulation aufgenommen oder von ihr ausgeschlossen werden.
:::

## Im Fokus: Content Production Biases {.smaller}

::: {.callout-important appearance="minimal"}
Behavioral biases that are expressed as lexical, syntactic, semantic, and structural differences in the content generated by users.
:::

##### Allgemeine Probleme:

-   Der Gebrauch der Sprache(n) variiert zwischen und innerhalb von Ländern und Bevölkerungsgruppen

    -   Mocanu et al. (2013): saisonale Schwankungen in der sprachlichen Zusammensetzung zwischen verschiedenen Gebiete (Länder, Regionen, Nachbarschaften etc.)

    -   Sprachgebrauchsvariationen je nach Geschlecht, Alter, regionaler Herkunft und politischer Orientierung auf Twitter (Rao et al. (2010)), sowie zwischen ethnischen Gruppen (Blodgett et al., 2016).

-   Kontextbedingte Faktoren beeinflussen die Art und Weise, wie Benutzer sprechen.

-   Die Inhalte von bekannten oder "erfahrenen" Nutzern unterscheiden sich von denen der normalen Nutzer.

-   Unterschiedliche Bevölkerungsgruppen haben unterschiedliche Neigungen, über bestimmte Themen zu sprechen.

::: notes
-   Unterschiede bei nutzergenerierten Inhalten, insbesondere bei Texten, zwischen und innerhalb von demografischen Gruppen
:::

## Im Fokus: Linking Bias {.smaller}

::: {.callout-important appearance="minimal"}
Behavioral biases that are expressed as differences in the attributes of networks obtained from user connections, interactions or activity.
:::

##### Allgemeine Probleme:

-   Netzattribute beeinflussen das Verhalten und die Wahrnehmung der Nutzer und umgekehrt.

-   Verhaltensbasierte und verbindungsbasierte soziale Verbindungen sind unterschiedlich.

-   Die Bildung sozialer Online-Netzwerke hängt auch von Faktoren außerhalb der sozialen Plattformen ab

::: notes
-   Sozialen Netzwerke, die aus beobachteten Mustern in Datensätzen (re)konstruiert werden, können sich grundlegend von den zugrunde liegenden (Offline-)Netzwerken unterscheiden (Schoenebeck, 2013a)\
    ➤ ⚠️externe Validität ➤ ⚠️ Typ-II & teilweise Typ-I (Fälle, in denen die Interaktions- oder Verknüpfungsmuster der Nutzer mit der Zeit oder dem Kontext variieren)

-   wirken sich beispielsweise auf die Untersuchung der Struktur und Entwicklung sozialer Netzwerke, des sozialen Einflusses und von Phänomenen der Informationsverbreitung aus (Wilson et al., 2009; Cha et al., 2010; Bakshy et al., 2012)

-   Auf sozialen Plattformen können sie auch zu systematisch verzerrten Wahrnehmungen über Nutzer oder Inhalte führen (Lerman et al., 2016).
:::

## Im Fokus: Temporal Biases {.smaller}

::: {.callout-important appearance="minimal"}
Systematic distortions across user populations or behaviors over time.
:::

##### Allgemeine Probleme:

-   Bevölkerungsgruppen, Verhaltensweisen und Systeme verändern sich mit der Zeit.

-   Saisonale und periodische Phänomene.

-   Plötzlich auftretende Phänomene wirken sich auf Populationen, Verhaltensweisen und Plattformen aus.

-   Die zeitliche Granularität kann zu feinkörnig sein, um langfristige Phänomene zu beobachten.

-   Die zeitliche Granularität kann zu grobkörnig sein, um kurzlebige Phänomene zu beobachten.

-   Datensätze verfallen und verlieren mit der Zeit an Nutzen.

::: notes
Auswirkunge:

-   ➤ ⚠️ eterne Validität

-   Beeinträchtigen Verallgemeinerbarkeit von Beobachtungen im Laufe der Zeit ➤ ⚠️ Typ-I & Typ-II-Forschung problematisch

Probleme:

-   Die Art und Weise, wie man Datensätze entlang der zeitlichen Achsen aggregiert und abschneidet, wirkt sich darauf aus, welche Art von Mustern beobachtet werden und welche Forschungsfragen beantwortet werden können.
:::

## Im Fokus: Redundancy {.smaller}

::: {.callout-important appearance="minimal"}
Single data items that appear in the data in multiple copies, which can be identical (duplicates), or almost identical (near duplicates).
:::

##### Allgemeine Probleme:

-   Lexikalische (z. B. Duplikate, erneute Tweets, erneut geteilte Inhalte) und semantische (z. B. Beinahe-Duplikate oder dieselbe Bedeutung, aber anders geschrieben) Redundanz macht oft einen erheblichen Teil der Inhalte aus (Baeza-Yates, 2013) und kann sowohl innerhalb als auch zwischen sozialen Datensätzen auftreten. Weitere Quellen für inhaltliche Redundanz sind häufig nicht-menschliche Konten (Abschnitt 4.4), wie z. B. ein und dieselbe Person, die von mehreren Konten oder Plattformen aus postet (z. B. Spam), mehrere Nutzer, die vom selben Konto aus posten (z. B. Konten von Organisationen), oder mehrere Personen, die denselben Inhalt posten oder erneut posten (z. B. das Posten von Zitaten, Memes oder anderen Arten von Inhalten). Dies kann manchmal die Ergebnisse verzerren, aber Redundanz kann auch ein Signal an sich sein, z. B. kann das erneute Posten ein Signal für Wichtigkeit sein.

::: notes
Auswirkunge:

-   Redundanz kann, wenn sie nicht berücksichtigt wird, sowohl die interne als auch die ökologische/externe Validität der Forschung beeinträchtigen, und zwar sowohl in der Forschung vom Typ I als auch vom Typ II (Abschnitt 2.1). Sie kann sich negativ auf den Nutzen von Instrumenten auswirken (Radlinski et al., 2011) und die Quantifizierung von Phänomenen in den Daten verzerren.

Probleme:

-   Die Art und Weise, wie man Datensätze entlang der zeitlichen Achsen aggregiert und abschneidet, wirkt sich darauf aus, welche Art von Mustern beobachtet werden und welche Forschungsfragen beantwortet werden können.
:::

## Potentielle Probleme mit der Datenquelle oder -herkunft

::: columns
::: {.column width="50%"}
![](images/bias_framework_lower_boxes-left.png){fig-align="center"}
:::

::: {.column width="50%"}
Überblick

-   Biases, die auf das Design und die Möglichkeiten der Plattformen zurückzuführen sind (functional biases)

-   Verhaltensnormen, die auf den einzelnen Plattformen bestehen oder sich herausbilden (normative biases).

-   Faktoren, die außerhalb der sozialen Plattformen liegen, aber das Nutzerverhalten beeinflussen können (external biases)

-   Vorhandensein von nicht-individuellen Konten ein (non-individuals).
:::
:::

## Im Fokus: Functional Biases

#### Probleme mit der Datenquelle oder -herkunft

::: {.callout-important appearance="minimal"}
Biases that are a result of platform-specific mechanisms or affordances, that is, the possible actions within each system or environment.
:::

##### Allgemeine Probleme:

-   Plattformspezifisches Design und Funktionen prägen das Nutzerverhalten.
-   Algorithmen, die für die Organisation und das Ranking von Inhalten verwendet werden, beeinflussen das Nutzerverhalten.
-   Die Präsentation von Inhalten beeinflusst das Nutzerverhalten.

::: notes
Hintergrund: - Die Möglichkeiten von Plattformen werden häufig von geschäftlichen Erwägungen und Interessen bestimmt (Van Dijck, 2013a; Salganik, 2017) und von der Politik, den Annahmen und den Interessen derjenigen beeinflusst, die diese Plattformen entwickeln und aufbauen (Van Dijck, 2013a; West et al., 2019). Die Möglichkeiten und Funktionen der Plattformen werden manchmal absichtlich eingeführt, um die Nutzer zu bestimmten Verhaltensweisen zu bewegen (Thaler und Sunstein, 2008). Jede Plattform verwendet einzigartige, proprietäre und oft nicht dokumentierte plattformspezifische Algorithmen, um Inhalte (oder Nutzer) zu organisieren und zu fördern, was sich auf das Engagement und Verhalten der Nutzer auswirkt. Idealerweise sollte die Forschung soziale Datenproben von verschiedenen Plattformen verwenden, aber aufgrund der begrenzten Verfügbarkeit von Daten konzentriert sich ein Großteil der Forschung auf Daten von einigen wenigen Plattformen, vor allem Twitter. Die Verwendung von Twitter als eine Art "Modellorganismus" in der Social-Media-Forschung ist kritisiert worden (Tufekci, 2014).

Auswirkunge: Funktionale Verzerrungen erschweren die Verallgemeinerung oder Übertragung von Schlussfolgerungen aus Forschungsstudien, da jede Plattform ihre eigenen strukturellen Unterschiede aufweist (Tufekci, 2014), was zu plattformspezifischen Phänomenen führen kann (Ruths und Pfeffer, 2014), die häufig übersehen werden. Die Tatsache, dass die meisten Forschungsarbeiten mit Daten von einer Handvoll Plattformen durchgeführt werden, verschärft dieses Problem noch. Funktionale Verzerrungen sind für die Forschung des Typs II (Abschnitt 2.1) problematisch und beeinträchtigen die externe/ökologische Validität der Sozialdatenforschung; und wenn sie sich z. B. im Laufe der Zeit ändern, können sie auch die Forschung des Typs I beeinträchtigen. Ihr Einfluss auf Verhaltens- und Adoptionsmuster ist jedoch oft subtil und lässt sich nur schwer von anderen Faktoren abgrenzen.
:::

## Im Fokus: Normative Biases

#### Probleme mit der Datenquelle oder -herkunft

::: {.callout-important appearance="minimal"}
Biases that are a result of written or unwritten norms and expectations of acceptable patterns of behavior on a given online platform or medium
:::

##### Allgemeine Probleme:

-   Normen werden durch die Einstellungen und Verhaltensweisen von Online-Gemeinschaften geprägt, die kontextabhängig sein können.
-   Das Bewusstsein, von anderen beobachtet zu werden, wirkt sich auf das Nutzerverhalten aus.
-   Soziale Konformität und "Herdentrieb" finden auf sozialen Plattformen statt, und solche Verhaltensmerkmale prägen das Nutzerverhalten.

::: notes
Hintergrund: Plattformen zeichnen sich durch ihre Verhaltensnormen aus, in der Regel in Form von Erwartungen darüber, was eine akzeptable Nutzung darstellt. Diese Normen werden durch Faktoren wie das spezifische Wertversprechen jeder Plattform und die Zusammensetzung ihrer Nutzerbasis geprägt (Boyd und Ellison, 2007; Ruths und Pfeffer, 2014; Newell et al., 2016b). Sukumaran et al. (2011) zeigen beispielsweise, wie die Nutzer von Nachrichten-Websites sich beim Verfassen von Kommentaren an informelle, von anderen gesetzte Standards halten, wie Länge oder Anzahl der behandelten Aspekte.

Auswirkunge: Wie funktionale Verzerrungen wirken sich auch normative Verzerrungen auf die ökologische/externe Validität der Forschung aus und sind für die Typ-II-Forschung (Abschnitt 2.1) problematisch, da die Forschungsergebnisse von den besonderen Normen der jeweiligen Plattform abhängen können. Sie können auch das Nutzerverhalten verzerren und neigen dazu, mit dem Kontext, der Zeit oder den verschiedenen Untergemeinschaften zu variieren, was ebenfalls die Forschung vom Typ I beeinträchtigt. Das Übersehen der Auswirkungen von Normen kann sich auf jede Sozialdatenanalyse auswirken, die das Nutzerverhalten untersucht oder Annahmen darüber trifft (Tufekci, 2014).
:::

## Im Fokus: External Sources of Bias

#### Probleme mit der Datenquelle oder -herkunft

::: {.callout-important appearance="minimal"}
Biases resulting from factors outside the social platform, including considerations of socioeconomic status, ideological/religious/political leaning, education, personality, culture, social pressure, privacy concerns, and external events.
:::

##### Allgemeine Probleme:

-   Kulturelle Elemente und soziale Kontexte spiegeln sich in sozialen Datensätzen wider.
-   Wie andere Medien enthalten auch die sozialen Medien Fehlinformationen und Desinformationen.
-   Inhalte zu verschiedenen Themen werden unterschiedlich behandelt.
-   Ereignisse von großer Tragweite werden in den sozialen Medien widergespiegelt, unabhängig davon, ob sie vorhergesehen wurden oder nicht.

::: notes
Hintergrund: Soziale Plattformen sind dem Einfluss einer Vielzahl von externen Faktoren ausgesetzt, die sowohl die demografische Zusammensetzung als auch das Verhalten ihrer Nutzer beeinflussen können.

Auswirkunge: Externe Verzerrungen sind eine weit gefasste Kategorie, die sich auf die Konstrukt-, interne und externe Validität der Forschung auswirken und sowohl für Typ-I- als auch für Typ-II-Forschung problematisch sein können (Abschnitt 2.1). Im Allgemeinen können sich externe Faktoren auf verschiedene Qualitätsdimensionen sozialer Datensätze auswirken, einschließlich Erfassungsgrad und Repräsentativität, doch können sie auch subtil und leicht zu übersehen sein und die Zuverlässigkeit der aus diesen Datensätzen gezogenen Beobachtungen beeinträchtigen (Silvestri, 2010; Kícíman, 2012; Olteanu et al., 2015).

Probleme: Wir befassen uns mit verschiedenen Arten von externen Faktoren wie dem sozialen und kulturellen Kontext, externen Ereignissen, semantischen Bereichen und Quellen.a
:::

## Im Fokus: Non-individual Accounts

#### Probleme mit der Datenquelle oder -herkunft

::: {.callout-important appearance="minimal"}
Interactions on social platforms that are produced by organizations or automated agents.
:::

##### Zwei Typen von nicht-individuellen Konten:

-   Konten von Organisationen
-   Bots

::: notes
Auswirkunge: Forscher gehen oft davon aus, dass jedes Konto ein Individuum ist; wenn dies nicht der Fall ist, kann die interne und externe Validität sowohl in der Typ-I- als auch in der Typ-II-Forschung beeinträchtigt werden (Abschnitt 2.1). Beispielsweise können Studien, die diese Datensätze verwenden, um Rückschlüsse auf die Prävalenz unterschiedlicher Meinungen in der Öffentlichkeit zu ziehen, besonders betroffen sein.

Probleme: Wir befassen uns mit verschiedenen Arten von externen Faktoren wie dem sozialen und kulturellen Kontext, externen Ereignissen, semantischen Bereichen und Quellen.a
:::

## Datenerfassung

#### Potentielle Probleme bei der Datenerhebung

Hintergrund

-   Die Erfassung sozialer Daten wird häufig von sozialen Plattformen geregelt und hängt von den Daten ab, die sie erfassen und zur Verfügung stellen, von den Grenzen, die sie für den Zugang setzen können, und von der Art und Weise, wie der Zugang gewährt wird.

Häufige Probleme:

-   Viele soziale Plattformen raten von der Datenerfassung durch Dritte ab.

-   Der programmatische Zugang ist oft mit Einschränkungen verbunden.

-   Die Plattform erfasst möglicherweise nicht alle relevanten Daten.

-   Die Plattformen gewähren möglicherweise keinen Zugang zu allen erfassten Daten.

-   Stichprobenstrategien sind oft undurchsichtig

## Abfrage von Daten

#### Potentielle Probleme bei der Datenerhebung

Hintergrund

-   Der Datenzugriff über APIs umfasst in der Regel eine Abfrage, die eine Reihe von Kriterien für die Auswahl, Einstufung und Rückgabe der angeforderten Daten angibt. Verschiedene APIs können unterschiedliche Arten von Abfragen unterstützen.

Häufige Probleme:

-   APIs haben eine begrenzte Aussagekraft in Bezug auf den Informationsbedarf.

-   Der Informationsbedarf kann auf unterschiedliche Weise operationalisiert (formuliert) werden.

-   Die Wahl der Schlüsselwörter in stichwortbasierten Abfragen prägt die resultierenden Datensätze.

## Filterung von Daten

#### Potentielle Probleme bei der Datenerhebung

Hintergrund

-   Bei der Datenfilterung werden irrelevante Teile der Daten entfernt, was manchmal während der Datenerfassung aufgrund der begrenzten Ausdruckskraft einer API oder Abfragesprache nicht möglich ist. Der Schritt der Datenfilterung am Ende einer Datenerfassungspipeline wird häufig als Nachfilterung bezeichnet, da er nach der Erfassung oder Abfrage der Daten erfolgt (daher das Präfix "post-").

Häufige Probleme

-   In der Regel impliziert die Entscheidung, bestimmte Daten zu entfernen, die Annahme, dass sie für eine Studie nicht relevant sind. Dies ist hilfreich, wenn die Annahme zutrifft, und schädlich, wenn sie nicht zutrifft.

    -   Ausreißer sind manchmal für die Datenanalyse von Bedeutung.

    -   Textfilterung kann bestimmte Analysen einschränken.

## Potentielle Probleme bei der Datenverarbeitung

Annahmen bei der Gestaltung von Datenverarbeitungspipelines können sich auf Datensätze auswirken und deren Inhalt, Struktur, Organisation oder Darstellung verändern (Barocas und Selbst, 2016; Poirier, 2018). Verzerrungen und Fehler können durch Vorgänge wie Bereinigung (Abschnitt 6.1), Anreicherung durch manuelle oder automatische Verfahren (Abschnitt 6.2) und Aggregation (Abschnitt 6.3) eingeführt werden.

Auswirkungen: Schlechte Entscheidungen bei der Datenverarbeitung gefährden vor allem die interne Validität der Forschung, können aber auch die ökologische/externe Validität beeinträchtigen. So ist beispielsweise Crowdsourcing einer der vorherrschenden Mechanismen zur Anreicherung von Daten und zur Erstellung von "Ground Truth"- oder "Goldstandard"-Datensätzen, die dann für eine Vielzahl von Modellierungs- oder Lernaufgaben verwendet werden können. Es hat sich jedoch gezeigt, dass einige "Goldstandards" je nachdem, wer die Annotation vornimmt, variieren, was sich wiederum auf die Leistung der Algorithmen auswirken kann (Sen et al., 2015). Folglich können sie sowohl die Forschung des Typs I als auch des Typs II beeinträchtigen (Abschnitt 2.1).

## Probleme bei der Datenbereinigung

#### Potentielle Probleme bei der Datenverarbeitung

Hintergrund

-   Mit der Datenbereinigung soll sichergestellt werden, dass die Daten das untersuchte Phänomen getreu wiedergeben (z. B. um die Konstruktvalidität zu gewährleisten). Sie zielt darauf ab, Fehler und Inkonsistenzen in den Daten zu erkennen und zu korrigieren, bis die "bereinigten" Daten Konsistenz- oder Validierungstests bestehen (Rahm und Do, 2000). Datenbereinigung ist nicht gleichbedeutend mit Datenfilterung: Während die Datenbereinigung die Entfernung bestimmter Datenelemente beinhalten kann, kann sie auch die Normalisierung von Daten durch Korrektur oder Ersetzung unvollständiger oder fehlender Werte umfassen.

Häufige Probleme

-   Datenbereinigungsverfahren können die Überzeugungen des Wissenschaftlers über ein Phänomen und das System im Allgemeinen in den Datensatz einbetten. Während gut begründete Änderungen die Validität eines Datensatzes verbessern, kann die Datenbereinigung zum Beispiel auch zu falschen oder irreführenden Datenmustern führen:

    -   Die Wahl der Datendarstellung und der Standardwerte kann zu Verzerrungen führen.

    -   Die Normalisierung von Text oder geografischen Referenzen kann zu Verzerrungen führen.

## Probleme bei der Datenanreicherung

#### Potentielle Probleme bei der Datenverarbeitung

Hintergrund

-   Die Datenanreicherung umfasst das Hinzufügen von Anmerkungen zu Datenelementen, die in der Analysephase verwendet werden können. Annotationen können von einfachen kategorischen Etiketten, die jedem Element zugeordnet sind, bis hin zu komplexeren Verarbeitungen wie Part-of-Speech-Tagging oder Dependency Parsing von Text reichen. Sie können entweder durch eine Form der (halb-)automatischen Klassifizierung oder durch menschliche Anmerkungen (z. B. Crowdsourcing, Umfragen) gewonnen werden.

Häufige Probleme

-   Sowohl die manuelle als auch die automatische Annotation sind jedoch fehleranfällig (Cohen und Ruths, 2013) und können sowohl bestehende Verzerrungen verschärfen als auch neue Verzerrungen und Fehler einführen.

    -   Die manuelle Beschriftung kann zu subjektiven und verrauschten Angaben führen.

    -   Die automatische Beschriftung durch statistische oder maschinelle Lernmethoden führt zu Fehlern.

## Probleme bei der Datenaggregation

#### Potentielle Probleme bei der Datenverarbeitung

Hintergrund

-   Die Datenaggregation wird durchgeführt, um Daten zu strukturieren, zu organisieren, darzustellen oder umzuwandeln; betrachten Sie Vorverarbeitungsheuristiken, die Daten aggregieren, um sie auf Kosten von Informationsverlusten besser handhabbar zu machen. Die Aggregation kann auch die Hervorhebung bestimmter Muster verringern oder verstärken (Olteanu et al., 2014b; Poirier, 2018).

Häufige Probleme

-   Die Art und Weise, wie diese Aggregationen vorgenommen werden, oder welche Informationen sie kompromittieren, kann zu unterschiedlichen Schlussfolgerungen führen. Bei der Aggregation nach geografischen Gesichtspunkten kann es tatsächlich zu einer Art "Gerrymandering" kommen, das zu sehr unterschiedlichen Ergebnissen führt.19 Bei der Betrachtung der Gesamthäufigkeit verschiedener Themen unter den Nutzern kann die Aggregation von Inhalten nach Nutzern den Interessen jedes Nutzers das gleiche Gewicht verleihen, während die Aggregation nach Themen den Inhalten hochaktiver Nutzer mehr Gewicht verleihen kann. Wenn die Daten entlang eines bestimmten Attributs organisiert sind (z. B. das Vorhandensein eines Schlüsselworts oder Hashtags) und es mehrere unabhängige Faktoren gibt, die dazu führen, dass das Attribut einen bestimmten Wert annimmt, ist die Analyse von Dateneinträgen mit diesem Wert gleichbedeutend mit einer Konditionierung auf diesen Wert und kann zu falschen Assoziationsmustern zwischen diesen Faktoren führen (Blyth, 1972; Tufekci, 2014).

## Methodische Fallstricke bei der Analyse von Daten

Die Wahl einer Analysemethode spiegelt in der Regel die Erfahrung und den Blickwinkel eines Forschers wider und kann verschiedene Bedenken hervorrufen, wie z. B. (i) die Verwendung von Daten als Quelle für Hypothesen und nicht als Instrument zu deren Prüfung; (ii) die Anpassung der Forschungsagenda an die Datenverfügbarkeit, was zu Verzerrungen bei der Art der gestellten Fragen führen kann; oder (iii) das Testen mehrerer Hypothesen, bis ein signifikantes, positives Ergebnis gefunden wird. Zu Letzterem gehören Praktiken wie das "Feature Hunting" (Ruths und Pfeffer, 2014): das gierige Testen mehrerer Merkmale für Klassifizierungsaufgaben, bis dasjenige gefunden ist, das die besten Verbesserungen liefert, anstatt diese Merkmale auf der Grundlage von A-priori-Hypothesen auszuwählen.

Implikationen. Eine wichtige Konsequenz ist die mangelnde Replizierbarkeit. Aufgrund von Unterschieden in der Analysemethodik, Messung und Datenerhebung konnten Liang und Fu (2015) beispielsweise 6 von 10 bekannten Thesen aus Social-Media-Studien nicht wiederholen. Generell kann die interne und externe Validität sowohl der Typ-I- als auch der Typ-II-Forschung durch die Wahl der Methoden beeinflusst werden, die bei der Analyse der Daten angewandt werden, um Nutzerpopulationen und -verhaltensweisen zu charakterisieren (Abschnitte 7.1-7.2), Schlussfolgerungen und Vorhersagen zu treffen (Abschnitt 7.3) und (kausale) Zusammenhänge zu ermitteln (Abschnitt 7.4).

## Probleme bei der Auswertung und Interpretation der Ergebnisse

Eine letzte Möglichkeit, Verzerrungen zu berücksichtigen und die Zuverlässigkeit der Ergebnisse zu beurteilen, besteht bei der Bewertung und Interpretation der Leistung eines Instruments oder der Ergebnisse einer Studie. Ein guter Ausgangspunkt ist ein angemessenes Verständnis für die Art der verwendeten Daten. Rost et al. (2013) argumentieren beispielsweise, dass Daten, die explizit von Nutzern in sozialen Medien generiert werden, eher als kommunikativ denn als repräsentativ interpretiert werden sollten, da diese Daten oft eine Aufzeichnung der Kommunikation und nicht eine direkte Darstellung anderer ("realer") Verhaltensweisen sind, was Fragen zur Konstrukt- und internen Validität aufwirft.

Implikationen. Die Art und Weise, wie die Evaluation in einer Studie durchgeführt wird, kann zu verzerrten Schlussfolgerungen oder Ergebnissen führen, u. a. aufgrund der Auswahl der Messgrößen (Abschnitt 8.1) oder der Bewertung und Interpretation der Ergebnisse (Abschnitt 8.2), die beide die Konstruktvalidität gefährden können. Solche Probleme können auch Bedenken hinsichtlich der Reproduzierbarkeit einer Studie aufkommen lassen; und wenn Verzerrungen nicht berücksichtigt werden, kann das Versäumnis, potenzielle Einschränkungen ordnungsgemäß anzuerkennen (Abschnitt 8.3), wichtige Validitätsprobleme verbergen, die sowohl Typ-I- als auch Typ-II-Forschung betreffen können.

## Ethische Erwägungen

In den vorangegangenen Abschnitten ging es um ethische Fragen, die Mittelstadt et al. (2016) als epistemische Bedenken bezeichnen (Abschnitte 3-8), wie etwa die Verwendung von nicht schlüssigen oder fehlgeleiteten Beweisen. Im Gegensatz dazu befasst sich dieser Abschnitt mit normativen Bedenken, die sich hauptsächlich auf die Folgen der Forschung beziehen.

Die Forschung an Menschen ist in vielen Ländern gesetzlich geregelt, und da die Datenelemente in sozialen Datensätzen Menschen oder Gruppen von Menschen repräsentieren (Varshney, 2015; Diaz, 2016), handelt es sich bei der Forschung an sozialen Daten zweifellos um Forschung an Menschen (Metcalf und Crawford, 2016). Die Tatsache, dass soziale Daten häufig öffentlich zugänglich sind, bedeutet nicht, dass die damit durchgeführte Forschung ethisch vertretbar ist (Zimmer, 2010; Boyd und Crawford, 2012). Infolgedessen haben sowohl Wissenschaftler (Dwork und Mulligan, 2013; Barocas und Selbst, 2016) als auch Journalisten (Hill, 2014; Kirchner, 2015) darauf gedrängt, die Nutzung sozialer Daten stärker auf mögliche ethische Fallstricke hin zu überprüfen, wie etwa die Verletzung der Privatsphäre der Nutzer (Goroff, 2015) oder die Ermöglichung von rassischem, sozioökonomischem oder geschlechtsspezifischem Profiling (Barocas und Selbst, 2016).

Derartige ethische Fragen wurden durch jüngste Fälle noch deutlicher, darunter das Facebook-Experiment zur Ansteckung (Anfang 2012 durchgeführt und Ende 2014 veröffentlicht), bei dem Forscher die sozialen Feeds von Nutzern so manipulierten, dass sie je nach den geäußerten Emotionen mehr oder weniger von bestimmten Inhalten enthielten (Kramer et al., 2014). Das Experiment wurde als ein Eingriff kritisiert, der den emotionalen Zustand von ahnungslosen Nutzern beeinflusste, die keine Zustimmung zur Teilnahme an der Studie gegeben hatten (Hutton und Henderson, 2015a). Ein weiteres Beispiel ist das Encore-Forschungsprojekt und die Messung der Internetzensur auf der ganzen Welt, bei der Webbrowser angewiesen wurden, zu versuchen, sensible Webinhalte ohne das Wissen oder die Zustimmung der Nutzer herunterzuladen (Burnett und Feamster, 2015), wodurch Menschen in einigen Ländern durch diese Zugriffsversuche möglicherweise gefährdet wurden. In einem beispiellosen Schritt beschloss das Programmkomitee (PC) der SIGCOMM 2015, das Encore-Forschungspapier unter der Bedingung zu akzeptieren, dass am Anfang des Papiers ein deutlicher Hinweis auf die ethischen Bedenken des PC platziert wird (Narayanan und Zevenbergen, 2015).

Im nächsten Abschnitt (Abschnitt 9.1) wird ein zentrales Spannungsverhältnis in der Forschungsethik digitaler Daten dargestellt. Anschließend wird die Diskussion spezifischer ethischer Probleme in der Sozialdatenforschung im Hinblick auf drei grundlegende Kriterien gegliedert, die im Belmont-Bericht (Ryan et al., 1978), einem grundlegenden Werk zur Forschungsethik, vorgebracht wurden: Autonomie (Abschnitt 9.2), Wohltätigkeit (Abschnitt 9.3) und Gerechtigkeit (Abschnitt 9.4).24 Da unsere Behandlung des Themas absichtlich schematisch ist, kann der interessierte Leser weitere Informationen in verwandten Arbeiten von Grimmelmann (2015), Metcalf und Crawford (2016), Bowser und Tsai (2015), Benton et al. (2017) und Mittelstadt et al.

## Ein schmaler Grat: Forschungethik bei digitalen Daten

Um ethische Fragen im Zusammenhang mit sozialen Daten zu klären, müssen zwei extreme Sichtweisen miteinander in Einklang gebracht werden: 1) Die Sozialdatenforschung ähnelt klinischen Versuchen und anderen Experimenten am Menschen in ihrer Fähigkeit, Menschen zu schaden, und sollte daher auch als solche reguliert werden; und 2) die Sozialdatenforschung ähnelt der sonstigen Computerforschung, die sich traditionell auf Methoden, Algorithmen und den Aufbau von Systemen konzentriert, mit minimalen direkten Auswirkungen auf Menschen.

###### Die Sozialdatenforschung unterscheidet sich von klinischen Versuchen.

Viele der traditionellen Verfahren zur Gewährleistung der Einhaltung ethischer Grundsätze in der Forschung am Menschen wurden im Zusammenhang mit klinischen Versuchen entwickelt, bei denen die Wirkung von Behandlungen an tatsächlichen Patienten getestet wird. Diese können schädliche, manchmal schwerwiegende und unumkehrbare unerwartete Auswirkungen haben. Im Gegensatz dazu sind die Schäden, die die üblichen Arten der Sozialdatenforschung verursachen können, oft anderer Natur, wie z. B. die Verletzung der Privatsphäre oder der Anblick verstörender Bilder. Ein speziell für die Sozialdatenforschung konzipiertes ethisches Genehmigungsverfahren, wie das von Bowser und Tsai (2015) vorgeschlagene, das Fragen enthält, die für soziale Medien spezifisch sind, oder die von Benton et al. (2017) skizzierten Praktiken könnten besser geeignet sein, um zu entscheiden, ob eine Forschungsaktivität stattfinden soll oder nicht und unter welchen Bedingungen.

###### Ethische Entscheidungen in der Sozialdatenforschung müssen gut überlegt sein.

Ethische Entscheidungen sind unter anderem deshalb schwierig, weil sie oft mehrere Werte betreffen, die miteinander in Konflikt stehen können. So kann die Datenanalyse beispielsweise erforderlich sein, um wichtige Dienste bereitzustellen, und es sollten Lösungen erwogen werden, die ein Gleichgewicht zwischen Datenschutz und Genauigkeit herstellen (Goroff, 2015). In anderen Fällen können Experimente erforderlich sein, um festzustellen, welche Maßnahmen oder Behandlungen angemessen sind - Meyer et al. (2019) stellten jedoch eine Abneigung gegen Experimente fest, wobei die Menschen die allgemeine Umsetzung von nicht getesteten Maßnahmen befürworten, aber randomisierte Experimente ablehnen, um zu testen, welche Maßnahme besser ist. Computerfachleute sind unterschiedlich gut darauf vorbereitet, diese Art von Problemen anzugehen. In der Regel lassen sich ethische Fragen am besten durch sachkundige Überlegungen und Gespräche klären. Aus diesem Grund sind die Genehmigung und Überwachung von Forschungsarbeiten durch institutionelle Prüfungsausschüsse (Institutional Review Boards, IRBs) so wichtig. IRBs legen gemeinsame Standards innerhalb einer Institution fest, bieten Forschern einen Rahmen, um kritisch über die Konsequenzen nachzudenken, und zeigen anderen, dass sorgfältige Entscheidungen für eine Studie getroffen wurden.

## Achtung der individuellen Autonomie

Der Respekt vor der Fähigkeit des Einzelnen, autonome Entscheidungen zu treffen, wird in der Forschung häufig durch die informierte Zustimmung zum Ausdruck gebracht. Die Einwilligung nach Aufklärung setzt voraus, dass (i) die Forscher den potenziellen Teilnehmern alle relevanten Informationen offenlegen; (ii) die potenziellen Teilnehmer in der Lage sind, diese Informationen zu bewerten; (iii) die potenziellen Teilnehmer freiwillig entscheiden können, ob sie teilnehmen wollen oder nicht; (iv) die Teilnehmer den Forschern ihre ausdrückliche Erlaubnis erteilen, häufig in schriftlicher Form; und (i) die Teilnehmer die Möglichkeit haben, ihre Einwilligung jederzeit zurückzuziehen.

###### Häufige Probleme

-   Die Zustimmung von Millionen von Nutzern einzuholen ist unpraktisch.

-   Das öffentliche Teilen von Inhalten im Internet bedeutet nicht unbedingt eine Zustimmung zur Forschung.

-   Die Nutzungsbedingungen sozialer Plattformen stellen möglicherweise keine informierte Zustimmung zur Forschung dar.

## Wohltätigkeit und Nicht-Malefizierung

Ein weiteres wichtiges ethisches Kriterium betrifft die Bewertung von Risiken und Nutzen; insbesondere sollte die Forschung von Nutzen sein und keinen Schaden verursachen (Non-Maleficence). Die Forscher sollten nicht nur über den Nutzen der Forschung nachdenken, sondern auch über die möglichen Arten von Schäden (Barocas et al., 2017), die betroffenen Gruppen und die Art und Weise, wie nachteilige Auswirkungen getestet werden können (Sweeney, 2013).

###### Häufige Probleme

-   Die Forschung zu sozialen Daten wird mit bestimmten Arten von Schäden in Verbindung gebracht, von denen die Verletzung der Privatsphäre vielleicht die offensichtlichste ist (Zimmer, 2010; Crawford und Finn, 2014).

-   Daten über Einzelpersonen können ihnen schaden, wenn sie offengelegt werden.

-   Forschungsergebnisse können verwendet werden, um Schaden anzurichten.

-   "Dual-Use"- und Sekundäranalysen sind in der Sozialdatenforschung immer häufiger anzutreffen.

## Recht

Ein Ideal der Gerechtigkeit in der Forschung besteht darin, dass Risiken und Nutzen gerecht verteilt werden, was voraussetzt, dass von Anfang an bekannt ist, wer durch die Forschung belastet wird und wer von den Ergebnissen profitieren wird.

###### Häufige Probleme

-   Die digitale Kluft kann das Forschungsdesign beeinflussen.

-   Algorithmen und Forschungsergebnisse können zu Diskriminierung führen.

-   Forschungsergebnisse sind möglicherweise nicht allgemein zugänglich.

-   Nicht alle Interessengruppen werden über die Verwendung von Forschungsergebnissen konsultiert.

## Zusammenfassung und Ausblick

-   Eine zunehmende Skepsis gegenüber einfachen Antworten

-   Vom Aufwerfen von Bedenken über soziale Daten zum Umgang mit ihnen. 4 Empfehlungen:

-   Unsere erste Empfehlung besteht darin, den Prozess, mit dem Datensätze und Modelle erstellt werden, detailliert zu dokumentieren und kritisch zu prüfen, wobei auch die von uns beschriebenen Verzerrungen berücksichtigt werden sollten.

-   Unsere zweite Empfehlung besteht darin, Studien zu sozialen Daten auf verschiedene Plattformen, Themen, Zeitpunkte und Teilpopulationen auszuweiten, um festzustellen, wie sich die Ergebnisse beispielsweise in verschiedenen kulturellen, demografischen und verhaltensbezogenen Kontexten unterscheiden.Sandvig et al. (2014) argumentieren, dass eine genaue Prüfung auch dann erforderlich ist, wenn ein Social-Software-System die Bedürfnisse der Nutzer zu erfüllen scheint, da es "subtile Muster problematischen Verhaltens" geben kann, die schwer zu erkennen sind. So stellen Kulshrestha et al. (2017) einen Rahmen für die Prüfung von Suchsystemen auf Social-Media-Plattformen vor, indem sie zwischen verschiedenen Quellen von Verzerrungen (z. B. aufgrund von Inhalten oder Ranking-Algorithmen) unterscheiden. Für Audits ist manchmal der Zugang zu proprietären Systemen erforderlich, was eine ausdrückliche Genehmigung für solche Systeme erfordert, die wahrscheinlich verweigert wird, wenn das Ziel darin besteht, ihre Fehler aufzudecken oder zu veröffentlichen. Ein Reverse-Engineering dieser Systeme oder die Nutzung dieser Systeme in einer unvorhergesehenen Weise, um ihre Voreingenommenheit aufzudecken, kann in den USA nach dem Computer Fraud and Abuse Act (CFAA) illegal sein, was von einer Gruppe von Forschern vor Gericht angefochten wurde.

-   Daher lautet unsere dritte Empfehlung, Transparenzmechanismen zu schaffen, die es ermöglichen, soziale Software zu überprüfen und Verzerrungen in sozialen Daten an der Quelle zu evaluieren (Abschnitt 4). Darüber hinaus gibt es auch immer mehr Bemühungen, die Grenzen sozialer Daten in Form von Leitlinien, Standards und neuen methodischen Ansätzen zu überwinden. Zu diesen Bemühungen gehört die Anwendung von Techniken aus der Kausalschlussliteratur, die zu robusteren Forschungsergebnissen führen können (Landeiro und Culotta, 2016; Proserpio et al., 2016), oder die Kalibrierung nicht repräsentativer Sozialdatenstichproben (Zagheni und Weber, 2015). Eine weitere Möglichkeit ist die Verwendung standardisierter Bewertungsprotokolle beim Testen neuer Instrumente oder Methoden (Diaz, 2014; Jurgens et al., 2015b).

-   Unsere vierte Empfehlung lautet, die Forschung zu diesen Leitlinien, Standards, Methoden und Protokollen auszuweiten und ihre Übernahme zu fördern.

-   Schließlich gibt es angesichts der Komplexität der inhärent kontextabhängigen, anwendungs- und bereichsabhängigen Verzerrungen und Probleme in sozialen Daten und Analysepipelines, die in diesem Papier behandelt werden, keine Einheitslösungen - bei der Bewertung und Bekämpfung von Verzerrungen ist Nuancierung entscheidend.

# Group Activity

Gruppendiskussion

## Verfügbarkeit vor Interesse

#### Zur Wichtigkeit von konzipierte Messungen & Designs

-   DBB sind häufig organische Daten, die zufällig gesammelt und dann von der Wissenschaft "recycelt" werden

    -   Zentrale Frage: Ist das ein Problem? Wenn ja, warum?

-   Repräsentativität vs. Stichprobenziehung

-   Kausalität vs. Korrelation

## Validität als Grundlage für "Aussagekraft" {.smaller}

#### Die verschiedenen Formen von Validität

**Interne Validität**

-   Does our analysis correctly lead from the measurements to the conclusions of the study?

**Externe Validität**

-   To what extent can research findings be generalized to other situations?

**Konstruktvaldität**

-   Do our measurements over our data measure what we think they measure?

<!-- -->

-   Type II: Nutzung sozialer Daten zur Beantwortung von Fragen und zur Ermittlung von Maßnahmen, die für Medien, Regierungen, Nichtregierungsorganisationen und Unternehmen relevant sind, oder zur Bearbeitung von Problemen aus Bereichen wie Gesundheit, Wirtschaft und Bildung..

-   Construct validity:

    -   Beispiel: If a hypothesis states that "self-esteem" increases with age, research tracking self-esteem over time from social media postings must ask whether its assessment of selfesteem from the postings actually measures self-esteem, or if instead it measures some other related or unrelated construct. In other words, we need to know whether the observed behaviors (such as words or phrases used in postings) are driven primarily by users' self-esteem vs. by community norms (section 4.2), system functionalities (section 4.1), or other reasons (section 3.3). Construct validity is specially important when the construct (self-esteem) is unobservable/latent and has to be operationalized via some observed attributes (words or phrases used)

-   Internal validity

    -   Does our analysis correctly lead from the measurements to the conclusions of the study?

    -   This survey covers subtle errors of this kind, such as biases that can be introduced through data cleaning procedures (section 6), the use of machine learned classifiers, mistaken assumptions about data distributions, and other inadvertent biases introduced through common analyses of social media (section 7).

    -   Example: An analysis of whether "self-esteem" increases with age may not be internally valid if text filtering operations accidentally remove terms expressing confidence (section 5.3); or if machine learned classifiers were inadvertently trained to recognize self-esteem only in younger people (section 7). Of course, while we do not dwell on them, researchers should also be aware of more blatant logical errors---e.g., comparing the self-esteem of today's younger population to the self-esteem of today's older population would not actually prove that selfesteem increases with age (section 3.6).

-   External validity:

    -   To what extent can research findings be generalized to other situations?

    -   For example, effects observed on a social platform may manifest differently on other platforms due to different functionalities, communities, or cultural norms (Wijnhoven and Bloemen, 2014; Malik and Pfeffer, 2016). The concept of external validity includes what is sometimes called ecological validity, which captures to what extent an artificial situation (constrained social media platform) properly reflects a broader real-world phenomenon (Ruths and Pfeffer, 2014). It also includes temporal validity, which captures to what extent constructs change over time (Howison et al., 2011) and may invalidate previous conclusions about societal and/or platform phenomena; e.g., see the case of Google Flu Trends (Lazer et al., 2014).

    -   Example: Even after we conclude a successful study of "selfesteem" in a longitudinal social media dataset collected from a given social media platform (section 4), its findings may not generalize to a broader setting as people who chose that particular platform may not be representative of the broader population (section 3.2); or perhaps their behaviors online are not representative of their behaviors in other settings (section 3.3).

# Bis zur nächsten Sitzung! {background-color="#04316a"}

## Literatur

::: {#refs}
:::
