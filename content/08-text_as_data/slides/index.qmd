---
title: "Text as data" 
title-slide-attributes:
  data-background-image: ../../../img/background_title.png
  data-background-color: "#04316a"
subtitle: "Digital behavioral data - Extra session"
author: 
  - name: Christoph Adrian 
    url: https://twitter.com/chrdrn
    affiliation: Lehrstuhl f√ºr Kommunikationswissenschaft
    affiliation-url: https://www.kowi.rw.fau.de/person/christoph-adrian/
date: 14 12 2022
date-format: "DD.MM.YYYY"
format:
  revealjs:
    theme: ../../slidetheme.scss
    template-partials:
      - title-slide.html
    slide-number: true
    chalkboard:
      buttons: false
    preview-links: auto
    logo: ../../../img/logo.png
    footer: "[Digital Behavioral Data](https://chrdrn.github.io/digital-behavioral-data/)"
highlight-style: atom-one
comments:
  hypothesis: 
    theme: clean
filters:
  - roughnotation
execute:
  echo: true
csl: ../../apa.csl
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

## Seminarplan {.smaller}

| Sitzung                                                | Datum                                                           | Thema                                                                              | Referent\*Innen                                                       |
|--------------------------------------------------------|-----------------------------------------------------------------|------------------------------------------------------------------------------------|-----------------------------------------------------------------------|
| [*1*]{.rn rn-type="strike-through" rn-color="#E6002E"} | [*26.10.2022*]{.rn rn-type="strike-through" rn-color="#E6002E"} | [*Kick-Off Session*]{.rn rn-type="strike-through" rn-color="#E6002E"}              | [*Christoph Adrian*]{.rn rn-type="strike-through" rn-color="#E6002E"} |
| [*2*]{.rn rn-type="strike-through" rn-color="#E6002E"} | [*02.11.2022*]{.rn rn-type="strike-through" rn-color="#E6002E"} | [*DBD: Einf√ºhrung und √úberblick*]{.rn rn-type="strike-through" rn-color="#E6002E"} | [*Christoph Adrian*]{.rn rn-type="strike-through" rn-color="#E6002E"} |
| [*3*]{.rn rn-type="strike-through" rn-color="#E6002E"} | [*09.11.2022*]{.rn rn-type="strike-through" rn-color="#E6002E"} | [*DBD: Datenerhebung*]{.rn rn-type="strike-through" rn-color="#E6002E"}            | [*Christoph Adrian*]{.rn rn-type="strike-through" rn-color="#E6002E"} |
| [*4*]{.rn rn-type="highlight"}                         | [*16.11.2022*]{.rn rn-type="highlight"}                         | [*API-Access (I): Twitter*]{.rn rn-type="highlight"}                               | [*Falk*]{.rn rn-type="highlight"}                                     |
| *5*                                                    | *23.11.2022*                                                    | *API-Access (II): YouTube*                                                         | *Denisov*                                                             |
| 6                                                      | 30.11.2022                                                      | API-Access (II): *Reddit*                                                          | Landauer                                                              |
| 7                                                      | 07.12.2022                                                      | Webscraping: *TikTok*                                                              | Brand & Kocher                                                        |
| **8**                                                  | **14.12.2022**                                                  | **Text as data**                                                                   | **Christoph Adrian**                                                  |
|                                                        |                                                                 | *WEIHNACHTSPAUSE*                                                                  |                                                                       |
| 9                                                      | 11.01.2023                                                      | ESM: *m-path*                                                                      | D√∂rr                                                                  |
| 10                                                     | 18.01.2023                                                      | Data Donations                                                                     | Hofmann & Wierzbicki                                                  |
| 11                                                     | 25.01.2023                                                      | *PUFFER*                                                                           |                                                                       |
| **12**                                                 | **02.02.2023**                                                  | ***Guest Lecture: Linking DBD & Survey data***                                     | [**Johannes Breuer**](https://www.johannesbreuer.com/)                |
| 13                                                     | 08.02.2023                                                      | Semesterabschluss & Evaluation                                                     | Christoph Adrian                                                      |

::: notes
‚ñ∂Ô∏è

‚ö†Ô∏èTerminkonflikt
:::

# Agenda {.bg_agenda}

1.  [A short introduction]
2.  [Case Study: Amazon Reviews](#sec2)
3.  [‚òï Kursevluation]
4.  [Text processing]
5.  [Sentiment Analyse]
6.  [Topic Modeling]

# A short introduction {background-color="#E6002E"}

im Fokus: *Text as data*

## Nicht neu, aber andere Dimension

#### Das Ph√§nomen *Text as data*

::: columns
::: {.column width="50%"}
-   Lange **Tradition** der Text- und Inhaltsanalyse (besonders in der Kommunikationswissenschaft)

-   **Neue Chancen & Herausforderungen** durch explosionsartige Vergr√∂√üerung des (Text-)Datenaufkommen in den letzten Jahren (Websites, Plattformen & Digitalisierung)
:::

::: {.column width="50%"}
[![](images/hilbert_lopez_2011.png){fig-align="center"}](https://www.bitbybitbook.com/en/1st-ed/introduction/digital-age/)

[@salganik2018]
:::
:::

## Neue Quellen, Neue Methoden, neue M√∂glichkeiten

#### Verschiedene Textgrundlagen als Beispiel

#### ![](graphs/graph_01.svg)

## Possibilities over possibilities

#### √úberblick √ºber verschiedene Methoden der Textanalyse

![](images/text_as_data_methods.png){fig-align="center"}

# Case study: {{< fa brands amazon >}} Reviews {#sec2 background-color="#E6002E"}

üïµÔ∏è ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

[![Binder](https://mybinder.org/badge_logo.svg){alt="Binder"}](https://mybinder.org/v2/gh/chrdrn/dbd_binder/HEAD) [![Google Colab](https://colab.research.google.com/assets/colab-badge.svg){alt="Google Colab"}](https://colab.research.google.com/github/chrdrn/dbd_binder/blob/main/session_08-showcase_text_as_data.ipynb) [![RStudio Quarto](https://img.shields.io/badge/RStudio-4285F4?style=for-the-badge&logo=rstudio&logoColor=white){alt="RStudio Quarto"}](https://chrdrn.github.io/digital-behavioral-data/content/08-text_as_data/08-exercise.html)

::: notes
But ... why?
:::

```{r preparation}
#| echo: false
# define options
options(scipen = 999)
# load packages
pacman::p_load(
  rmarkdown, kableExtra,
  here, fs, # file management
  sjmisc, magrittr, lubridate, janitor, # data processing
  sjPlot, ggpubr, ggsci, # visual
  RCurl, XML, rvest, # scraping
  quanteda, quanteda.textplots, quanteda.textstats, fastText, # text processing
  stm, tidytext, glue,
  tictoc,
  tidyverse  # last to resolve masking issues
)
# load data
amazon <- readRDS(
  url("https://github.com/chrdrn/dbd_binder/blob/main/data/08-text_as_data/reviews_tpm.RDS?raw=true"), "rb")

```

## Im Fokus: üíä Di√§tpillen

#### Hintergrund

::: columns
::: {.column width="50%"}
-   **Reviews** von **f√ºnf "Di√§tpillen"**
-   Automatisch Scraping via eigener R-Funktion
-   Datensatz mit **knapp √ºber 2000 Reviews** (ohne Bereinigung)
-   Exemplarische Darstellung folgender Schritte:
    -   Text-Processing

    -   Sentiment-Analyse

    -   Topic Modeling
:::

::: {.column width="50%"}
::: r-stack
![](images/amazon_review_01.png){.fragment .semi-fade-out fragment-index="1" fig-align="center"}

![](images/amazon_review_02.png){.fragment fragment-index="1" fig-align="center"}
:::
:::
:::

::: notes
-   Ausf√ºhrliche Schritte im Google Colab & Showcase

-   Hier nur Ausz√ºge
:::

## Quick scraping step-by-step

#### 1Ô∏è‚É£ Custom function f√ºr Export der html-Elemente

```{r}
#| eval: false
scrape_amazon <- function(page_num, review_url) {
  url_reviews <- paste0(review_url, "&pageNumber=", page_num, "&sortBy=recent")
  doc <- read_html(url_reviews)
  map_dfr(doc %>% html_elements("[id^='customer_review']"), ~ data.frame(
    review_title = .x %>% html_element(".review-title") %>% html_text2(),
    review_text = .x %>% html_element(".review-text-content") %>% html_text2(),
    review_star = .x %>% html_element(".review-rating") %>% html_text2(),
    date = .x %>% html_element(".review-date") %>% html_text2() %>% gsub(".*vom ", "", .),
    author = .x %>% html_element(".a-profile-name") %>% html_text2(),
    page = page_num
  )) %>%
    as_tibble %>%
    return()
}
```

## Quick scraping step-by-step

#### 2Ô∏è‚É£ Definition von Amazon Review URLs

```{r}
#| eval: false
url <- list(
  p01 = "https://www.amazon.de/LINEAVI-Eiwei%C3%9F-Shake-Kombination-Molkeneiwei%C3%9F-laktosefrei/product-reviews/B018IB02AU/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews",
  p02 = "https://www.amazon.de/Detoxkuren%E2%80%A2-Entw%C3%A4sserung-Entschlackung-Stoffwechsel-entschlacken/product-reviews/B072QW5ZN1/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews",
  p03 = "https://www.amazon.de/Saint-Nutrition%C2%AE-KETO-BURN-Appetitz%C3%BCgler/product-reviews/B08B67V8G5/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews",
  p04 = "https://www.amazon.de/Yokebe-vegetarisch-Mahlzeitersatz-Gewichtsabnahme-hochwertigen/product-reviews/B08GYZ8LRB/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews",
  p05 = "https://www.amazon.de/Vihado-Liquid-chlorophyll-drops-alfalfa/product-reviews/B093XNC8QH/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews"
)
```

## Quick scraping step-by-step

#### 3Ô∏è‚É£ Scrape with for-loops

```{r}
#| eval: false
amazon <- list()
# p01 
for (i in 1:79) {
  df <- scrape_amazon(page_num = i, review_url = url$p01)
  amazon$raw$p01[[i]] <- df
}
```

<br>

```{r}
#| eval: false
product <- names(url)
# bind rows for each product
for (i in product) {
  amazon$data$raw[[i]] <- amazon$raw[[i]] %>% 
    bind_rows() %>% 
    rownames_to_column("id") %>% 
    mutate(across(id, as.numeric))
}
# bind rows of all products
amazon$data$full <- amazon$data$raw %>% 
  bind_rows(.id = "src")
```

## Quick scraping step-by-step

#### 4Ô∏è‚É£ Endresultat

```{r}
amazon$data$full %>% 
  glimpse()
```

# Explorative Datenanalyse {background-color="#E6002E"}

Im Fokus: üîç {{< fa brands amazon >}} Reviews

## Anzahl der Reviews nach Produkt

#### Kennenlernen des Datensatzes

```{r}
amazon$data$de %>% 
  frq(src)
```

## Anzahl der Reviews nach Produkt im Zeitverlauf

#### Kennenlernen des Datensatzes

```{r}
amazon$data$de %>% 
  ggplot(aes(year, fill = src)) +
  geom_bar() +
  scale_fill_locuszoom() +
  theme_pubr()
```

## Bewertungen der Produkte: Absolute Zahlen

#### Kennenlernen des Datensatzes

```{r}
amazon$data$de %>% 
  mutate(across(rating, as.factor)) %>% 
  ggplot(aes(src, fill = rating)) + 
  geom_bar() +
  scale_fill_brewer(palette = "RdYlGn") +
  theme_pubr()
```

## Bewertungen der Produkte: Kummulierte Anteile

#### Kennenlernen des Datensatzes

```{r}
amazon$data$de %>% 
  mutate(across(rating, as.factor)) %>% 
  ggplot(aes(src, fill = rating)) + 
  geom_bar(position = "fill") +
  scale_fill_brewer(palette = "RdYlGn") +
  theme_pubr()
```

# ‚òï Kursevaluation {background-color="#04316a"}

Bitte nehmen Sie √ºber den QR Code oder folgenden Link an der Evaluation teil:

::: columns
::: {.column width="50%"}
-   <https://www.eva.fau.de>

-   Losung: **QNWVC**
:::

::: {.column width="50%"}
![](images/paste-52FCC08F.png){width="500"}
:::
:::

# Text processing {background-color="#E6002E"}

Kurze Erkl√§rung der Grundbegriffe *Korpus, Token & DFM*

## Vom Korpus bis zum Model

#### Prozess der Textverarbeitung

![](https://miro.medium.com/max/1400/1*DocMTV7nTAomKxcu3m-tyw.webp){fig-align="center"}

## S√§tze -- Token -- Lemma -- POS

#### Vorverarbeitungsschritte f√ºr Textanalyse

::: callout-note
## 1. Satzerkennung

Was gibt's in New York zu sehen?
:::

::: callout-note
## 2. Tokenisierung

was; gibt; \`s; in; new; york; zu; sehen; ?
:::

::: callout-important
## 3. Lemmatisierung

was; geben; \`s; in; new; york; zu; sehen; ?
:::

::: callout-tip
## 4. Part-Of-Speech (POS) Tagging

\>Was`/PWS` \>gibt`/VVFIN` \>'s`/PPER` \>in`/APPR` \>New`/NE` \>York`/NE` \>zu`/PTKZU` \>sehen`/VVINF`
:::

::: notes
Satzerkennung: *Aufl√∂sung der Satzstruktur; Aber: Probleme mit Datumsangaben, Uhrzeit, Abk√ºrzungen, URLS*

Tokenisierung: *Zerteilung in kleinste Einheiten, Abtrennung von Satzzeichen; Fragen: Umgang mit Zeichen, Symbolen, Zahlen, N-Gramme ...*

Definition Lemmatisierung: *Grundform eines Worters, als diejenige Form, unter dem an einen Begriff in einem Nachschlagewerk findet / R√ºckf√ºhrung auf die ‚ÄûVollfrom"*

Definition POS: *Zuordnung von W√∂rtern und Satzzeichen eines Textes zu Wortarten*
:::

## Von BOW zu DFM

#### Bag-of-words (BOW) und Document-Feature-Matrix (DFM)

#### ![](images/bow_dfm_example.png)

::: notes
*Bag-of-Words*-Modell: es z√§hlt lediglich die Worth√§ufigkeit je Dokument, die syntaktischen und grammatikalischen Zusammenh√§nge zwischen einzelnen W√∂rtern werden ignoriert.
:::

# Sentiment Analyse {background-color="#E6002E"}

üëé & üëç?

## The good, the bad and the ugly

#### Grundidee & Ziele und der Sentimentanalyse

-   Auf Basis von speziellen **Wortlisten bzw. Lexika** werden bestimmte **Begriffe ausgez√§hlt**, denen zuvor entweder ein numerischer Wert (Score) oder eine Kategorien (*positiv* oder *negativ)* zugeordnet wurden.

-   Ziel ist die Bestimmung der Polarit√§t (positive/negative Emotion) eines Textes

-   Aber: Wie gut ist die Klassifizierung?

## Grundlagen f√ºr die Analyse

#### Hintergrundinformationen zum Processing & Scoring

-   Die Verarbeitung und Analyse der Daten erfolgt mit dem`quanteda` bzw. `quanteda.textstat` Paket und basiert nur auf dem Korpus der deutschen Reviews

-   Als Sentiment-Grundlage wird SentiWS genutzt

    -   √∂ffentlich verf√ºgbare deutschsprachige Ressource f√ºr die Sentiment Analyse, Opinion Mining und √§hnliche Zwecke

    -   positive und negative Polarit√§t im Intervall \[-1; 1\] f√ºr enthaltene W√∂rter die angegeben, sowie deren Wortart

    -   enth√§lt ungef√§hr 1.650 positive und 1.800 negative Grundformen

::: notes
-   und (falls anwendbar) Flexionsvarianten

-   so dass, inklusive der verschiedenen Flexionsformen, insgesamt etwa 16.000 positive und 18.000 negative Wortformen enthalten sind. SentiWS enth√§lt nicht nur Adjektive und Adverbien, sondern auch Nomen und Verben die Tr√§ger von Sentiment sind
:::

## Erste Ergebnisse

#### Bewertung der verschiedenen Di√§tpillen

```{r}
#| output-location: slide
amazon$data$de_senti %>% 
  ggplot(aes(x = src, y = valence)) + 
  ggdist::stat_halfeye(
    aes(fill = src),
    adjust = .5, 
    width = .6, 
    .width = 0, 
    justification = -.3, 
    point_colour = NA) + 
  geom_boxplot(
    aes(color = src),
    width = .25, 
    outlier.shape = NA
  ) +
  geom_point(
    aes(color = src),
    size = 1.3,
    alpha = .3,
    position = position_jitter(
      seed = 1, width = .1
    )
  ) + 
  coord_cartesian(xlim = c(1.2, NA), clip = "off") +
  scale_color_startrek() +
  scale_fill_startrek() +
  theme_pubr()
```

::: notes
Valence = average valence of document, based on a dictionary whose values have numeric valence scores.
:::

## Ein gro√ües "... , ABER ..."

#### Kritische Anmerkungen zur Sentimentanalyse

::: columns
::: {.column width="50%"}
```{r}
amazon$data$de_senti %>% 
  flat_table(rating, val_fct)
```
:::

::: {.column width="50%"}
-   Probleme der "dictionary method" mit **Negationen** ("nicht gut") und "**Verst√§rkern**" ("sehr gut")

-   Besser: Codierung bzw. **Klassifizierung mit Hilfe von √ºberwachtem maschinellem Lernen**
:::
:::

# Topic Modeling {background-color="#E6002E"}

Grundidee und beispielhafter Ablauf

## Grundidee

#### Quick Introduction to Topic Modeling

-   Verfahren des ***un√ºberwachten maschinellen Lernens***, das sich daher insbesondere zur Exploration und Deskription gro√üer Textmengen eignet

-   Themen werden strikt auf Basis von Worth√§ufigkeiten in den einzelnen Dokumenten vermeintlich objektiv berechnet, ganz ohne subjektive Einsch√§tzungen und damit einhergehenden etwaigen Verzerrungen

-   Bekanntesten dieser Verfahren sind ***LDA*** **(Latent Dirichlet Allocation)** sowie die darauf aufbauenden ***CTM*** **(Correlated Topic Models)** und ***STM*** **(Structural Topic Models)**

## Annahmen {.smaller}

#### Quick Introduction to Topic Modeling

-   Ein Textkorpus besteht aus **`D` Dokumenten** und **`V` W√∂rtern bzw. Terms**

-   Es wird nun angenommen, dass **latente Themen `K`** zu unterschiedlichen **Anteilen in den Dokumenten `D`** vorkommen und **alle W√∂rter `V`** mit unterschiedlicher **Wahrscheinlichkeit zu den `K` Themen geh√∂ren**.

-   Ziel der Verfahren ist die Berechnung zweier Matrizen **`D√óK`** und **`V√óK`**.

    -   Die erste Matrix **`D√óK`** enth√§lt f√ºr jedes einzelne Dokument **`d`** und jedes einzelne Thema **`k`** die Wahrscheinlichkeit, dass **das Thema in diesem Dokument vorkommt (ùöπ).**

    -   Analog enth√§lt **`V√óK`** f√ºr jedes einzelne Wort **`w`** und jedes einzelne Thema **`k`** die Wahrscheinlichkeit, dass d**as jeweilige Wort in diesem Thema vorkommt (ùõÉ).**

::: notes
-   Mit Hilfe dieser Matrizen k√∂nnen die Themen dann beschrieben und interpretiert werden.
    -   So k√∂nnen aus **`V√óK`** die wichtigsten W√∂rter je Thema (d.h., die W√∂rter mit der h√∂chsten konditionalen Wahrscheinlichkeit, zu einem bestimmten Thema kk zu geh√∂ren) abgelesen werden;

    -   mittels **`D√óK`** k√∂nnen Themen Dokumenten und umgekehrt zugeordnet werden, z. B. in dem f√ºr jedes Dokument dd das Thema kk mit der h√∂chsten konditionalen Wahrscheinlichkeit identifiziert wird.
-   Zur Berechnung dieser Matrizen wird sozusagen der umgekehrte Weg gegangen und die Erzeugung der Dokumente als statistischer Prozess beschrieben: ein Dokument wird demnach erzeugt, in dem zuf√§llig Themen aus der zum Dokument zugeh√∂rigen Themenverteilung und W√∂rter aus der den Themen zugeh√∂rigen Wortverteilungen gezogen werden. Hierzu wird das Topic Model zun√§chst mit zuf√§lligen Themen- und Wortverteilungen initialisiert und dann in einem iterativen, algorithmischen Verfahren nach und nach adaptiert, bis es m√∂glichst gut zu den Daten (dem Textkorpus) passt (d.h. die gemeinsame Likelihood der Themen- und Wortverteilungen maximiert wird).^[53](https://bookdown.org/joone/ComputationalMethods/topicmodeling.html#fn53)^
:::

## Structual Topic Modeling im Fokus

#### Idee und Hintergrund von STM

![](images/stm.png){fig-align="center"}

::: notes
ùõ¥ = Topic covariance

x = Prevalende Metadata

gamma = Metadata weights

smaller box = N = number of words in a document

bigger box = M = number of documents in a corpus
:::

## Welches K?

#### Bestimmung der optimalen Themenanzahl

```{r}
#| output-location: slide
amazon$tpm$model_scores %>% 
  transmute(
    k,
    `Lower bound` = lbound,
    Residuals = map_dbl(residual, "dispersion"),
    `Semantic coherence` = map_dbl(semantic_coherence, mean),
    `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%   
  pivot_longer(-k, names_to = "Metric", values_to = "Value") %>% 
  ggplot(
    aes(k, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics")
```

## Kombination aus Koh√§renz & Exklusivit√§t

#### Bestimmung der optimalen Themenanzahl

```{r}
#| output-location: slide
amazon$tpm$model_scores %>%
  select(k, exclusivity, semantic_coherence) %>%
  filter(k %in% c(5, 7, 11)) %>%
  unnest(cols = c(exclusivity, semantic_coherence)) %>%
  mutate(k = as.factor(k)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = k)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence",
       subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity") 
```

## Top Features der Themen

#### Interpretation des Models

```{r}
amazon$stm$mdl %>% labelTopics()
```

## Prevalence der Themen

#### Interpretation des Models

```{r}
#| output-location: slide
amazon$stm$preval$tgt  %>% 
  ggplot(aes(topic, gamma, label = terms, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 1.1, nudge_y = 0.0005, size = 3, color = "white") +
  coord_flip() +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.35), labels = scales::percent) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  labs(x = NULL, y = expression(gamma)) +
  scale_fill_jama()
```

## Validieren, Validieren, Validieren

#### Kritisiche Anmerkungen zum Topic Modeling

> *Automated text analysis methods can substantially reduce the costs and time of analyzing massive collections of political texts. When applied to any one problem, however, the output of the models may be misleading or simply wrong. \[...\] What should be avoided, then, is the blind use of any method without a validation step (Grimmer & Stewart, 2013, S. 5).*

-   Klassifikationsmodell klassifiziert alle Dokumente, ein Diktion√§r spuckt f√ºr jedes Dokument ein Ergebnis aus, ein Topic Model findet immer die vorgegebene Anzahl an Themen.

-   Ob es sich dabei auch um inhaltlich sinnvolle Ergebnisse handelt, **kann und muss durch manuelle Validierungen festgestellt werden.**

-   Moderne Verfahren (z.B. BERT) potentiell besser geeignet f√ºr bestimmte Texte

# Time for questions {background-color="#E6002E" background-image="../../00-images/background_questions.svg"}

# Bis zur n√§chsten Sitzung! {background-color="#E6002E" background-image="../../00-images/background_end_of_session.svg"}

## Literatur

::: {#refs}
:::
