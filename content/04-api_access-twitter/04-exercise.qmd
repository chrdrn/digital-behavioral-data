---
title: "Showcase"
subtitle: "Focus on Twitter API v2"
format: 
  html:
    toc: true
    toc-depth: 4
callout-appearance: simple
execute: 
  cache: true
  eval: true
  echo: true
  message: false
  warning: false
highlight-style: atom-one
editor_options: 
  chunk_output_type: console
bibliography: references.bib
csl: ../apa.csl
---

```{r Setup local OSF environment}
#| echo: false
#| eval: false
#| message: false

# Load packages
pacman::p_load(
  pacman,
  digest,
  here, osfr, fs, 
  tidyverse)

# Get custom functions
source(here("functions/osf_dependencies.R"))
v_digest <- Vectorize(digest::digest)

# Get custom paths
dirs <- osf_dependencies("04-api_access-twitter")

# Get data
osf_download(
  dirs$osf_data_path,
  path = here(dirs$local_data_path),
  recurse = TRUE,
  progress = TRUE
)

# Unzip files
zipfiles <- list.files(
  here(dirs$local_data_path),
  pattern = "*.zip",
  full.names = TRUE
)

map(zipfiles, unzip, exdir = dirs$local_data_path)
```

::: callout-tip
Open this showcase in other interactive and executable environments:

[![Binder RStudio](/img/badge-binder_rstudio.svg)](https://mybinder.org/v2/gh/chrdrn/digital-behavioral-data-binder/HEAD?urlpath=rstudio) [![Binder Jupyter](/img/badge-binder_jupyter.svg){alt="Binder"}](https://mybinder.org/v2/gh/chrdrn/digital-behavioral-data-binder/HEAD) [![Google Colab](https://colab.research.google.com/assets/colab-badge.svg){alt="Google Colab"}](https://colab.research.google.com/github/chrdrn/digital-behavioral-data-binder/blob/main/session_04-showcase_twitter.ipynb)
:::

## Background

Practical application of the *Twitter Academic Research Product Track v2 API endpoint* with the help of the [academictwitteR](https://github.com/cjbarrie/academictwitteR) [@barrie2021] package. Visit the [{{< fa brands github >}} repository](https://github.com/cjbarrie/academictwitteR) of the package for further information.

This version of the Twitter API allows researchers to access larger volumes of Twitter data. For more information on the the Twitter API, including how to apply for access to the Academic Research Product Track, see the [Twitter Developer platform.](https://developer.twitter.com/en/use-cases/do-research/academic-research)

::: callout-note
## Disclaimer

This session was created before Twitter announced changes to its API access:

{{< tweet user=TwitterDev id=1621026986784337922 >}}

As things stand (28.02.2023), there will no longer be free academic research.
:::

This showcase contains two exemplary uses of the API: Analysis of specific hashtags (e.g. [#Karneval](https://twitter.com/search?q=%23karneval&src=typed_query)) and/or specific accounts (e.g. [\@elonmusk](https://twitter.com/elonmusk))

## Preparation

### Load necessary packages

```{r load packages}
library(here) # Easy file path construction
library(academictwitteR) # Collecting the data
library(lubridate) # Work with date-times and time-spans
library(sjmisc) # Collection of miscellaneous utility functions
library(tidyverse) # Preparation of the data
library(quanteda) # Text mining
library(quanteda.textstats) # Text statistics
library(quanteda.textplots) # Visualisation of text data
library(ggthemes) # Custom ggplot themes
library(ggpubr) # Convenience functions for 'ggplot2'-plots
```

```{r}
#| echo: false
library(digest)
v_digest <- Vectorize(digest::digest)
```

## Set personal bearer token

```{r set bearer}
personal_bearer_token <- "INSERT BEARER TOKEN HERE"
```

## Mining tweets: hashtag(s)

### Data collection

```{r mine tweets hashtag}
#| eval: false

get_all_tweets(
    query = "#Karneval", 
    start_tweets = "2022-11-11T00:00:00Z",
    end_tweets = "2022-11-13T12:00:00Z",
    file = "karneval",
    data_path = "data.local/raw_karneval/",
    n = 100000,
    #bearer_token = personal_bearer_token
  )
```

#### Read data from disc

```{r read tweets hashtag}
tweets_karneval <- bind_tweets(
  data_path = here("content/04-api_access-twitter/data.local/raw_karneval"),
  output_format = "tidy") %>% 
    mutate(
    datetime = ymd_hms(created_at),
    date = date(datetime),
    hour = hour(datetime),
    min  = minute(datetime),
    hms  = hms::as_hms(datetime),
    hm   = hms::parse_hm(hms)
  ) 

# Anonymization of potentially sensitive information
tweets_karneval_hash <- tweets_karneval %>% 
  mutate(
     across(c(
       tweet_id:text,
       author_id, conversation_id, in_reply_to_user_id,
       starts_with("user_"),
       starts_with("sourcetweet_")), 
       ~ v_digest(.)))
```

### Data analysis

#### Overview of dataset

```{r hashtag analysis overview}
tweets_karneval_hash %>% 
       glimpse()
```

#### Language of tweets

```{r hashtag analysis language}
frq(tweets_karneval$lang, sort.frq = "desc")
```

#### Tweets over time

```{r hashtag analysis time}
tweets_karneval %>% 
  ggplot(aes(hour)) +
  geom_bar() +
  facet_grid(cols = vars(date)) +
  theme_pubr()
```

#### Most frequent time (HH:MM) of sending tweets

```{r hashtag analysis time peaks}
tweets_karneval %>%
  frq(hm,
      sort.frq = "desc", 
      min.frq = 10)
```

#### User with the most tweets

```{r hashtag analysis frequent users}
tweets_karneval_hash %>% 
  frq(user_username,
      sort.frq = "desc", 
      min.frq = 5)
```

<!--#  The following section is hidden due to DSGVO -->

<!--#  #### Display tweets from most freuquent users -->

```{r hashtag analysis user example 1}
#| echo: false
#| eval: false

tweets_karneval %>% 
  filter(user_username == "Gun17170309") %$% 
  glue::glue(
    "Tweet-ID: {tweet_id} 
    Inhalt:
    {text}\n\n") %>% 
  head()
```

```{r hashtag analysis user example 2}
#| echo: false
#| eval: false

tweets_karneval %>% 
  filter(user_username == "berlinaffaires") %$% 
  glue::glue(
    "Tweet-ID: {tweet_id} 
    Inhalt:
    {text}\n\n") %>% 
  head()
```

## Mining tweets: profile(s)

### Data collection

```{r mine tweets profile}
#| eval: false

get_all_tweets(
    users = c("elonmusk"),
    start_tweets = "2020-11-11T00:00:00Z",
    end_tweets = "2022-11-13T12:00:00Z",
    file = "elonmusk",
    data_path = here("content/04-api_access-twitter/data.local/raw_elonmusk/"),
    n = 100000,
    bearer_token = personal_bearer_token
  )
```

#### Read data from disc

```{r read tweets profile}
tweets_musk <- bind_tweets(
  data_path = here("content/04-api_access-twitter/data.local/raw_elonmusk"),
  # data_path = "data/raw_karneval",
  output_format = "tidy") %>% 
    mutate(
    datetime = ymd_hms(created_at),
    date = date(datetime),
    hour = hour(datetime),
    min  = minute(datetime),
    hms  = hms::as_hms(datetime),
    hm   = hms::parse_hm(hms)
  )
```

### Data analysis

#### Overview of dataset

```{r profile analysis overview}
tweets_musk %>% glimpse
```

#### Tweets over time

```{r profile analysis time}
tweets_musk %>% 
  ggplot(aes(date)) +
  geom_bar() +
  theme_pubr()
```

#### Tweets with the most likes

```{r profile analysis most likes}
tweets_musk %>% 
  filter(is.na(sourcetweet_type)) %>% 
  arrange(-like_count) %>% 
  select(text, created_at, like_count) %>% 
  head(10)
```

#### Tweets with the most retweets

```{r profile analysis most retweets}
tweets_musk %>% 
  filter(is.na(sourcetweet_type)) %>% 
  arrange(-retweet_count) %>% 
  select(text, created_at, retweet_count) %>% 
  head(10)
```

#### Proportion of tweets

```{r profile analysis tweet type}
tweets_musk %>% 
  frq(sourcetweet_type)
```

#### Languate of tweets

```{r profile analysis language}
tweets_musk %>% 
  frq(lang)
```

## Text mining

### Preprocessing

```{r text mining preprocessing}
remove_html <- "&amp;|&lt;|&gt;"

tweets_en <- tweets_musk %>% 
  filter(lang == "en",
         is.na(sourcetweet_type)) %>% 
  select(tweet_id, text, user_username) %>% 
  mutate(text = str_remove_all(text, remove_html))
```

```{r text mining corpus creation}
tweets_en_corpus <- corpus(tweets_en,
                           docid_field = "tweet_id",
                           text_field = "text")
```

```{r text mining tokenization}
tweets_en_tokens <- 
  tokens(tweets_en_corpus,
         remove_punct = TRUE,
         remove_numbers = TRUE,
         remove_symbols = TRUE,
         remove_url = TRUE) %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english"))
```

```{r text mining dfm creation}
tweets_en_dfm <- dfm(tweets_en_tokens)
```

### Analysis

#### Top Hashtags

```{r corpus analysis top hashtags}
tag_dfm <- dfm_select(tweets_en_dfm, pattern = "#*")
toptag <- names(topfeatures(tag_dfm, 50))
head(toptag, 10)
```

#### Top Mentions

```{r corpus analysis top metions}
user_dfm <- dfm_select(tweets_en_dfm, pattern = "@*")
topuser <- names(topfeatures(user_dfm, 50))
head(topuser, 10)
```

#### Exclude Hashtags & Metions

```{r corpus analysis dfm processing}
tweets_en_clean <- tweets_en_dfm %>% 
  dfm_remove(pattern = "@*") %>% 
  dfm_remove(pattern = "#*")
```

#### Top 10 features

```{r corpus analysis top features}
term_freq_en <- textstat_frequency(tweets_en_clean)
head(term_freq_en, n = 10)
```

#### Wordcloud with Top 50 features

```{r corpus analysis top features wordcloud}
textplot_wordcloud(tweets_en_clean, max_words = 50)
```
