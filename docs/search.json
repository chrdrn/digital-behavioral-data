[
  {
    "objectID": "content/01-kick-off/01-slides.html",
    "href": "content/01-kick-off/01-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you find the slides for the kick-off session:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome."
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#herzlich-willkommen",
    "href": "content/01-kick-off/slides/index.html#herzlich-willkommen",
    "title": "Kick-Off",
    "section": "Herzlich willkommen!",
    "text": "Herzlich willkommen!\nVorstellungsrunde\nChristoph Adrian\n\nseit 2016: Research Assistant / PhD Candidate FAU\nM. Sc. Sozialökonomik (FAU)\nB.A. Sozialökonomik (FAU)\n\nForschungs- und Lehrschwerpunkte\n\nComputer-assisted and automated (corpus-based) content analysis (with R)\nSocial media research (Twitter)\nPotential influence of media use on attitude changes"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#und-nun-zu-ihnen",
    "href": "content/01-kick-off/slides/index.html#und-nun-zu-ihnen",
    "title": "Kick-Off",
    "section": "Und nun zu Ihnen!",
    "text": "Und nun zu Ihnen!\nVorstellungsrunde\n\nWie heißen Sie?\nWas und wo haben Sie im Bachelor studiert?\nWas studieren Sie aktuell?\nWelches soziale Netzwerk/Medium nutzen Sie aktuell am häufigsten?\nWelche Erwartungen haben Sie an das Seminar?"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#covid-19-lässt-grüßen",
    "href": "content/01-kick-off/slides/index.html#covid-19-lässt-grüßen",
    "title": "Kick-Off",
    "section": "COVID-19 lässt grüßen",
    "text": "COVID-19 lässt grüßen\nCorona-Regeln im Wintersemester\n\n\n\nUm sich und andere zu schützen bitte wir Sie herzlich:\nTragen Sie weiterhin eine FFP2- oder OP-Maske!\n\nCOVID-o-mat"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#was-verstehen-sie-unter-digital-behavioral-data",
    "href": "content/01-kick-off/slides/index.html#was-verstehen-sie-unter-digital-behavioral-data",
    "title": "Kick-Off",
    "section": "Was verstehen Sie unter Digital Behavioral Data?",
    "text": "Was verstehen Sie unter Digital Behavioral Data?\nInteraktive Session\n\n\nBitte scannen Sie den QR-Code oder nutzen Sie folgenden Link für die Teilnahme an einer kurzen Umfrage:\n\nhttps://www.menti.com/alxiy2f2n6oj\nTemporary Voting Code: 49 29 72 5"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#ergebnis",
    "href": "content/01-kick-off/slides/index.html#ergebnis",
    "title": "Kick-Off",
    "section": "Ergebnis",
    "text": "Ergebnis"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#ein-definitionsversuch-von-dbd",
    "href": "content/01-kick-off/slides/index.html#ein-definitionsversuch-von-dbd",
    "title": "Kick-Off",
    "section": "Ein Definitionsversuch von DBD",
    "text": "Ein Definitionsversuch von DBD\nnach Weller (2021)\n\n\n… fasst eine Vielzahl von möglichen Datenquellen zusammen, die verschiedene Arten von Aktivitäten aufzeichnen\n… können dabei helfen, Meinungen, Verhalten und Merkmale der menschlichen Nutzung digitaler Technologien zu erkennen"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#beispiel-covid-19-mobility-monitor",
    "href": "content/01-kick-off/slides/index.html#beispiel-covid-19-mobility-monitor",
    "title": "Kick-Off",
    "section": "Beispiel: COVID-19 Mobility Monitor",
    "text": "Beispiel: COVID-19 Mobility Monitor"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#beispiel-datenvolumen-einer-internet-minute",
    "href": "content/01-kick-off/slides/index.html#beispiel-datenvolumen-einer-internet-minute",
    "title": "Kick-Off",
    "section": "Beispiel: Datenvolumen einer Internet-Minute",
    "text": "Beispiel: Datenvolumen einer Internet-Minute"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#kernbereiche-von-dbd",
    "href": "content/01-kick-off/slides/index.html#kernbereiche-von-dbd",
    "title": "Kick-Off",
    "section": "Kernbereiche von DBD",
    "text": "Kernbereiche von DBD\nin Anlehnung an Aufteilung der GESIS\n\n\n\n\n\n\n\n\n\n\nUnterschiedliche Heraus- bzw. Anforderungen (je nach Bereich)\nSeminar legt Schwerpunkt auf Datenerhebung, mit kurzen Ausblick auf die anderen Bereiche"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#seminarplan",
    "href": "content/01-kick-off/slides/index.html#seminarplan",
    "title": "Kick-Off",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\nSitzung\nDatum\nThema\nReferent*Innen\n\n\n\n\n1\n26.10.2022\nKick-Off Session\nChristoph Adrian\n\n\n2\n02.11.2022\nDBD: Einführung und Überblick\nChristoph Adrian\n\n\n3\n09.11.2022\nDBD: Datenerhebung\nChristoph Adrian\n\n\n4\n16.11.2022\nAPI-Access (I): Twitter\n\n\n\n5\n23.11.2022\nAPI-Access (II): YouTube\n\n\n\n6\n30.11.2022\nAPI-Access (II): Reddit\n\n\n\n7\n07.12.2022\nWebscraping: TikTok\n\n\n\n8\n14.12.2022\nESM: m-path\n\n\n\n\n\nWEIHNACHTSPAUSE\n\n\n\n9\n12.01.2023\nData Donations\n\n\n\n10\n19.01.2023\nMock-Up-Virtual Environments\n\n\n\n11\n26.01.2023\nOpen Science\n\n\n\n12\n02.02.2023\nGuest Lecture: Linking DBD & Survey data\nJohannes Breuer\n\n\n13\n09.02.2023\nSemesterabschluss & Evaluation\nChristoph Adrian"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#typische-session",
    "href": "content/01-kick-off/slides/index.html#typische-session",
    "title": "Kick-Off",
    "section": "Typische Session",
    "text": "Typische Session\nErst Präsentation, dann Vertiefung\n\n\nPräsentation (ca. 35-45 Min)\n\nUmfasst Ihre Präsentation (inkl. Zeit für Fragen und Diskussionen)\nOption auf weitere, offenere Diskussion im Plenum\n\n\nGroup Activity (ca. 45 - 55 Min)\n\nkleine Gruppenarbeiten zur Vertiefung\nvariiert abhängig vom Thema der jeweiligen Sitzung\nBeispiele:\n\nAnwendung von Tool/Methode mit anschließender kritschen Diskussion\nErstellung eines einfachen Forschungs- oder Analysedesign"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#different-tools-for-different-tasks",
    "href": "content/01-kick-off/slides/index.html#different-tools-for-different-tasks",
    "title": "Kick-Off",
    "section": "Different tools for different tasks",
    "text": "Different tools for different tasks\nOrganisation der Lehre auf einen Blick\n\nMS Teams: wichtige Ankündigungen, asynchrone Unterhaltungen & Fragen zum Kurs\nStudOn: Kursmaterialien, ggf. Beispieldatensätze und Präsentationsaufnahmen\nEmail: persönliche Fragen an Dozent\nZoom: ggf. für Sitzung & Sprechstunde"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#what-is-expected",
    "href": "content/01-kick-off/slides/index.html#what-is-expected",
    "title": "Kick-Off",
    "section": "What is expected",
    "text": "What is expected\nLeistungsanforderungen & Prüfungsleistungen\n\nRegelmäßige Teilnahme an den Sitzungen (max. 2 unentschuldigtes Fehlen)\nEigenständige Auf- und Vorbereitung von Pflichtlektüre (“bestehen”)\nPräsentation (50%)\nSeminarbericht (50%)"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#präsentation",
    "href": "content/01-kick-off/slides/index.html#präsentation",
    "title": "Kick-Off",
    "section": "Präsentation",
    "text": "Präsentation\nUmfang & Inhalt: max. 30 Min.\n\n~ 25 Min.: Zentrale Begriffe, Definitionen und Merkmale der jeweiligen Plattform, Methode und/oder des Tools inklusive kurzem Überblick über Forschung(-sfeld)\n~ 5 Min: Fragen & Diskussion\nZiel: Vorstellung Tool & Beispiel für Forschung(-sdesign)\n\nSprechstunde:\n\nEinreichung vollständiger Präsentationsentwurf mindestens eine Woche + 24 Stunden vor Ihrer Präsentation per E-Mail an christoph.adrian@fau.de\nAusführliches Feedback und Tipps zur Überarbeitung\n\nLiteratur:\n\nSiehe Syllabus & Teams/StudOn (PDFs werden bereitgestellt)"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#ein-kurzer-appell",
    "href": "content/01-kick-off/slides/index.html#ein-kurzer-appell",
    "title": "Kick-Off",
    "section": "Ein kurzer Appell",
    "text": "Ein kurzer Appell\nHinweis zu “Abbruch” bzw. Nicht-Erscheinen\n\nIhre Anmeldung bedeutet üblicherweise eine sehr intensive, aufwändige Betreuung für uns Lehrende\nÄrgerlicherweise gibt es Studierende, die sich anmelden und betreuen lassen, aber dann einfach irgendwann (teilweise sehr kurzfristig) “verschwinden”.\n\n⚠️ Bitte seien Sie sich folgender Dinge bewusst:\n\nDie „Prüfung” bzw. die „Prüfungszeit” bei einem Seminar beginnt mit der Bearbeitungszeit und damit mit der Ausgabe des Themas an die Kandidaten.\n24h Bedenkzeit / Rücktrittsrecht"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#please-state-your-preference",
    "href": "content/01-kick-off/slides/index.html#please-state-your-preference",
    "title": "Kick-Off",
    "section": "Please state your preference",
    "text": "Please state your preference\nVergabe der Präsentationsthemen\n\n\nBitte scannen Sie den QR-Code oder nutzen Sie folgenden Link und wählen Sie Ihr favorisiertes Thema aus\n\nhttps://www.menti.com/alj8fncx1zz4\nTemporary Voting Code: 7312 2522"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#lets-spin-the-wheel",
    "href": "content/01-kick-off/slides/index.html#lets-spin-the-wheel",
    "title": "Kick-Off",
    "section": "Let’s spin the wheel",
    "text": "Let’s spin the wheel\n\n\n\nSession\nDatum\nThema\n\n\n\n\n4\n16.11.2022\nAPI-Access (I): Twitter\n\n\n5\n23.11.2022\nAPI-Access (II): YouTube\n\n\n6\n30.11.2022\nAPI-Access (II): Reddit\n\n\n7\n07.12.2022\nWebscraping: TikTok\n\n\n8\n14.12.2022\nESM: m-path\n\n\n\n\nWEIHNACHTSPAUSE\n\n\n9\n12.01.2023\nData Donations\n\n\n10\n19.01.2023\nMock-Up-Virtual Environments\n\n\n11\n26.01.2023\nOpen Science\n\n\n\n\nIn case of emergency: Wheel of Names"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#lektüre-zwischen-den-sitzungen",
    "href": "content/01-kick-off/slides/index.html#lektüre-zwischen-den-sitzungen",
    "title": "Kick-Off",
    "section": "Lektüre zwischen den Sitzungen",
    "text": "Lektüre zwischen den Sitzungen\nPflichtlektüre\n\npro Woche ein wissenschaftlicher Text als Grundlage für das Thema (Sitzungen 4 - 11)\n\nEinreichung von Fragen zum Text in Teams (spätestens Dienstag Abend)\n\ngilt für alle Kursteilnehmer*Innen\nmind. 1 Verständnisfrage zum Text\nmind. 1 Diskussionsfrage zum Text bzw. Thema insgesamt\nWo? -> in wöchentlicher Teams-Unterhaltung zur jeweligen Sitzung\nWhy? -> wird in der Sitzung genutzt; bereiten Sie Ihre Frage so vor, dass Sie sie in der Sitzung stellen & erklären können\n\nLiteratur:\n\nSiehe Syllabus & Teams/StudOn (PDFs werden bereitgestellt)"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#seminararbeit",
    "href": "content/01-kick-off/slides/index.html#seminararbeit",
    "title": "Kick-Off",
    "section": "Seminararbeit",
    "text": "Seminararbeit\nUmfang & Inhalt (ausführliche Infos im Syllabus)\n\n3500 - 4000 Wörter\nAnfertigung einer Projektskizze für eine wissenschaftliche Arbeit\nVerwendung von mindestens eine der vorgestellten Methoden/Tools\nSozialwissenschaftlicher Fokus\n\n(vorläufige) Deadline: 03.03.2023; 23:59"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#before-we-meet-again",
    "href": "content/01-kick-off/slides/index.html#before-we-meet-again",
    "title": "Kick-Off",
    "section": "Before we meet again",
    "text": "Before we meet again\nHinweise und offene Fragen\n\nGenerell: ausführliche Informationen im Syllabus!\nKursmaterialien: MS Teams vs. StudOn?\nWhy no English?\nIhre Fragen?"
  },
  {
    "objectID": "content/01-kick-off/slides/index.html#literatur",
    "href": "content/01-kick-off/slides/index.html#literatur",
    "title": "Kick-Off",
    "section": "Literatur",
    "text": "Literatur\n\n\nWeller, K. (2021). A short introduction to computational social science and digital behavioral data. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf\n\n\n\n\n\nDigital Behavioral Data"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/02-slides.html",
    "href": "content/02-dbb-introduction_overview/02-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you find the slides for the second session:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome."
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#seminarplan",
    "href": "content/02-dbb-introduction_overview/slides/index.html#seminarplan",
    "title": "Einführung & Überblick",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\nSitzung\nDatum\nThema\nReferent*Innen\n\n\n\n\n1\n26.10.2022\nKick-Off Session\nChristoph Adrian\n\n\n2\n02.11.2022\nDBD: Einführung und Überblick\nChristoph Adrian\n\n\n3\n09.11.2022\nDBD: Datenerhebung\nChristoph Adrian\n\n\n4\n16.11.2022\nAPI-Access (I): Twitter\nFalk\n\n\n5\n23.11.2022\nAPI-Access (II): YouTube\nDenisov\n\n\n6\n30.11.2022\nAPI-Access (II): Reddit\nLandauer\n\n\n7\n07.12.2022\nWebscraping: TikTok\nBrand & Kocher\n\n\n8\n14.12.2022\nESM: m-path\nDörr\n\n\n\n\nWEIHNACHTSPAUSE\n\n\n\n9\n12.01.2023\nData Donations\n\n\n\n10\n19.01.2023\nMock-Up-Virtual Environments\n\n\n\n11\n26.01.2023\nOpen Science\n\n\n\n12\n02.02.2023\nGuest Lecture: Linking DBD & Survey data\nJohannes Breuer\n\n\n13\n09.02.2023\nSemesterabschluss & Evaluation\nChristoph Adrian\n\n\n\n\n▶️"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#kursmaterialien-literatur-etc.",
    "href": "content/02-dbb-introduction_overview/slides/index.html#kursmaterialien-literatur-etc.",
    "title": "Einführung & Überblick",
    "section": "Kursmaterialien, Literatur etc.",
    "text": "Kursmaterialien, Literatur etc.\nKurze Einführung in Teams\n\n\n\nSiehe: Webpage zum Kurs\nSiehe: Syllabus"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#vorschlag-alternativer-seminarplan",
    "href": "content/02-dbb-introduction_overview/slides/index.html#vorschlag-alternativer-seminarplan",
    "title": "Einführung & Überblick",
    "section": "Vorschlag: Alternativer Seminarplan",
    "text": "Vorschlag: Alternativer Seminarplan\n\n\n\nSitzung\nDatum\nThema\nReferent*Innen\n\n\n\n\n1\n26.10.2022\nKick-Off Session\nChristoph Adrian\n\n\n2\n02.11.2022\nDBD: Einführung und Überblick\nChristoph Adrian\n\n\n3\n09.11.2022\nDBD: Datenerhebung\nChristoph Adrian\n\n\n4\n16.11.2022\nAPI-Access (I): Twitter\nFalk\n\n\n5\n23.11.2022\nAPI-Access (II): YouTube\nDenisov\n\n\n6\n30.11.2022\nAPI-Access (II): Reddit\nLandauer\n\n\n7\n07.12.2022\nWebscraping: TikTok\nBrand & Kocher\n\n\n8\n14.12.2022\nExkurs: DBD Analyse mit R\nChristoph Adrian\n\n\n\n\nWEIHNACHTSPAUSE\n\n\n\n9\n12.01.2023\nESM: m-path\nDörr\n\n\n10\n19.01.2023\nTBD\nHofmann & Wierzbicki\n\n\n11\n26.01.2023\nPuffer\n\n\n\n12\n02.02.2023\nGuest Lecture: Linking DBD & Survey data\nJohannes Breuer\n\n\n13\n09.02.2023\nSemesterabschluss & Evaluation\nChristoph Adrian\n\n\n\n\n▶️"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#dbd-was-ist-das-eigentlich",
    "href": "content/02-dbb-introduction_overview/slides/index.html#dbd-was-ist-das-eigentlich",
    "title": "Einführung & Überblick",
    "section": "DBD – Was ist das eigentlich?",
    "text": "DBD – Was ist das eigentlich?\nRückblick auf Definition nach Weller (2021)\n\n… fasst eine Vielzahl von möglichen Datenquellen zusammen, die verschiedene Arten von Aktivitäten aufzeichnen (häufig sogar “nur” als Nebenprodukt)\n… können dabei helfen, Meinungen, Verhalten und Merkmale der menschlichen Nutzung digitaler Technologien zu erkennen\n\n\n\n\nIm Kontext dieses Seminars:\n\nSchwerpunkt: Nutzung und Inhalte von soziale Medien\nComputational Social Science [CSS] Verfahren, z.B. zur Erhebung, Verarbeitung, Auswertung und Präsentation"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#ohne-css-keine-dbd",
    "href": "content/02-dbb-introduction_overview/slides/index.html#ohne-css-keine-dbd",
    "title": "Einführung & Überblick",
    "section": "Ohne CSS keine DBD",
    "text": "Ohne CSS keine DBD\nKurzer Exkurs zur Bedeutung von Computational Social Science\n\n\n\nDefinition (Computational Social Science).\nWe define CSS as the development and application of computational methods to complex, typically large-scale, human (sometimes simulated) behavioral data.” (Lazer et al., 2020)\n\n\n\nhilft dabei …\n\ngenuine digitale Phänomene zu untersuchen\ndigitale Verhaltensdaten zu sammeln und vorzuverarbeiten\nneue Methoden zur Analyse von großen Datensätzen anzuwenden\n\n\nCSS = neues Teilgebiet der Sozialwissenschaften oder neuer “Werkzeugkasten” zur Ergänzung der traditionellen sozialwissenschaftlichen Ansätze"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#und-was-können-wir-damit-untersuchen",
    "href": "content/02-dbb-introduction_overview/slides/index.html#und-was-können-wir-damit-untersuchen",
    "title": "Einführung & Überblick",
    "section": "Und was können wir damit untersuchen?",
    "text": "Und was können wir damit untersuchen?\nBeispiele für & Kategorisierung von untersuchbaren Verhalten & Interaktionen\n\n\n\n\n\nQuelle: Engel et al. (2021)\n\n\n\n\nEinschränkungen\n\nKategorisierung ist Momentaufnahme und nicht überschneidungsfrei\nSelektive Nutzung von bestimmten digitalen Geräten bzw. Funktionen\n\n\n\n\n\nKategorien: Digital/Analog individual/social behavior\nEinige inhärent digitale Verhalten (z.B. Web Searches) bei zunehmender Digitalisierung von analogen Verhalten (z.B. Collaborative Work)\nFehlen digitaler Spurendaten in all diesen Quadranten für bestimmte Personen und bestimmte Verhaltensweisen durch selektive Nutzung digitaler Geräte."
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#verfügbarkeit-als-pluspunkt",
    "href": "content/02-dbb-introduction_overview/slides/index.html#verfügbarkeit-als-pluspunkt",
    "title": "Einführung & Überblick",
    "section": "Verfügbarkeit als Pluspunkt",
    "text": "Verfügbarkeit als Pluspunkt\nDBD als wertvolle Quelle bei aktuellen, sensiblen & unvorhersehbaren Themen\n\nEinsatz besonders Vorteilhaft bei Themen bzw. Untersuchungen …\n\n… für die es schwierig ist, Studienteilnehmer*innen zu rekrutieren\n… bei denen Beobachtungen vorteilhafter sind als Befragungen\n\nBeispiel: Streaming und/oder Mining von Inhalten aus bestehenden digitalen Kommunikationsströmen\n\nZeitnaher als die Erstellung einer Umfrage\nZusätzlicher Nutzen als Archiv bei unvorhersehbaren Ereignissen\n\n\n🔔 –> Beispiele?\n\nMeinung zu Corona auf Basis von Tweets\nWell-being auf Basis von Instagram-Bildern & Texten"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#mehr-daten-durch-technologischen-fortschritt",
    "href": "content/02-dbb-introduction_overview/slides/index.html#mehr-daten-durch-technologischen-fortschritt",
    "title": "Einführung & Überblick",
    "section": "Mehr Daten durch technologischen Fortschritt",
    "text": "Mehr Daten durch technologischen Fortschritt\nBeispiel: Wachsenden Anzahl eingebauter Smartphone-Sensoren\n\n\nGraphik aus Struminskaya et al. (2020)"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#eine-kleine-lobeshymne-auf-dbd",
    "href": "content/02-dbb-introduction_overview/slides/index.html#eine-kleine-lobeshymne-auf-dbd",
    "title": "Einführung & Überblick",
    "section": "Eine kleine Lobeshymne auf DBD",
    "text": "Eine kleine Lobeshymne auf DBD\nZwischenfazit\n\nDigitale Geräte oder Sensoren können sich besser an bestimmte Fakten besser “erinnern” als das menschliche Gedächtnis.\nSensoren sind oft bereits in alltägliche Technologie eingebaut und produzieren digitale Verhaltensdaten als ein “Nebenprodukt”.\nUnaufdringliche Erfassung als potentieller Vorteil bzw. Entlastung für Teilnehmer*Innen\nKombination mit Umfragedaten möglich (und bereichernd!)\n\n\n\nAber:\nZur erfolgreichen Nutzung müssen Forschungsziele & verfügbare Daten in Einklang gebracht, mögliche Biases und methodische Probleme berücksichttigt sowie die Datenqualität evaluiert werden.\n\n\n\n▶️"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#wenn-der-vorteil-zum-nachteil-wird",
    "href": "content/02-dbb-introduction_overview/slides/index.html#wenn-der-vorteil-zum-nachteil-wird",
    "title": "Einführung & Überblick",
    "section": "Wenn der Vorteil zum Nachteil wird",
    "text": "Wenn der Vorteil zum Nachteil wird\nAmbivalenz der Unaufdringlichkeit (Engel et al., 2021)\n\nUnterscheidung zwischen aufdringlichen (z.B. spezielle Research-App & Befragungen) & unaufdringlichen (z.B. Cookies, Browserplugins & APIs) erhobenen Daten\nBewertung und Erwartung an Datensammlung ist abhängig vom Kontext (z.B. Amazon vs. Researchgate)\n\n\n\nDilema:\nEinerseits bereitwillige (oft unwissende) Abgabe der Daten an Konzerne, andererseits häufig Bedenken bezüglich Datenschutz & Privatsphäre bei wissenschaftlichen Studien\n\n\n\n▶️ | 🔔 –> Gründe für Ablehnung: Nutzenorientierung?"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#the-end-of-theory",
    "href": "content/02-dbb-introduction_overview/slides/index.html#the-end-of-theory",
    "title": "Einführung & Überblick",
    "section": "The End of Theory",
    "text": "The End of Theory\n\n\nZur Wichtigkeit von konzipierte Messungen & Designs\n\n\n“Who knows why people do what they do? The point is they do it, and we can track and measure it with unprecedented fidelity. With enough data, the numbers speak for themselves.” (Anderson, 2008)\n\n\n\nWas denken Sie?\n\n\n\n“Size alone does not necessarily make the data better” (boyd & Ellison, 2007)\n\n\n“There are a lot of small data problems that occur in big data [which] don’t disappear because you’ve got lots of the stuff. They get worse.” (Harford, 2014)\n\n\n\n▶️ | 🔔"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#we-need-to-talk-about-biases",
    "href": "content/02-dbb-introduction_overview/slides/index.html#we-need-to-talk-about-biases",
    "title": "Einführung & Überblick",
    "section": "We need to talk about biases",
    "text": "We need to talk about biases\nSpezifische und allgemeine Herausforderungen für die Forschung mit DBD\nHintergrund: (Big) Data ist zunehmend Grundlage für politische Maßnahmen, die Gestaltung von Produkten und Dienstleistungen und für die automatisierte Entscheidungsfindung\n\nHerausforderungen in Bezug auf DBD-Forschung: fehlender Konsens über ein Vokabular oder eine Taxonomie, häufig nur impliziter Bezug in der Forschung\nGenerelle Herausforderung: bias ist ein weit gefasster & in unterschiedlichen Disziplinen genutzter Begriff\n\n\n\n▶️\n“bias” hier im statistischen Sinne\nPunkt2:\n\nconformation bias und andere kognitive Voreingenommenheiten (Croskerry, 2002)\nsystemische, diskriminierende Ergebnisse (Friedman und Nissenbaum, 1996)\nsystemische Schäden (Barocas et al., 2017)"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#know-your-bias",
    "href": "content/02-dbb-introduction_overview/slides/index.html#know-your-bias",
    "title": "Einführung & Überblick",
    "section": "Know your bias!",
    "text": "Know your bias!\nFramework zur Minimierung von Fehlern und Problemen (Olteanu et al., 2019)\n\n\nBeschreibung:\n\nDie Analyse sozialer Daten beginnt mit bestimmten Zielen (Abschnitt 2.1), wie dem Verständnis oder der Beeinflussung von Phänomenen, die für soziale Plattformen spezifisch sind (Typ I) und/oder von Phänomenen, die über soziale Plattformen hinausgehen (Typ II).\nDiese Ziele erfordern, dass die Forschung bestimmte Validitätskriterien erfüllt, die weiter oben beschrieben wurden (Abschnitt 2.2).\nDiese Kriterien können ihrerseits durch eine Reihe von allgemeinen Verzerrungen und Problemen beeinträchtigt werden (Abschnitt 3).\nDiese Herausforderungen können von den Merkmalen der einzelnen Datenplattformen (Abschnitt 4) abhängen - die oft nicht unter der Kontrolle des Forschers stehen - und von den Entscheidungen des Forschungsdesigns entlang einer Datenverarbeitungspipeline (Abschnitte 5 bis 8) - die oft unter der Kontrolle des Forschers stehen.\nPfeile zeigen an, wie sich Komponenten in unserem Rahmenwerk direkt auf andere auswirken"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#worauf-wirkt-die-verzerrung",
    "href": "content/02-dbb-introduction_overview/slides/index.html#worauf-wirkt-die-verzerrung",
    "title": "Einführung & Überblick",
    "section": "Worauf wirkt die Verzerrung?",
    "text": "Worauf wirkt die Verzerrung?\nBeispiele für Forschung von Typ I & II (Olteanu et al., 2019)\n\n\nTyp I: understand/influence phenomena specific to social platforms\n\nDynamik der Verbreitung von “Memes”\nSteigerung der Attraktivität bzw. besonders Features\nVerbesserung der Suchfunktion oder des Empfehlungssystems\n\nTyp II: understand/influence phenomena beyond social platforms\n\nBeschreibung des Einflusses sozialer Medien auf eine politische Wahl.\nNutzung sozialer Daten zur Verfolgung der Entwicklung ansteckender Krankheiten durch Analyse der von Social-Media-Nutzern online gemeldeten Symptomen"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#zu-welchen-problemen-führen-verschiedene-biases",
    "href": "content/02-dbb-introduction_overview/slides/index.html#zu-welchen-problemen-führen-verschiedene-biases",
    "title": "Einführung & Überblick",
    "section": "Zu welchen Problemen führen verschiedene Biases?",
    "text": "Zu welchen Problemen führen verschiedene Biases?\nEinflüsse von Biases auf Datenqualität\n\n\n\nUm diese Probleme in das umfassendere Konzept der Datenqualität einzuordnen, geben wir zunächst einen kurzen Überblick über bekannte Probleme der Datenqualität.\nDatenqualität ist ein vielschichtiges Konzept, typische Elemente sind Genauigkeit, Vollständigkeit, Konsistenz, Aktualität und Zugänglichkeit."
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#datenqualität-data-bias",
    "href": "content/02-dbb-introduction_overview/slides/index.html#datenqualität-data-bias",
    "title": "Einführung & Überblick",
    "section": "Datenqualität & data bias",
    "text": "Datenqualität & data bias\n\n\n\nDefinition (Data bias).\nA systematic distortion in the sampled data that compromises its representativeness.\n\n\n\nPotentielle Probleme\n\nSparsity: Häufig Heavy-Tail-Verteilung, was Analyse am “Kopf” (in Bezug auf häufige Elemente oder Phänomene) erleichtert, am “Schwanz” (wie seltene Elemente oder Phänomene) jedoch erschwert (Baeza-Yates, 2013)\nNoise: Unvollständige, beschädigte, unzuverlässige oder unglaubwürdige Inhalte (boyd & Crawford, 2012; Naveed et al., 2011)\n\nUnterscheidung von “Noise” und “Signal” ist oft unklar und hängt von der Forschungsfrage ab (Salganik, 2018)\n\nOrganische vs gemessene Daten: Fragen zur Repräsentativität (vs. Stichprobenbeschreibung), Kausalität (vs. Korrelation) und Vorhersagegüte"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#im-fokus-population-bias",
    "href": "content/02-dbb-introduction_overview/slides/index.html#im-fokus-population-bias",
    "title": "Einführung & Überblick",
    "section": "Im Fokus: Population Bias",
    "text": "Im Fokus: Population Bias\n\n\n\nDefinition (Population biases).\nSystematic distortions in demographics or other user characteristics between a population of users represented in a dataset or on a platform and some target population.\n\n\n\nPotentielle Probleme\n\nUnterschiedliche Demographien (z.B. Geschlechts-, Alters- & Bildungsgruppen) neigen zu unterschiedlichen sozialen Plattformen1 und nutzen deren Mechanismen2 unterschiedlich\nProxies für Eigenschaften oder demografische Kriterien der Nutzenden sind unterschiedlich verlässlich3\n\n\nHäufig ist der Zusammenhang von Untersuchungspopulation (z.B. in Deutschland lebende Erwachsene) zu Zielpopulation (Erwachsene auf Twitter, die angeben, in DE zu leben) unbekannt.\nBeispiel “Auswirkungen”\n\nkann die (Stichproben)-Repräsentativität beeinträchtigen\n➥ ⚠️externe Validität\nbesonders problematisch für Forschungsarbeiten des Typs II\nkann sich auch auf die Leistung von Algorithmen auswirken, die Rückschlüsse auf die Nutzer ziehen\n➥ ⚠️interne Validität (Typ-I & Typ II)\n\nSchätzung der Geo-Location im Stadt-Land-Spektrum (z.B Johnson et al., 2017)\n\n\n\nSignifikant mehr Twitter-Nutzer (Mislove et al. ,2011), bei Pinterest tendenziell Nutzerinnen überrepräsentiert (Ottoni et al., 2013)Unterschiedliche Twitter-Nutzung in Deutschland (Fokus auf Hashtags) und Korea (Fokus auf Konversationen) (Hong et al., 2011)Angabe über Alumni-Status einer bestimmten Gruppe von Universitäten als Quelle für Verzerrung bei Meinung junger Hochschulabsolvent*Innen zu einem neuen Gesetz (Ruths und Pfeffer, 2014)"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#im-fokus-behavioral-biases",
    "href": "content/02-dbb-introduction_overview/slides/index.html#im-fokus-behavioral-biases",
    "title": "Einführung & Überblick",
    "section": "Im Fokus: Behavioral Biases",
    "text": "Im Fokus: Behavioral Biases\n\n\n\nDefinition (Behavioral biases).\nSystematic distortions in user behavior across platforms or contexts, or across users represented in different datasets.\n\n\n\nPotentielle Probleme\n\nBeeinflussung der Art und Weise, wie Nutzer*Innen miteinander interagieren1\nAuftreten von Selbstselektion2 und Reaktionsverzerrungen34\n\n\nUnterschiede in Bezug auf Nutzerpersönlichkeiten (Hughes et al., 2012), die Verbreitung von Nachrichten (Lerman und Ghosh, 2010) oder den Austausch von Inhalten (Ottoni et al., 2014)\nAuswirkungen:\n\nErgebnisse einer Studie von der gewählten Plattform oder dem Kontext abhängig\n➤ ⚠️externe Validität\nnur Teilweise von population bias abhängig\nbei (expliziten oder impliziten) Annahmen über die Verhaltensmuster der Nutzenden\n➤ ⚠️potentielle Effekte auf Untersuchung von Typ-I & Typ II (z.B. Untersuchung der Bedürfnisse oder Interessen der Nutzenden)\n\nBeispiel “Probleme”:\n\nGesonderte Diskussion von Verhalten, die sich auf die Erstellung von Inhalten durch die Nutzer auswirken (“content production bias”) und solche, die sich auf die Verknüpfungsmuster zwischen Nutzern auswirken (“linking bias”).\nDrei weitere häufige Klassen von Verhaltensverzerrungen betreffen die Interaktionen zwischen Nutzern, die Interaktionen zwischen Nutzern und Inhalten und die Verzerrungen, die dazu führen, dass Nutzer in eine Studienpopulation aufgenommen oder von ihr ausgeschlossen werden.\n\n\nInteraktionsmuster viel spärlicher als explizit erstellte soziale Verbindungen (20 % der Verbindungen haben 80 % der Interaktionen) (Wilson et al., 2009)Passivität trotz Interesse an bestimmten Themen (Gong et al., 2016), aber: Aktivität nicht sichtbar oder Selbstzensur (Wang et al., 2011; Das und Kramer, 2013; Matias et al., 2015)?Nutzer*Innen reden eher über extreme oder positive Erfahrungen als über gewöhnliche oder negative Erfahrungen (Kícíman, 2012; Guerra et al., 2014). (response bias)75 % der Foursquare-Check-ins stimmen nicht mit der tatsächlichen Mobilität der Nutzer übereinstimmen (Zhang et al., 2013)"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#im-fokus-content-production-biases",
    "href": "content/02-dbb-introduction_overview/slides/index.html#im-fokus-content-production-biases",
    "title": "Einführung & Überblick",
    "section": "Im Fokus: Content Production Biases",
    "text": "Im Fokus: Content Production Biases\n\n\n\nDefinition (Content Production Biases)\nBehavioral biases that are expressed as lexical, syntactic, semantic, and structural differences in the content generated by users.\n\n\n\nPotentielle Probleme:\n\nDer Gebrauch der Sprache(n) variiert zwischen und innerhalb von Ländern und Bevölkerungsgruppen.1\nKontextbedingte Faktoren (z.B. zwischenmenschliche Beziehungen) beeinflussen die Art und Weise, wie Benutzer sprechen.2\nDie Inhalte von bekannten oder “erfahrenen” Nutzerinnenunterscheiden sich von denen der normalen Nutzer*innen.3\nUnterschiedliche Bevölkerungsgruppen haben unterschiedliche Neigungen, über bestimmte Themen zu sprechen.4\n\n\nUnterschiede bei nutzergenerierten Inhalten, insbesondere bei Texten, zwischen und innerhalb von demografischen Gruppen\nBeispiel 1: Sprachgebrauchsvariationen je nach Geschlecht, Alter, regionaler Herkunft und politischer Orientierung auf Twitter (Rao et al. (2010)), sowie zwischen ethnischen Gruppen (Blodgett et al., 2016).\nBeispiel 2: Außerdem zeigen Schwartz et al. (2015), dass die zeitliche Ausrichtung von Botschaften (Betonung der Vergangenheit, Gegenwart oder Zukunft) von Faktoren wie Offenheit für neue Erfahrungen, Anzahl der Freunde, Lebenszufriedenheit oder Depression beeinflusst werden kann.\nBeispiel 3: Zafar et al. (2015) zeigen, wie die Konzentration der Stichprobe von Inhalten auf “Experten”-Nutzer die resultierende Stichprobe in Richtung vertrauenswürdigerer und hochwertigerer Inhalte verzerrt.\n\nSaisonale Schwankungen in der sprachlichen Zusammensetzung zwischen verschiedenen Gebiete (Länder, Regionen, Nachbarschaften etc.) (Mocanu et al., 2013)Mütter und Väter verwenden auf Facebook jeweils unterschiedliche Ansprache für Töchter & Söhne, und umgekehrt. (Burke et al., 2013)“Experten”-Nutzer auf Twitter neigen dazu, hauptsächlich Inhalte zu ihrem Fachgebiet zu erstellen (Bhattacharya et al., 2014)Diaz et al. (2016) bei der Auswahl politischer Tweets während der US-Wahlen 2012 fest, dass die Nutzerpopulation eher auf Washington, DC, ausgerichtet war, während Olteanu et al. (2016) feststellten, dass Afroamerikaner eher den Twitter-Hashtag #BlackLivesMatter (über eine große Bewegung zur Rassengleichheit in den USA) verwendeten."
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#im-fokus-linking-bias",
    "href": "content/02-dbb-introduction_overview/slides/index.html#im-fokus-linking-bias",
    "title": "Einführung & Überblick",
    "section": "Im Fokus: Linking Bias",
    "text": "Im Fokus: Linking Bias\n\n\n\nDefinition (Linking Bias)\nBehavioral biases that are expressed as differences in the attributes of networks obtained from user connections, interactions or activity.\n\n\n\nPotentielle Probleme:\n\nNetzattribute123 beeinflussen das Verhalten und die Wahrnehmung der Nutzer und umgekehrt\nVerhaltensbasierte und verbindungsbasierte4 soziale Verbindungen sind unterschiedlich.\nDie Bildung sozialer Online-Netzwerke hängt auch von Faktoren56 außerhalb der sozialen Plattformen ab\n\n\n\nSozialen Netzwerke, die aus beobachteten Mustern in Datensätzen (re)konstruiert werden, können sich grundlegend von den zugrunde liegenden (Offline-)Netzwerken unterscheiden (Schoenebeck, 2013a)\n➤ ⚠️externe Validität ➤ ⚠️ Typ-II & teilweise Typ-I (Fälle, in denen die Interaktions- oder Verknüpfungsmuster der Nutzer mit der Zeit oder dem Kontext variieren)\nwirken sich beispielsweise auf die Untersuchung der Struktur und Entwicklung sozialer Netzwerke, des sozialen Einflusses und von Phänomenen der Informationsverbreitung aus (Wilson et al., 2009; Cha et al., 2010; Bakshy et al., 2012)\nAuf sozialen Plattformen können sie auch zu systematisch verzerrten Wahrnehmungen über Nutzer oder Inhalte führen (Lerman et al., 2016).\n\n\nAnzahl der Follower der Nutzer*Innen (Kıcıman, 2010)Altersspezifische Distanzen in sozialen Netzwerken (Dong et al., 2016)Homophilie - die Tendenz ähnlicher Menschen, miteinander zu interagieren und sich zu verbinden (McPherson et al., 2001)Auf expliziten Verbindungen basierende Netzwerk deutlich dichter war als das auf Nutzerinteraktionen basierende (Wilson et al., 2009)Geografie (Poblete et al., 2011; Scellato et al., 2011)Art und die Dynamik der Offline-Beziehungen die Neigung der Nutzer, soziale Bindungen einzugehen und online zu interagieren (Subrahmanyam et al., 2008; Gilbert und Karahalios, 2009)."
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#im-fokus-temporal-biases",
    "href": "content/02-dbb-introduction_overview/slides/index.html#im-fokus-temporal-biases",
    "title": "Einführung & Überblick",
    "section": "Im Fokus: Temporal Biases",
    "text": "Im Fokus: Temporal Biases\n\n\n\nDefinition (Temporal Biases)\nSystematic distortions across user populations or behaviors over time.\n\n\n\nPotentielle Probleme:\n\nBevölkerungsgruppen, Verhaltensweisen1 und Systeme verändern sich mit der Zeit2.\nSaisonale und periodische Phänomene34.\nPlötzlich auftretende Phänomene (z.B. Anstieg oder Rückgang von besteimmten Aktivitäten5 oder externe Ereignisse wie z.B. Katastrophen) wirken sich auf Populationen, Verhaltensweisen und Plattformen aus.\nDie zeitliche Granularität kann zu feinkörnig sein, um langfristige Phänomene zu beobachten, und zu grobkörnig sein, um kurzlebige Phänomene zu beobachten.\nDatensätze verfallen und verlieren mit der Zeit an Nutzen6.\n\n\nAuswirkunge:\n\n➤ ⚠️ eterne Validität\nBeeinträchtigen Verallgemeinerbarkeit von Beobachtungen im Laufe der Zeit ➤ ⚠️ Typ-I & Typ-II-Forschung problematisch\n\nProbleme:\n\nDie Art und Weise, wie man Datensätze entlang der zeitlichen Achsen aggregiert und abschneidet, wirkt sich darauf aus, welche Art von Mustern beobachtet werden und welche Forschungsfragen beantwortet werden können.\n\n\nSchwankungen in Bezug darauf, wann und wie lange sich die Nutzer*Innen auf bestimmte Themen konzentrieren, was durch aktuelle Trends, saisonale oder periodische Aktivitäten oder sogar durch Lärm ausgelöst werden kann (Radinsky et al., 2012).Drei Arten von zeitlichen Schwankungen: Populationsdrift, Verhaltensdrift und Systemdrift.(Salganik, 2017)Unterschiedliche zeitliche Kontexte (Tag vs. Nacht, Wochentag vs. Wochenende) verändert die Form der abgeleiteten Nachbarschaftsgrenzen eografisch verorteter Tweets (Kıcıman et al., 2014).Zusammenhänge zwischen der Stimmung von Tweets und Schlafzyklen und Saisonalität (Golder und Macy, 2011)Einführung einer neuen Plattformfunktion resutliert in plötzlichen Anstieg der Aktivität (Malik und Pfeffer, 2016)Nutzer ihre Inhalte und Konten löschen (Liu et al., 2014; Gillespie, 2015)"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#im-fokus-redundancy",
    "href": "content/02-dbb-introduction_overview/slides/index.html#im-fokus-redundancy",
    "title": "Einführung & Überblick",
    "section": "Im Fokus: Redundancy",
    "text": "Im Fokus: Redundancy\n\n\n\nDefinition (Redundancy)\nSingle data items that appear in the data in multiple copies, which can be identical (duplicates), or almost identical (near duplicates).\n\n\n\nPotentielle Probleme:\n\nLexikalische (z. B. Duplikate, erneute Tweets, erneut geteilte Inhalte) und semantische (z. B. Beinahe-Duplikate oder dieselbe Bedeutung, aber anders geschrieben) Redundanz macht oft einen erheblichen Teil der Inhalte aus und kann sowohl innerhalb als auch zwischen Datensätzen auftreten.\nWeitere Quellen für inhaltliche Redundanz sind häufig nicht-menschliche Konten, wie z.B.\n\nein und dieselbe Person, die von mehreren Konten oder Plattformen aus postet (z. B. Spam),\nmehrere Nutzer, die vom selben Konto aus posten (z. B. Konten von Organisationen),\nmehrere Personen, die denselben Inhalt posten oder erneut posten (z. B. das Posten von Zitaten, Memes oder anderen Arten von Inhalten).\n\n\n\nAuswirkunge:\n\nRedundanz kann, wenn sie nicht berücksichtigt wird, sowohl die interne als auch die ökologische/externe Validität der Forschung beeinträchtigen, und zwar sowohl in der Forschung vom Typ I als auch vom Typ II (Abschnitt 2.1). Sie kann sich negativ auf den Nutzen von Instrumenten auswirken (Radlinski et al., 2011) und die Quantifizierung von Phänomenen in den Daten verzerren.\n\nProbleme:\n\nDie Art und Weise, wie man Datensätze entlang der zeitlichen Achsen aggregiert und abschneidet, wirkt sich darauf aus, welche Art von Mustern beobachtet werden und welche Forschungsfragen beantwortet werden können.\nDies kann manchmal die Ergebnisse verzerren, aber Redundanz kann auch ein Signal an sich sein, z. B. kann das erneute Posten ein Signal für Wichtigkeit sein."
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#sneak-preview-in-die-nächste-sitzung",
    "href": "content/02-dbb-introduction_overview/slides/index.html#sneak-preview-in-die-nächste-sitzung",
    "title": "Einführung & Überblick",
    "section": "Sneak Preview in die nächste Sitzung",
    "text": "Sneak Preview in die nächste Sitzung\nDatenerhebung im Fokus\n\n\n\nUm diese Probleme in das umfassendere Konzept der Datenqualität einzuordnen, geben wir zunächst einen kurzen Überblick über bekannte Probleme der Datenqualität.\nDatenqualität ist ein vielschichtiges Konzept, typische Elemente sind Genauigkeit, Vollständigkeit, Konsistenz, Aktualität und Zugänglichkeit."
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#erweiterung-des-blickwinkels",
    "href": "content/02-dbb-introduction_overview/slides/index.html#erweiterung-des-blickwinkels",
    "title": "Einführung & Überblick",
    "section": "Erweiterung des Blickwinkels",
    "text": "Erweiterung des Blickwinkels\nEthische Erwägungen bei DBD-Forschung\nAus öffentlicher Zugänglich- bzw. Verfügbarkeit von Daten leitet sich nicht automatisch ethische Verwertbarkeit ab (boyd & Crawford, 2012; Zimmer, 2010)\n\nVerletzung der Privatsphäre der Nutzer (Goroff, 2015)\nErmöglichung von rassischem, sozioökonomischem oder geschlechtsspezifischem Profiling (Barocas & Selbst, 2016)\n\nNegative Beispiele\n\nFacebook contagion experiment (2012-2014): Feeds von Nutzer*Innen so manipulierten, dass sie je nach den geäußerten Emotionen mehr oder weniger von bestimmten Inhalten enthielten (Kramer et al., 2014)\nEncore-Forschungsprojekt: Messung der Internetzensur auf der ganzen Welt, bei der Webbrowser angewiesen wurden, zu versuchen, sensible Webinhalte ohne das Wissen oder die Zustimmung der Nutzer herunterzuladen (Burnett & Feamster, 2014)\n\n\nHintergrund:\n\nEthische Fragen bisher epistemische Bedenken (Verwendung von nicht schlüssigen oder fehlgeleiteten Beweisen), jetzt normativ Bedenken (Folgen der Forschung)\nForschung grundsätzlich in vielen Ländern gesetztlich geregelt\n\nNegativbeispiele:\n\nFacebook contagion experiment: Das Experiment wurde als ein Eingriff kritisiert, der den emotionalen Zustand von ahnungslosen Nutzern beeinflusste, die keine Zustimmung zur Teilnahme an der Studie gegeben hatten (Hutton und Henderson, 2015a).\nEncore-Forschngsprojekt: Menschen in einigen Ländern durch diese Zugriffsversuche möglicherweise gefährdet wurden\n\nFolgende Abschnitte:\n\nzentrales Spannungsverhältnis in der Forschungsethik digitaler Daten dargestellt.\nAnschließend wird die Diskussion spezifischer ethischer Probleme in der Sozialdatenforschung im Hinblick auf drei grundlegende Kriterien gegliedert, die im Belmont-Bericht (Ryan et al., 1978), einem grundlegenden Werk zur Forschungsethik, vorgebracht wurden: Autonomie (Abschnitt 9.2), Wohltätigkeit (Abschnitt 9.3) und Gerechtigkeit (Abschnitt 9.4)."
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#ein-schmaler-grat",
    "href": "content/02-dbb-introduction_overview/slides/index.html#ein-schmaler-grat",
    "title": "Einführung & Überblick",
    "section": "Ein schmaler Grat",
    "text": "Ein schmaler Grat\nForschungethik bei digitalen Daten\nHintergrund: Die Herausforderung besteht in der Kombination von zwei extremen Sichtweisen, der Betrachtung der Forschung mit sozialen Daten als “klinische” Forschung oder als Computerforschung\n\nDie Sozialdatenforschung unterscheidet sich von klinischen Versuchen.\nEthische Entscheidungen in der Sozialdatenforschung müssen gut überlegt sein, da oft sind mehrere Werte betroffen, die miteinander in Konflikt stehen können\n\n\nHintergrund:\n\nDie Sozialdatenforschung ähnelt klinischen Versuchen und anderen Experimenten am Menschen in ihrer Fähigkeit, Menschen zu schaden, und sollte daher auch als solche reguliert werden\ndie Sozialdatenforschung ähnelt der sonstigen Computerforschung, die sich traditionell auf Methoden, Algorithmen und den Aufbau von Systemen konzentriert, mit minimalen direkten Auswirkungen auf Menschen.\n\nPunkt 2: Schäden, die die üblichen Arten der Sozialdatenforschung ( z. B. die Verletzung der Privatsphäre oder der Anblick verstörender Bilder)verursachen können, oft nicht mit Schäden von klinischen Versuchen gleichzusetzen\nPunkt 3: Datenanalyse beispielsweise erforderlich sein, um wichtige Dienste bereitzustellen, und es sollten Lösungen erwogen werden, die ein Gleichgewicht zwischen Datenschutz und Genauigkeit herstellen (Goroff, 2015)."
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#achtung-der-individuellen-autonomie",
    "href": "content/02-dbb-introduction_overview/slides/index.html#achtung-der-individuellen-autonomie",
    "title": "Einführung & Überblick",
    "section": "Achtung der individuellen Autonomie",
    "text": "Achtung der individuellen Autonomie\nDiskussion der Informierte Zustimmung als Indikator autonomer Entscheidung\nDie Einwilligung nach Aufklärung setzt voraus, dass\n\ndie Forscher*Innen den potenziellen Teilnehmenden alle relevanten Informationen offenlegen;\ndie potenziellen Teilnehmenden in der Lage sind, diese Informationen zu bewerten;\ndie potenziellen Teilnehmenden freiwillig entscheiden können, ob sie teilnehmen wollen oder nicht;\ndie Teilnehmenden den Forschernden ihre ausdrückliche Erlaubnis erteilen, häufig in schriftlicher Form; und\ndie Teilnehmende die Möglichkeit haben, ihre Einwilligung jederzeit zurückzuziehen.\n\n\nPotentielle Probleme\n\nDie Zustimmung von Millionen von Nutzern einzuholen ist unpraktisch.\nDas öffentliche Teilen von Inhalten im Internet bedeutet nicht unbedingt eine Zustimmung zur Forschung1.\nDie Nutzungsbedingungen sozialer Plattformen stellen möglicherweise keine informierte Zustimmung zur Forschung dar.\n\n\nBeispiel 1:\n\nHäufig keine Zustimmung bei Studien mit Daten von Millionen von Social-Media-Nutzern (Zimmer, 2010; Hutton und Henderson, 2015a)\nObwohl die Einholung der Zustimmung oft als unpraktisch angesehen wird (Boyd und Crawford, 2012), gibt es Bemühungen, Methoden zur Einholung der Zustimmung zu entwickeln, die den Aufwand für die Teilnehmer möglichst gering halten (Hutton und Henderson, 2015a).\n\n\nDatenschutzpräferenzen hängen häufig von Lebensumständen ab (Crawford und Finn, 2014)"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#wohltätigkeit-und-unschädlichkeit-als-ziel",
    "href": "content/02-dbb-introduction_overview/slides/index.html#wohltätigkeit-und-unschädlichkeit-als-ziel",
    "title": "Einführung & Überblick",
    "section": "Wohltätigkeit und Unschädlichkeit als Ziel",
    "text": "Wohltätigkeit und Unschädlichkeit als Ziel\nBewertung von Risken & Nutzen\nHintergrund: Nicht nur Fokus auf den Nutzen der Forschung, sondern auch auf die möglichen Arten von Schäden, die betroffenen Gruppen und die Art und Weise, wie nachteilige Auswirkungen getestet werden können .(Sweeney, 2013)\n\nPotentielle Probleme\n\nDaten über Einzelpersonen können ihnen schaden, wenn sie offengelegt werden12.\nForschungsergebnisse können verwendet werden, um Schaden anzurichten3.\n“Dual-Use”- und Sekundäranalysen sind in der Sozialdatenforschung immer häufiger anzutreffen4.\n\n\nDie Forschung zu sozialen Daten wird mit bestimmten Arten von Schäden in Verbindung gebracht, von denen die Verletzung der Privatsphäre vielleicht die offensichtlichste ist (Zimmer, 2010; Crawford und Finn, 2014).\nBeispiel 1: Einige prominente Beispiele sind die Datenpanne bei Ashley Madison im Jahr 2015, bei der einer Website, die sich als Dating-Netzwerk für betrügerische Ehepartner anpreist, Kontoinformationen (einschließlich der vollständigen Namen der Nutzer) gestohlen und online gestellt wurden (Thomsen, 2015), sowie die jüngsten Datenpannen bei Facebook, bei denen Hunderte Millionen von Datensätzen mit Kommentaren, Likes, Reaktionen, Kontonamen, App-Passwörtern und mehr öffentlich gemacht wurden.\n\nStalking, Diskriminierung, Erpressung oder Identitätsdiebstahl (Gross und Acquisti, 2005).Zu lange Archivierung personenbezogener Daten oder die öffentliche Freigabe schlecht anonymisierter Datensätze kann zu Verletzungen der Privatsphäre führen, da diese Daten mit anderen Quellen kombiniert werden können, um Erkenntnisse über Personen ohne deren Wissen zu gewinnen (Crawford und Finn, 2014; Goroff, 2015; Horvitz und Mulligan, 2015)Abgesehen von der Tatsache, dass aus sozialen Daten gezogene Rückschlüsse in vielerlei Hinsicht falsch sein können, wie in dieser Studie hervorgehoben wird, können zu präzise Rückschlüsse dazu führen, dass Menschen in immer kleinere Gruppen eingeteilt werden können (Barocas, 2014).Daten, Instrumente und Schlussfolgerungen, die für einen bestimmten Zweck gewonnen wurden, für einen anderen Zweck verwendet werden (Hovy und Spruit, 2016; Benton et al., 2017)"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#faire-verteilung-von-risiken-nutzen",
    "href": "content/02-dbb-introduction_overview/slides/index.html#faire-verteilung-von-risiken-nutzen",
    "title": "Einführung & Überblick",
    "section": "Faire Verteilung von Risiken & Nutzen",
    "text": "Faire Verteilung von Risiken & Nutzen\nRecht & Gerechtigkeit\nAnnahme: Es ist von Anfang an bekannt, wer durch die Forschung belastet wird und wer von den Ergebnissen profitieren wird.\n\nPotentielle Probleme\n\nDie digitale Kluft kann das Forschungsdesign beeinflussen1 (Stichwort: WEIRD Samples)\nAlgorithmen und Forschungsergebnisse können zu Diskriminierung führen.\nForschungsergebnisse sind möglicherweise nicht allgemein zugänglich2.\nNicht alle Interessengruppen werden über die Verwendung von Forschungsergebnissen konsultiert3.\n\n\n🔔-> Plausibilität der Annahme?\n\nData divide: mangelnde Verfügbarkeit von hochwertigen Daten über Entwicklungsländer und unterprivilegierte Gemeinschaften (Cinnamon und Schuurman, 2013).Idealerweise sollten die Menschen Zugang zu den Forschungsergebnissen und Artefakten haben, die aus der Untersuchung ihrer persönlichen Daten entstanden sind (Gross und Acquisti, 2005; Crawford und Finn, 2014).In die Überlegungen darüber, wie, für wen und wann Forschungsergebnisse umgesetzt werden, sollten diejenigen einbezogen werden, die möglicherweise betroffen sind oder deren Daten verwendet werden (Costanza-Chock, 2018; Design Justice, 2018; Green, 2018)"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#zwei-trends-drei-fragen-vier-empfehlungen",
    "href": "content/02-dbb-introduction_overview/slides/index.html#zwei-trends-drei-fragen-vier-empfehlungen",
    "title": "Einführung & Überblick",
    "section": "Zwei Trends, Drei Fragen, Vier Empfehlungen",
    "text": "Zwei Trends, Drei Fragen, Vier Empfehlungen\nZusammenfassung und Ausblick\nTrend 1: Eine zunehmende Skepsis gegenüber einfachen Antworten\n\nWie einstehen die Daten, was enthalten sie tatsächlich und wie die Arbeitsdatensätze zusammengestellt?\nWird deutlich, was was ausgewertet wird?\nWird die Verwendung von vorgefertigten Datensätzen und Modellen des maschinellen Lernens hinterfragt?\n\n\nTrend 2: Vom Aufwerfen von Bedenken über soziale Daten zum Umgang mit ihnen. 4 Empfehlungen:\n\nDetaillierte Dokumentation und kritische Prüfung der Datensatz- und Modellerstellung\nDBD-Studien auf verschiedene Plattformen, Themen, Zeitpunkte und Teilpopulationen auszuweiten, um festzustellen, wie sich die Ergebnisse beispielsweise in verschiedenen kulturellen, demografischen und verhaltensbezogenen Kontexten unterscheiden\nTransparenzmechanismen zu schaffen, die es ermöglichen, soziale Software zu überprüfen und Verzerrungen in sozialen Daten an der Quelle zu evaluieren\nForschung zu diesen Leitlinien, Standards, Methoden und Protokollen auszuweiten und ihre Übernahme zu fördern.\n\nSchließlich gibt es angesichts der Komplexität der inhärent kontextabhängigen, anwendungs- und bereichsabhängigen Verzerrungen und Probleme in sozialen Daten und Analysepipelines, die in diesem Papier behandelt werden, keine Einheitslösungen - bei der Bewertung und Bekämpfung von Verzerrungen ist Nuancierung entscheidend."
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#vielen-dank-für-ihre-fragen",
    "href": "content/02-dbb-introduction_overview/slides/index.html#vielen-dank-für-ihre-fragen",
    "title": "Einführung & Überblick",
    "section": "Vielen Dank für Ihre Fragen!",
    "text": "Vielen Dank für Ihre Fragen!\n\nWie wird mit ungenauen, verzerrten und/oder unvollständigen digitalen Verhaltensdaten umgegangen? Was passiert mit ihnen?\n\n\nBereinigen –> Validieren –> Bereinigen –> Validieren …\n\n\n\nWas versteht man unter der Herausforderung des Privacy-Utility-Trade-Off und wie kann dieses “Problem” bewältigt werden?\n\n\nLeider kein Zugang zur Quelle, Antwort “verschoben”\n\n\nWe consider a privacy-utility trade-off encountered by users who wish to disclose some information to an analyst, that is correlated with their private data, in the hope of receiving some utility."
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#was-denken-sie-1",
    "href": "content/02-dbb-introduction_overview/slides/index.html#was-denken-sie-1",
    "title": "Einführung & Überblick",
    "section": "Was denken Sie?",
    "text": "Was denken Sie?\n\n\nNutzer sind sich oftmals nicht bewusst, dass ihre Posts/Tweets zu Forschungszwecken verwendet werden.\nSollten im Sinne der Transparenz von Vornherein mehr Informationen zur Verwendung von Daten gegeben werden? Würde dieses tatsächliche Wissen etwas am Verhalten/Akzeptanz der Datenerhebung der Nutzer ändern?\n\n\n\n\nDa die Daten der Nutzer meist ohne Ihr aktives Wissen darüber erhoben werden ist ihr Verhalten sehr nah an der Realität. Würde eine bessere Information und Transparenz über das mitschreiben der Daten Ihr Verhalten beeinflussen und verfälschen? Kann dies bereits durch Benachrichtigungen wie “Coockies” der Fall sein?"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#lets-discuss",
    "href": "content/02-dbb-introduction_overview/slides/index.html#lets-discuss",
    "title": "Einführung & Überblick",
    "section": "Let’s discuss!",
    "text": "Let’s discuss!\n\nEs gibt Individuen, die sich nicht im digitalen Raum bewegen und somit in den Daten nicht erfasst werden können. Dadurch können bestimmte Gruppen nicht untersucht werden, was zu verzerrten Daten führt. Welche Möglichkeiten hat man dieses Problem zu umgehen.\n\n\nWie würden Sie (methodisch) vorgehen?\n\n\n\nIst es das eine Frage der Methodik oder des Forschungsdesigns?"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#lets-discuss-1",
    "href": "content/02-dbb-introduction_overview/slides/index.html#lets-discuss-1",
    "title": "Einführung & Überblick",
    "section": "Let’s discuss!",
    "text": "Let’s discuss!\n\nWenn Unternehmen bei der Verwendung unserer Verhaltensdaten Profit erwirtschaften, sollten wir dann nicht auch etwas daran verdienen?\n\n\nWas denken Sie?\nWie könnten/sollte ein potentielles Bezahlungssystem aussehen?"
  },
  {
    "objectID": "content/02-dbb-introduction_overview/slides/index.html#literatur",
    "href": "content/02-dbb-introduction_overview/slides/index.html#literatur",
    "title": "Einführung & Überblick",
    "section": "Literatur",
    "text": "Literatur\n\n\nAnderson, C. (2008). The end of theory: The data deluge makes the scientific method obsolete. Wired. https://www.wired.com/2008/06/pb-theory/\n\n\nBaeza-Yates, R. A. (2013). Big data or right data.\n\n\nBarocas, S., & Selbst, A. D. (2016). Big Data’s Disparate Impact. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2477899\n\n\nboyd, danah m., & Crawford, K. (2012). CRITICAL QUESTIONS FOR BIG DATA: Provocations for a cultural, technological, and scholarly phenomenon. Information, Communication & Society, 15(5), 662–679. https://doi.org/10.1080/1369118X.2012.678878\n\n\nboyd, danah m., & Ellison, N. B. (2007). Social Network Sites: Definition, History, and Scholarship. Journal of Computer-Mediated Communication, 13(1), 210–230. https://doi.org/10.1111/j.1083-6101.2007.00393.x\n\n\nBurnett, S., & Feamster, N. (2014). Encore: Lightweight measurement of web censorship with cross-origin requests. https://doi.org/10.48550/ARXIV.1410.1211\n\n\nEngel, U., Quan-Haase, A., Liu, S. X., & Lyberg, L. (2021). Digital trace data (1st ed., pp. 100–118). Routledge. https://doi.org/10.4324/9781003024583-8\n\n\nGoroff, D. L. (2015). Balancing privacy versus accuracy in research protocols. Science, 347(6221), 479–480. https://doi.org/10.1126/science.aaa3483\n\n\nHarford, T. (2014). Big data: A big mistake? Significance, 11(5), 14–19. https://doi.org/10.1111/j.1740-9713.2014.00778.x\n\n\nKramer, A. D. I., Guillory, J. E., & Hancock, J. T. (2014). Experimental evidence of massive-scale emotional contagion through social networks. Proceedings of the National Academy of Sciences, 111(24), 8788–8790. https://doi.org/10.1073/pnas.1320040111\n\n\nLazer, D. M. J., Pentland, A., Watts, D. J., Aral, S., Athey, S., Contractor, N., Freelon, D., Gonzalez-Bailon, S., King, G., Margetts, H., Nelson, A., Salganik, M. J., Strohmaier, M., Vespignani, A., & Wagner, C. (2020). Computational social science: Obstacles and opportunities. Science, 369(6507), 1060–1062. https://doi.org/10.1126/science.aaz8170\n\n\nNaveed, N., Gottron, T., Kunegis, J., & Alhadi, A. C. (2011). the 20th ACM international conference. 183. https://doi.org/10.1145/2063576.2063607\n\n\nOlteanu, A., Castillo, C., Diaz, F., & Kıcıman, E. (2019). Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2, 13. https://doi.org/10.3389/fdata.2019.00013\n\n\nSalganik, M. J. (2018). Bit by bit: Social research in the digital age. Princeton University Press.\n\n\nStruminskaya, B., Lugtig, P., Keusch, F., & Höhne, J. K. (2020). Augmenting Surveys With Data From Sensors and Apps: Opportunities and Challenges. Social Science Computer Review, 089443932097995. https://doi.org/10.1177/0894439320979951\n\n\nSweeney, L. (2013). Discrimination in Online Ad Delivery: Google ads, black names and white names, racial discrimination, and click advertising. Queue, 11(3), 10–29. https://doi.org/10.1145/2460276.2460278\n\n\nWeller, K. (2021). A short introduction to computational social science and digital behavioral data. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf\n\n\nZimmer, M. (2010). “But the data is already public”: on the ethics of research in Facebook. Ethics and Information Technology, 12(4), 313–325. https://doi.org/10.1007/s10676-010-9227-5\n\n\n\n\n\nDigital Behavioral Data"
  },
  {
    "objectID": "content/03-dbd-data_collection/03-slides.html",
    "href": "content/03-dbd-data_collection/03-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you find the slides for the third session:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome."
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#seminarplan",
    "href": "content/03-dbd-data_collection/slides/index.html#seminarplan",
    "title": "Datenerhebung im Fokus",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\nSitzung\nDatum\nThema\nReferent*Innen\n\n\n\n\n1\n26.10.2022\nKick-Off Session\nChristoph Adrian\n\n\n2\n02.11.2022\nDBD: Einführung und Überblick\nChristoph Adrian\n\n\n3\n09.11.2022\nDBD: Datenerhebung\nChristoph Adrian\n\n\n4\n16.11.2022\nAPI-Access (I): Twitter\nFalk\n\n\n5\n23.11.2022\nAPI-Access (II): YouTube\nDenisov\n\n\n6\n30.11.2022\nAPI-Access (II): Reddit\nLandauer\n\n\n7\n07.12.2022\nWebscraping: TikTok\nBrand & Kocher\n\n\n8\n14.12.2022\nESM: m-path\nDörr\n\n\n\n\nWEIHNACHTSPAUSE\n\n\n\n9\n12.01.2023\nData Donations\n\n\n\n10\n19.01.2023\nMock-Up-Virtual Environments\n\n\n\n11\n26.01.2023\nOpen Science\n\n\n\n12\n02.02.2023\nGuest Lecture: Linking DBD & Survey data\nJohannes Breuer\n\n\n13\n09.02.2023\nSemesterabschluss & Evaluation\nChristoph Adrian\n\n\n\n\n▶️"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#brace-yourself-presentations-are-coming",
    "href": "content/03-dbd-data_collection/slides/index.html#brace-yourself-presentations-are-coming",
    "title": "Datenerhebung im Fokus",
    "section": "Brace yourself, presentations are coming!",
    "text": "Brace yourself, presentations are coming!\nAllgemeine Hinweise rund um die Präsentation\n\n📑 Denken Sie bitte an die Fragen zur Pflichtlektüre!\n\n🕦 Office Hours bzw. Feedbackgespräch nach dem Kurs\n\n🔍 Info zu den Evaluationskriterien\n\n\n❓ Fragen\n\n\n▶️\nKriterien:\n\nPräsentationsstil (Roter Faden) –> Verständlichkeit & Konsistenz\nPräsentationsfolien (Funktionalität über Design)\nBearbeitung des Arbeitsauftrages\nSelbstständigkeit & Tiefer der Bearbeitung\nEinleitung, Abschluss & Diskussion\nExtra: Bezug zur Kommunikationswissenschaft"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#die-power-von-social-sensing",
    "href": "content/03-dbd-data_collection/slides/index.html#die-power-von-social-sensing",
    "title": "Datenerhebung im Fokus",
    "section": "Die Power von Social Sensing",
    "text": "Die Power von Social Sensing\nForschungsdesign zur Erhebung digitaler Verhaltensdaten (Flöck & Sen, 2022)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Zukunft: Linking"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#mit-fokus-auf-die-platform",
    "href": "content/03-dbd-data_collection/slides/index.html#mit-fokus-auf-die-platform",
    "title": "Datenerhebung im Fokus",
    "section": "Mit Fokus auf die Platform",
    "text": "Mit Fokus auf die Platform\nForschungsdesign zur Erhebung digitaler Verhaltensdaten (Flöck & Sen, 2022)"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#online-plattformen-prägen-die-gesellschaft",
    "href": "content/03-dbd-data_collection/slides/index.html#online-plattformen-prägen-die-gesellschaft",
    "title": "Datenerhebung im Fokus",
    "section": "Online-Plattformen prägen die Gesellschaft",
    "text": "Online-Plattformen prägen die Gesellschaft\nGründe für den Fokus auf Onlineplattformen (Ulloa, 2021)\n\n\nvermitteln & formen menschliche Kommunikation (z.B. Tweet mit 280 Zeichen)\npolitische (Miss-)Nutzung\nGatekeeper für Informationen (z.B. “Dr.Google”)\ntägliche algorithmische Empfehlungen und Werbung: Nachrichten, Produkte, Jobangebote, Bewerbungen, Versicherungen, Hotels, …"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#aber-welche-und-warum",
    "href": "content/03-dbd-data_collection/slides/index.html#aber-welche-und-warum",
    "title": "Datenerhebung im Fokus",
    "section": "Aber welche? Und warum?",
    "text": "Aber welche? Und warum?\nDenken Sie über Ihre Forschungsfrage nach … (Ulloa, 2021)\n\nwelche Population ist vertreten?\nwelche Arten von Interaktionen sind wichtig? (z. B.: eins zu eins oder eins zu vielen)\nWelche Interaktionsregeln sind wichtig?\nBietet die Plattform Zugang zu den benötigten Daten?\n\nWenn nicht, gibt es alternative Weg um an die Daten zu gelangen?\nWenn ja, ist dies legal/ethisch?\n\n\n\n\nABER:\nBeachten Sie die der Art und Weise, wie Sie die Daten sammeln!\n\n\n\n▶️"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#possibilities-over-possibilities",
    "href": "content/03-dbd-data_collection/slides/index.html#possibilities-over-possibilities",
    "title": "Datenerhebung im Fokus",
    "section": "Possibilities over possibilities",
    "text": "Possibilities over possibilities\nBeispiele für verschiedene Datenquellen\n\nAutomatisiertes Browsing\nDaten-Spenden\nDirekter Zugang\nRepositories\n\n\nWeb-APIs\nWeb Scraping / Crawling\n\n\nWeb-Tracking\n…"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#bedeutung-ist-eine-frage-der-disziplin",
    "href": "content/03-dbd-data_collection/slides/index.html#bedeutung-ist-eine-frage-der-disziplin",
    "title": "Datenerhebung im Fokus",
    "section": "Bedeutung ist eine Frage der Disziplin",
    "text": "Bedeutung ist eine Frage der Disziplin\nApplication Programming Interfaces [APIs] im Fokus\n\n\n\nInformatik:\n\nz.B. Routinen, die Maschineninteraktionen strukturieren.\n\n\nSozialwissenschaften:\n\nClient-Server-Interaktionen (Web-APIs), zur Abfrage von Daten aus einem Dienst\n\n\n\n\n\nMerke:\nIn dieser Präsentation bezieht sich der Begriff API in der Regel bzw. wenn nicht anders erwähnt auf die sehr enge Teilmenge von (spezifischen) Web-APIs.\n\n\n\n▶️\nAber selbst innerhalb der Untergruppe der Web-APIs beschränkt sich das sozialwissenschaftliche Interesse in der Regel auf diejenigen APIs, die die Abfrage von Daten aus einem Dienst ermöglichen, während Web-APIs in der modernen Web-Entwicklung wesentlich breitere Anwendungen haben"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#api-endet-wo-scraping-beginnt",
    "href": "content/03-dbd-data_collection/slides/index.html#api-endet-wo-scraping-beginnt",
    "title": "Datenerhebung im Fokus",
    "section": "API endet, wo Scraping beginnt",
    "text": "API endet, wo Scraping beginnt\nZentrale Unterschiede in der Vorgehensweise\n\nTypische Vorgehen beim Web-Scraping als zweistufiger Prozess:\n\nAbfrage an Server senden, um eine bestimmte Ressource, häufig ein HTML-Dokument, anzufordern\n(Häufig sehr aufwändige) Extraktion der relevanten Information aus dem HTML-Dokument\n\n\nAPI im Vergleich:\n\nErster Schritt ähnlich wie beim Web-Scraping, aber API legen fest, welche Art von Informationen angefordert werden können und wie das Format einer gültigen Abfrage aussieht"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#api-endet-wo-scraping-beginnt-1",
    "href": "content/03-dbd-data_collection/slides/index.html#api-endet-wo-scraping-beginnt-1",
    "title": "Datenerhebung im Fokus",
    "section": "API endet, wo Scraping beginnt",
    "text": "API endet, wo Scraping beginnt\nZentrale Unterschiede in der Vorgehensweise\n\nTypische Vorgehen beim Web-Scraping als zweistufiger Prozess:\n\nAbfrage an Server senden, um eine bestimmte Ressource, häufig ein HTML-Dokument, anzufordern\n(Häufig sehr aufwändigere) Extraktion der relevanten Information aus dem HTML-Dokument\n\nAPI im Vergleich:\n\nErster Schritt ähnlich wie beim Web-Scraping, aber API legen fest, welche Art von Informationen angefordert werden können und wie das Format einer gültigen Abfrage aussieht\n\n\n\nZusammengefasst:\n\nAPI-Zugriff = kontrolliertes Scraping\nHauptunterschied liegt in der “Antwort” des Server (“Einfaches” Datenformat bei API, statt komplettes HTML-Dokument beim Scraping)\n\n\n\n▶️"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#kurze-einführung",
    "href": "content/03-dbd-data_collection/slides/index.html#kurze-einführung",
    "title": "Datenerhebung im Fokus",
    "section": "Kurze Einführung",
    "text": "Kurze Einführung\n(Web) Application Programming Interface (API)"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#scraping-oder-api",
    "href": "content/03-dbd-data_collection/slides/index.html#scraping-oder-api",
    "title": "Datenerhebung im Fokus",
    "section": "Scraping oder API?",
    "text": "Scraping oder API?\nVor- und Nachteile\n\n\nWeb scraping\n\n👍 WYSIWYG-Prinzip\n\n\n👎 viel Programmierung\n\n\n👎 Verstößt häufig gegen die AGBs\n\n\nAPIs\n\n👎 Platform bestimmt Inhalte & Limits\n\n\n👍 wenig Programmierung\n\n\n👍 API selbst verhindert Verstöße gegen AGBs"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#schlüsseltechnologie-für-übertragung-der-nachrichten",
    "href": "content/03-dbd-data_collection/slides/index.html#schlüsseltechnologie-für-übertragung-der-nachrichten",
    "title": "Datenerhebung im Fokus",
    "section": "Schlüsseltechnologie für Übertragung der Nachrichten",
    "text": "Schlüsseltechnologie für Übertragung der Nachrichten\nHypertext transfer protocol (HTTP)\nZwei für die API-Abfrage wichtige Aspekte dieser Nachrichten:\n\nUniform Resource Locator (URL), bestehend aus dem Protokoll, der Domain und dem Pfad zu einer spezifischen Ort der “Ressource”\n\nBeispiel: https://en.wikipedia.org/wiki/API\n\n\nAustausch der Nachrichten hauptsächlich über GET- und POST-Methode. Bei GET-Methode werden die Abfrageparameter an die URL angehängt, bei der POST-Methode in den Textkörper der Nachricht aufgenommen\n\nBeispiel: https://en.wikipedia.org/w/index.php?search=API&fulltext=1\n\n\n\n\nSchlüsseltechnologie für die Übertragung von Nachrichten zwischen Client und Server\nEine einzige Benutzeranfrage resultiert i.d.R. in einer Reihe von Nachrichten zwischen Client und Server, die alle dem HTTP-Protokoll entsprechen\nEine sehr kurze Einführung in Webtechnologien und Datenformate\nHauptunterschied der Methoden (GET-/POST) ist, wie sie Abfrageparameter einbeziehen\nBeispiel:\n\nProtokoll & Domaine unverändert, aber Pfad zur Ressource verändert\nDie spezifische Abfrage wird mit einem Fragezeichen (?) eingeleitet, und die Parameter liegen immer in Form von Schlüssel-Wert-Paaren vor, die durch Gleichheitszeichen (=) getrennt sind. Die möglichen Schlüssel werden von jeder Seite beliebig festgelegt. Im allgemeinen Fall von mehreren Parametern werden sie durch ein kaufmännisches Und (&) getrennt, wie im folgenden Beispiel, in dem wir Wikipedia bitten, eine Liste von Artikeln zu liefern, die den Suchbegriff enthalten"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#xml-json",
    "href": "content/03-dbd-data_collection/slides/index.html#xml-json",
    "title": "Datenerhebung im Fokus",
    "section": "XML & JSON",
    "text": "XML & JSON\nBeispiele für prominente Formate von API-Exporten\n\n\nXML (HTML ähnlich)\n<dataformats>\n  <formats>\n    <names>XML</names>\n    <file_extension>.xml</file_extension>\n  </format>\n  <formats>\n    <names>JSON</names>\n    <file_extension>.json</file_extension>\n  </format>\n</dataformats>\n\n\nXML (Alternative)\n<dataformats>\n  <formats name=\"XML\" file_extension=\".xml\"/>\n  <formats name=\"JSON\" file_extension=\".json\"/>\n</dataformats>\n\n\nJSON\n{\"dataformats\":[\n  {\"name\":\"XML\", \"file_extension\":\".xml\"},\n  {\"name\":\"JSON\", \"file_extension\":\".JSON\"},\n]}"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#koordination-nicht-bereitstellung",
    "href": "content/03-dbd-data_collection/slides/index.html#koordination-nicht-bereitstellung",
    "title": "Datenerhebung im Fokus",
    "section": "Koordination, nicht Bereitstellung",
    "text": "Koordination, nicht Bereitstellung\nImplikationen der Nutzen von APIs\n\nQuery- bzw. Abfragesystem (basierend auf Parametern)\nProgrammierung notwendig (API scraping)\nDaten nicht im Tabellenformat, sondern in JSON/XML\nErgebnisse werden in “Chunks” geliefert (z.B. 100 “Reihen”)\nLimit: Anfragen/MB pro Minuten\n\n\n\nABER:\nImmer häufiger auch für die gezielte Bereitstellung von Daten genutzt\n\n\n\n▶️\nDer Zweck von APIs ist die Kommunikation zwischen Programmen zu koordinieren, nicht die Bereitstellung von Daten für Wissenschaft."
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#know-your-bias",
    "href": "content/03-dbd-data_collection/slides/index.html#know-your-bias",
    "title": "Datenerhebung im Fokus",
    "section": "Know your bias!",
    "text": "Know your bias!\nEin Framework zur Minimierung von Bias (Olteanu et al., 2019)\n\n\nDescription:\n\nSocial data analysis starts with certain goals (section 2.1), such as understanding or influencing phenomena specific to social platforms (Type I) and/or phenomena beyond social platforms (Type II).\nThese goals require that research satisfies certain validity criteria, described earlier (section 2.2).\nThese criteria, in turn, can be compromised by a series of general biases and issues (section 3).\nThese challenges may depend on the characteristics of each data platform (section 4)—which are often not under the control of the researcher—and on the research designs choices made along a data processing pipeline (from sections 5 to 8)–which are often under the researcher control.\n\nPfeile zeigen an, wie sich Komponenten in unserem Rahmenwerk direkt auf andere auswirken\n\nErreichen bestimmter Ziele (Type I & II) bei der Analyse von Sozialdaten (Abschnitt 2.1) voraussetzt, dass die Forschung bestimmte Validitätskriterien erfüllt (Abschnitt 2.2),\ndie durch Verzerrungen und andere Probleme mit Sozialdaten beeinträchtigt werden können (Abschnitt 3)\nDiese Verzerrungen und Probleme können an der Quelle der Daten auftreten (Abschnitt 4), oder sie können im Verlauf der Datenanalyse eingeführt werden (Abschnitte 5-8)."
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#bias-at-the-source",
    "href": "content/03-dbd-data_collection/slides/index.html#bias-at-the-source",
    "title": "Datenerhebung im Fokus",
    "section": "Bias at the source",
    "text": "Bias at the source\nPotentielle Probleme mit der Datenquelle oder -herkunft\n\nBiases, die auf das Design und die Möglichkeiten der Plattformen zurückzuführen sind (functional biases).\nVerhaltensnormen, die auf den einzelnen Plattformen bestehen oder sich herausbilden (normative biases).\nFaktoren, die außerhalb der sozialen Plattformen liegen, aber das Nutzerverhalten beeinflussen können (external biases)\nVorhandensein von nicht-individuellen Konten ein (non-individuals)."
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#be-aware-when-collecting-data",
    "href": "content/03-dbd-data_collection/slides/index.html#be-aware-when-collecting-data",
    "title": "Datenerhebung im Fokus",
    "section": "Be aware when collecting data",
    "text": "Be aware when collecting data\nPotentielle Probleme bei der Datenerhebung\n\nAkquisition: Abhängig von den Daten, die von Plattformen erfasst und zur Verfügung gestellt werden, den für den Zugang gesetzten Grenzen und von der Art und Weise, wie der Zugang gewährt wird.\nAbfrage: APIs basieren auf unterschiedlich Dokumentationen sowie Arten von (unterstüzten) Abfragen.\nFilterung: In der Regel impliziert die Entscheidung, bestimmte Daten zu entfernen, die Annahme, dass sie für eine Studie nicht relevant sind. Dies ist hilfreich, wenn die Annahme zutrifft, und schädlich, wenn sie nicht zutrifft.\n\n\n▶️\nProbleme bei Akquisition:\n\nViele soziale Plattformen raten von der Datenerfassung durch Dritte ab.\nDer programmatische Zugang ist oft mit Einschränkungen verbunden.\nDie Plattform erfasst möglicherweise nicht alle relevanten Daten.\nDie Plattformen gewähren möglicherweise keinen Zugang zu allen erfassten Daten.\nStichprobenstrategien sind oft undurchsichtig\n\nProbleme bei Abfrage:\n\nAPIs haben eine begrenzte Aussagekraft in Bezug auf den Informationsbedarf.\nDer Informationsbedarf kann auf unterschiedliche Weise operationalisiert (formuliert) werden.\nDie Wahl der Schlüsselwörter in stichwortbasierten Abfragen prägt die resultierenden Datensätze.\n\nFilterung: Bei der Datenfilterung werden irrelevante Teile der Daten entfernt, was manchmal während der Datenerfassung aufgrund der begrenzten Ausdruckskraft einer API oder Abfragesprache nicht möglich ist. Der Schritt der Datenfilterung am Ende einer Datenerfassungspipeline wird häufig als Nachfilterung bezeichnet, da er nach der Erfassung oder Abfrage der Daten erfolgt (daher das Präfix “post-”).\n\nProbleme\n\nAusreißer sind manchmal für die Datenanalyse von Bedeutung.\nTextfilterung kann bestimmte Analysen einschränken."
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#handle-with-care",
    "href": "content/03-dbd-data_collection/slides/index.html#handle-with-care",
    "title": "Datenerhebung im Fokus",
    "section": "Handle with care",
    "text": "Handle with care\nPotentielle Probleme bei der Datenverarbeitung\n\nBereinigung: Können die Überzeugungen der Wissenschaftler*Innen über ein Phänomen sowie das System im Allgemeinen in den Datensatz einbetten und in falschen oder irreführenden Datenmustern resultieren\nAnreicherung: Sowohl manuelle als auch automatische Annotation ( (z.B. Part-of-Speech Tagging) sind fehleranfällig und können sowohl bestehende Verzerrungen verschärfen als auch neue Verzerrungen und Fehler einführen.\nAggregation: Die Art und Weise, wie diese Aggregationen vorgenommen werden, oder welche Informationen sie komprimieren, kann zu unterschiedlichen Schlussfolgerungen führen.\n\n\n▶️\nAnnahmen bei der Gestaltung von Datenverarbeitungspipelines können sich auf Datensätze auswirken und deren Inhalt, Struktur, Organisation oder Darstellung verändern (Barocas und Selbst, 2016; Poirier, 2018).\nDatenbereinigung:\n\nnicht gleichbedeutend mit Datenfilterung: Während die Datenbereinigung die Entfernung bestimmter Datenelemente beinhalten kann, kann sie auch die Normalisierung von Daten durch Korrektur oder Ersetzung unvollständiger oder fehlender Werte umfassen.\nProbleme\n\nDie Wahl der Datendarstellung und der Standardwerte kann zu Verzerrungen führen.\nDie Normalisierung von Text oder geografischen Referenzen kann zu Verzerrungen führen.\n\n\nAnreicherungen:\n\numfasst das Hinzufügen von Anmerkungen zu Datenelementen, die in der Analysephase verwendet werden können. Annotationen können von einfachen kategorischen Etiketten, die jedem Element zugeordnet sind, bis hin zu komplexeren Verarbeitungen wie Part-of-Speech-Tagging oder Dependency Parsing von Text reichen. Sie können entweder durch eine Form der (halb-)automatischen Klassifizierung oder durch menschliche Anmerkungen (z. B. Crowdsourcing, Umfragen) gewonnen werden.\nProbleme\n\nDie manuelle Beschriftung kann zu subjektiven und verrauschten Angaben führen.\nDie automatische Beschriftung durch statistische oder maschinelle Lernmethoden führt zu Fehlern.\n\n\nAggregation:\n\nwird durchgeführt, um Daten zu strukturieren, zu organisieren, darzustellen oder umzuwandeln; betrachten Sie Vorverarbeitungsheuristiken, die Daten aggregieren, um sie auf Kosten von Informationsverlusten besser handhabbar zu machen. Die Aggregation kann auch die Hervorhebung bestimmter Muster verringern oder verstärken (Olteanu et al., 2014b; Poirier, 2018).\nProbleme\n\nBei der Aggregation nach geografischen Gesichtspunkten kann es tatsächlich zu einer Art “Gerrymandering” kommen, das zu sehr unterschiedlichen Ergebnissen führt.\nBei der Betrachtung der Gesamthäufigkeit verschiedener Themen unter den Nutzern kann die Aggregation von Inhalten nach Nutzern den Interessen jedes Nutzers das gleiche Gewicht verleihen, während die Aggregation nach Themen den Inhalten hochaktiver Nutzer mehr Gewicht verleihen kann.\nWenn die Daten entlang eines bestimmten Attributs organisiert sind (z. B. das Vorhandensein eines Schlüsselworts oder Hashtags) und es mehrere unabhängige Faktoren gibt, die dazu führen, dass das Attribut einen bestimmten Wert annimmt, ist die Analyse von Dateneinträgen mit diesem Wert gleichbedeutend mit einer Konditionierung auf diesen Wert und kann zu falschen Assoziationsmustern zwischen diesen Faktoren führen (Blyth, 1972; Tufekci, 2014)."
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#pre-regestrierung-statt-harking",
    "href": "content/03-dbd-data_collection/slides/index.html#pre-regestrierung-statt-harking",
    "title": "Datenerhebung im Fokus",
    "section": "Pre-Regestrierung statt HARKing",
    "text": "Pre-Regestrierung statt HARKing\nMethodische Fallstricke bei der Analyse von Daten\nDie Wahl einer Analysemethode spiegelt in der Regel die Erfahrung und den Blickwinkel der Forschenden wider und kann verschiedene Bedenken hervorrufen, wie z. B.\n\ndie Verwendung von Daten als Quelle für Hypothesen und nicht als Instrument zu deren Prüfung;\ndie Anpassung der Forschungsagenda an die Datenverfügbarkeit, was zu Verzerrungen bei der Art der gestellten Fragen führen kann; oder\ndas Testen mehrerer Hypothesen, bis ein signifikantes, positives Ergebnis gefunden wird (“feature hunting”)\n\n\n▶️\n“HARKing = hypothesizing after results are known”\nImplikationen. Eine wichtige Konsequenz ist die mangelnde Replizierbarkeit. Aufgrund von Unterschieden in der Analysemethodik, Messung und Datenerhebung konnten Liang und Fu (2015) beispielsweise 6 von 10 bekannten Thesen aus Social-Media-Studien nicht wiederholen. Generell kann die interne und externe Validität sowohl der Typ-I- als auch der Typ-II-Forschung durch die Wahl der Methoden beeinflusst werden, die bei der Analyse der Daten angewandt werden, um Nutzerpopulationen und -verhaltensweisen zu charakterisieren (Abschnitte 7.1-7.2), Schlussfolgerungen und Vorhersagen zu treffen (Abschnitt 7.3) und (kausale) Zusammenhänge zu ermitteln (Abschnitt 7.4)."
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#offenheit-und-transparenz-als-lösung-statt-als-problem",
    "href": "content/03-dbd-data_collection/slides/index.html#offenheit-und-transparenz-als-lösung-statt-als-problem",
    "title": "Datenerhebung im Fokus",
    "section": "Offenheit und Transparenz als Lösung statt als Problem",
    "text": "Offenheit und Transparenz als Lösung statt als Problem\nProbleme bei der Auswertung und Interpretation der Ergebnisse\n\nAuswahl der Messgrößen: Besonders bei latenten Konstrukten ist die Art der Operationalisierung (z. B. Sharing-Muster als Näherungswert für Beliebtheit) entscheidend für die Aussagekraft der Studie(n)\nBewertung und Interpretation der Ergebnisse: Abhängig von den ursprünglichen Annahmen (z.B. dass soziale Online-Spuren in gewisser Weise quantifizierbare Phänomene der realen Welt\nDisclaimer und Reproduzierbarkeit: Um die Reproduzierbarkeit (durch standardisierte experimentelle Methoden) zu fördern, ist die Entwicklung von Standardbewertungsverfahren und -metriken notwendig\n\n\n▶️\nEine letzte Möglichkeit, Verzerrungen zu berücksichtigen und die Zuverlässigkeit der Ergebnisse zu beurteilen, besteht bei der Bewertung und Interpretation der Leistung eines Instruments oder der Ergebnisse einer Studie. Ein guter Ausgangspunkt ist ein angemessenes Verständnis für die Art der verwendeten Daten. Rost et al. (2013) argumentieren beispielsweise, dass Daten, die explizit von Nutzern in sozialen Medien generiert werden, eher als kommunikativ denn als repräsentativ interpretiert werden sollten, da diese Daten oft eine Aufzeichnung der Kommunikation und nicht eine direkte Darstellung anderer (“realer”) Verhaltensweisen sind, was Fragen zur Konstrukt- und internen Validität aufwirft.\nProbleme bei Auswahl der Messgrößen:\n\nDie Wahl der Metriken prägt eine Forschungsstudie - Schlussfolgerungen\nDie Bewertung der Fairness bringt ihre eigenen Herausforderungen mit sich.\nKontext- oder bereichsspezifische Leistungsindikatoren werden selten verwendet.\n\nProbleme bei der Bewertung und Interpretation der Ergebnisse\n\nDie Bedeutung sozialer Spuren kann sich je nach Kontext ändern; dies ist jedoch bei der Bewertung schwer zu erkennen\nAnalysen und Bewertungen, die sich auf einen einzigen Datensatz oder eine einzige Methode beschränken, können nicht verallgemeinert werden.\nDie Interpretation und Bewertung der Ergebnisse wird zu oft von Datenexperten und nicht von Fachleuten vorgenommen.\n\nDisclaimers und Reproduzierbarkeit\n\nUm die Reproduzierbarkeit zu fördern, müssen Grundlagen und Leitlinien entwickelt werden (Tufekci, 2014; Weller und Kinder-Kurlanda, 2015), eine gemeinsame Basis für methodische Ansätze gefunden werden (Counts et al., 2014) und selbst entwickelte Instrumente und Methoden sowie die Datenherkunft besser dokumentiert werden (Bruns, 2013; Weller und Kinder-Kurlanda, 2015).\nProbleme:\n\nDisclaimers und negative Ergebnisse werden übersehen.\nDie gemeinsame Nutzung von Daten und Instrumenten muss erleichtert werden."
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#final-takeaways",
    "href": "content/03-dbd-data_collection/slides/index.html#final-takeaways",
    "title": "Datenerhebung im Fokus",
    "section": "Final Takeaways",
    "text": "Final Takeaways\nAbschließende Bewertung der Arbeit mit DBD\n\nViele Möglichkeiten zur Untersuchung verschiedener Forschungsfragen\nDie (begründete) Auswahl der Plattform, die für die Forschung dienen soll, ist wichtig\nAPIs sind nicht für Forschende gedacht, um auf Daten zuzugreifen.\nAPIs bieten Zugang zu Modellen des maschinellen Lernens:\n\nBei “langweilige” Annotationsaufgabe lohnt sich die Suchen nach einer API. Die Chancen stehen gut, dass bereits eine API für diese Aufgabe existiert.\n\n\n\n\nEmpfehlungen:\nKnow your biases and validate, validate, validate!\n\n\n\n▶️"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#vielen-dank-für-ihre-fragen",
    "href": "content/03-dbd-data_collection/slides/index.html#vielen-dank-für-ihre-fragen",
    "title": "Datenerhebung im Fokus",
    "section": "Vielen Dank für Ihre Fragen!",
    "text": "Vielen Dank für Ihre Fragen!\n\n\nDie Vernetzung in unserer Welt nimmt immer mehr zu und Prozesse müssen perfekt aufeinander abgestimmt sein. Es gibt eine riesige Menge an verschiedenen APIs und unsere Abhängigkeit von ihnen wird zunehmend größer. Aber wie zuverlässig sind APIs eigentlich und wie kann ein API-Missbrauch aussehen?\n\n\n\n\nFacebook und andere Plattformen haben den Zugang zu Nutzerdaten über ihr API aus Datenschutzgründen stark eingeschränkt. Jedoch kann bislang jede Organisation individuell darüber entscheiden, welche Restriktionen es in dieser Hinsicht geben soll. Sollte es übergreifende „Regeln” für APIs geben, die von jedem einzuhalten sind? Wenn ja, könnten Verstöße strafrechtlich verfolgt werden?\n\n\n\n\nMithilfe von APIs kann man verhältnismäßig einfach an Daten gelangen. Jedoch sind die Algorithmen hinter den Plattformen oft nicht nachvollziehbar (z.B. woher kommen personalisierte Empfehlungen). Können die extrahierten Daten tatsächlich interpretiert werden, wenn die Entstehung nicht nachvollziehbar ist?\n\n\n\n\nWeb-APIs erfordern grundlegende Programmierkenntnisse, um den Zugang zu ermöglichen. Leider gibt es bislang keine effizienten Alternativen. Gibt es potentielle Ansätze, um diese Hürde auf eine andere Art und Weise zu überwinden."
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#mehr-als-eine-enzyklopädie",
    "href": "content/03-dbd-data_collection/slides/index.html#mehr-als-eine-enzyklopädie",
    "title": "Datenerhebung im Fokus",
    "section": "Mehr als eine Enzyklopädie",
    "text": "Mehr als eine Enzyklopädie\nDie Vielfalt von Wikipedia\n\n\n… als soziales Netzwerk der Zusammenarbeit\n\n20K aktive Redakteur*Innen pro Monat (englisches Wikipedia)\nWikipedia-Seitengespräche (wo Diskussionen über Artikelüberarbeitungen stattfinden)\nAufzeichnung aller Überarbeitungen\nQuellen (Referenzen), die den Inhalt unterstützen (Zagovora et al., 2020)\n\n\n… als eine Vielzahl von APIs und Tools, die damit verbunden sind, z. B:\n\nStatistiken: https://xtools.wmflabs.org\nWissensdatenbank: https://www.wikidata.org\nKlassifizierungssysteme: https://ores.wikimedia.org\nVerfolgung von Änderungen: https://www.wikiwho.net (Flöck & Acosta, 2014)\nBedeutung von Wikipedia als Korpus für maschinelle Lernsysteme (NLP)\nEine kostenlose API (oder direkter Download von Dumps)\n\n\n\n\n▶️"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#im-fokus-wikiwho-whocolor-api",
    "href": "content/03-dbd-data_collection/slides/index.html#im-fokus-wikiwho-whocolor-api",
    "title": "Datenerhebung im Fokus",
    "section": "Im Fokus: WikiWho & WhoColor API",
    "text": "Im Fokus: WikiWho & WhoColor API\nAuswertung von Änderungen und Interaktionen auf überarbeiteten Schreibplattformen.\n\n\nKernfunktionalität von WikiWho\n\nBei einem revidierten Textdokument werden aller historischen Revisionen (Versionen) analysiert, um herauszufinden, wer welchen genauen Text auf Token-Ebene in welcher Revision geschrieben und/oder entfernt und/oder wieder eingefügt hat\nFür jedes Token (~Wort) ist seine individuelle Hinzufügungs-/Entfernungs-/Wiedereinfügungsgeschichte verfügbar.\n\n… in Kombination mit WhoColor API\n\nBeim Öffnen eines Wikipedia-Artikels wird eine farbliche Markierung des Textes erstellt, die die ursprünglichen Autor:Innen des Inhalts, eine Autor:Innenliste, geordnet nach dem prozentualen Anteil an der Erstellung des Artikels, und zusätzliche Herkunftsinformationen anzeigt.\nEs kann auch Konflikte in Bezug auf bestimmte Textteile und die Historie des Hinzufügens/Löschens eines bestimmten Wortes anzeigen.\n\n\nBeispiel:"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#and-now-you",
    "href": "content/03-dbd-data_collection/slides/index.html#and-now-you",
    "title": "Datenerhebung im Fokus",
    "section": "And now … you!",
    "text": "And now … you!\nAnwendung von WhoColor API\n\n\nErster Schritt:\n\nfür Google Chrome: Laden und installieren Sie die Tampermonkey-Erweiterung.\nfür Mozilla Firefox: Laden und installieren Sie die Greasemonkey-Erweiterung.\n\nZweiter Schritt:\n\nSobald Sie eine der *Monkey-Erweiterungen erfolgreich installiert haben, gehen Sie zu whocolor.user.js.\nIhre *monkey-Browsererweiterung sollte Ihnen dann automatisch eine Installationsaufforderung anzeigen, die Sie bestätigen müssen.\n\nDritter Schritt:\nÖffnen Sie entweder den Wikipedia-Artikel von Donald Trump oder Elon Musk und wenden Sie das “WhoColor”-Plugin an.\n\n\n\n\n\n\n\n\n\nVierter Schritt: Explore!\nWas fällt Ihnen auf …\n\nbezüglich der Editors List?\nmit Blick auf besonders “konfliktreiche” Stellen?\nmit Blick auf aktuelle Veränderungen?\n…"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#many-more-apis-to-access-data",
    "href": "content/03-dbd-data_collection/slides/index.html#many-more-apis-to-access-data",
    "title": "Datenerhebung im Fokus",
    "section": "Many more APIs to access data",
    "text": "Many more APIs to access data\n\nSuchmaschinen-APIs (Google, Bing)\nStaatliche Daten (abgeordnetenwatch.de, data.gov, data.gov.uk, open-data.europa.eu)\nInternationale Agenturen: UN, WHO, die Weltbank\nNachrichtenorganisationen: BBC, The New York Times, The Guardian, NPR, USA Today und ZEIT Online\nWissenschaftliche Archive und Fachzeitschriften: arXiv, PLoS, Mendeley\nMetadaten von Daten: Dryad (https://datadryad.org/api/v2/docs/), Figshare (https://docs.figshare.com/)\nMusik: Spotify, Soundcloud\n…"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/index.html#literatur",
    "href": "content/03-dbd-data_collection/slides/index.html#literatur",
    "title": "Datenerhebung im Fokus",
    "section": "Literatur",
    "text": "Literatur\n\n\nFlöck, F., & Acosta, M. (2014). the 23rd international conference. 843–854. https://doi.org/10.1145/2566486.2568026\n\n\nFlöck, F., & Sen, I. (2022). Digital traces of human behaviour in online platforms  research design and error sources. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meet_the_experts_Digitaltraces_humanbehaviour.pdf\n\n\nOlteanu, A., Castillo, C., Diaz, F., & Kıcıman, E. (2019). Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2, 13. https://doi.org/10.3389/fdata.2019.00013\n\n\nUlloa, R. (2021). Introduction to online data acquisition. https://www.gesis.org/fileadmin/user_upload/MeettheExperts/GESIS_Meettheexperts_Introductioncss.pdf\n\n\nZagovora, O., Ulloa, R., Weller, K., & Flöck, F. (2020). ’I updated the ’: The evolution of references in the english wikipedia and the implications for altmetrics. https://doi.org/10.48550/ARXIV.2010.03083"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/scripts/graph_creation.html",
    "href": "content/03-dbd-data_collection/slides/scripts/graph_creation.html",
    "title": "graph_creation",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(svgtools)\nlibrary(here)\n\nhere() starts at D:/repos/github/digital-behavioral-data"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/scripts/graph_creation.html#dbd-research-design",
    "href": "content/03-dbd-data_collection/slides/scripts/graph_creation.html#dbd-research-design",
    "title": "graph_creation",
    "section": "DBD Research design",
    "text": "DBD Research design\n\nGet base graphic\n\ngraph_base_dbd <- svgtools::read_svg(\n  here(\"content/03-dbd-data_collection/slides/graphs/graph_dbd-01_base.svg\")) \n\n\n\ngraph_dbd_01\n\n# Create graph\ngraph_dbd_01 <- graph_base_dbd %>% \n  toString() %>%\n  #### GROUP: Population\n   # Circles\n  stringr::str_replace_all('\"#00B050\"', '\"#04316A\"') %>% # change color\n   # Label\n  stringr::str_replace(\"symbol1label\", \"Population\") %>% # change label\n  #### GROUP: Construct\n    # Shape\n  stringr::str_replace_all('\"#FFFF00\"', '\"#FFFF0000\"') %>% # hide symbol\n    # Label\n  stringr::str_replace('symbol2label', '') %>% # hide label\n  #### GROUP: Text bubbles (Population and Construct)\n    # Shape\n  stringr::str_replace_all('\"#7F6000\"', '\"#7F600000\"') %>% # hide bubble\n    # Text\n  stringr::str_replace(\"bubble1label1\", \"\") %>% # hide text\n  stringr::str_replace(\"bubble1label2\", \"\") %>% # hide text\n  stringr::str_replace(\"bubble2label1\", \"\") %>% # hide text\n  stringr::str_replace(\"bubble2label2\", \"\") %>% # hide text\n  #### GROUP: Survey\n    # Symbol\n  stringr::str_replace_all('\"#70AD47\"', '\"#70AD4700\"') %>% # hide symbol\n    # Label\n  stringr::str_replace_all('label_box_3', '') %>% # hide label\n  #### GROUP: Free Text(Survey)\n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff8080\"',\n    'style=\"fill:#ff808000\"') %>% # hide bulletpoints \n    # Text\n  stringr::str_replace(\"surveylabel1\", \"\") %>% # hide text\n  stringr::str_replace(\"surveylabel2\", \"\") %>% # hide text\n  stringr::str_replace(\"surveylabel3\", \"\") %>% # hide text\n  #### GROUP: SNS Twitter\n    # Box & Symbol\n  stringr::str_replace_all('\"#00B0F0\"', '\"#00B0F000\"') %>% # hide filling\n    # Cirles\n  stringr::str_replace_all('\"#0066FF\"', '\"#0066FF00\"') %>% # hide filling  \n  stringr::str_replace_all('\"#FF0000\"', '\"#FF000000\"') %>% # hide strokes\n    # Label\n  stringr::str_replace_all('User', '') %>% # hide label\n  #### GROUP: SNS Populations\n    # Boxes & Symbols\n  stringr::str_replace_all('\"#C55A11\"', '\"#C55A1100\"') %>% # hide stroke\n    # Cirles\n  stringr::str_replace_all('\"#800000\"', '\"#80000000\"') %>% # hide filling  \n  stringr::str_replace_all('\"#9900FF\"', '\"#9900FF00\"') %>% # hide strokes\n  #### GROUP: Links\n  stringr::str_replace_all('\"#7F7F7F\"', '\"#7F7F7F00\"') %>% # hide stroke\n  #### GROUP: SNS Population Textboxes\n    # Shape\n  stringr::str_replace_all('\"#7030A0\"', '\"#7030A000\"') %>% # hide boxes \n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff00ff\"',\n    'style=\"fill:#ff00ff00\"') %>% # hide bulletpoints \n    # Text\n    ## Box 1\n  stringr::str_replace_all('box1h1', '') %>% # hide text\n  stringr::str_replace_all('box1text1', '') %>% # hide text\n  stringr::str_replace_all('box1text2', '') %>% # hide text\n  stringr::str_replace_all('box1text3', '') %>% # hide text\n  stringr::str_replace_all('box1text4', '') %>% # hide text\n    ## Box 2 \n  stringr::str_replace_all('box2h1', '') %>% # hide text\n  stringr::str_replace_all('box2text1', '') %>% # hide text\n  stringr::str_replace_all('box2text2', '') %>% # hide text\n  stringr::str_replace_all('box2text3', '') %>% # hide text\n #### Convert back to .svg\n  svgtools::read_svg(.) \n  \n# Save output\nsvgtools::write_svg(\n  graph_dbd_01, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_dbd_01.svg\"))\n\n\n\ngraph_dbd_02\n\n# Create graph\ngraph_dbd_02 <- graph_base_dbd %>% \n  toString() %>%\n  #### GROUP: Population\n   # Circles\n  stringr::str_replace_all('\"#00B050\"', '\"#04316A\"') %>% \n   # Label\n  stringr::str_replace(\"symbol1label\", \"Population\") %>%\n  #### GROUP: Construct\n    # Shape\n  stringr::str_replace_all('\"#FFFF00\"', '\"#E6002E\"') %>% # CHANGE color \n    # Label\n  stringr::str_replace('symbol2label', 'Konstrukt') %>% # CHANGE text\n  #### GROUP: Text bubbles (Population and Construct)\n    # Shape\n  stringr::str_replace_all('\"#7F6000\"', '\"#7F600000\"') %>%\n    # Text\n  stringr::str_replace(\"bubble1label1\", \"\") %>% \n  stringr::str_replace(\"bubble1label2\", \"\") %>% \n  stringr::str_replace(\"bubble2label1\", \"\") %>% \n  stringr::str_replace(\"bubble2label2\", \"\") %>% \n  #### GROUP: Survey\n    # Symbol\n  stringr::str_replace_all('\"#70AD47\"', '\"#70AD4700\"') %>% \n    # Label\n  stringr::str_replace_all('label_box_3', '') %>% \n  #### GROUP: Free Text(Survey)\n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff8080\"',\n    'style=\"fill:#ff808000\"') %>%  \n    # Text\n  stringr::str_replace(\"surveylabel1\", \"\") %>% \n  stringr::str_replace(\"surveylabel2\", \"\") %>% \n  stringr::str_replace(\"surveylabel3\", \"\") %>% \n  #### GROUP: SNS Twitter\n    # Box & Symbol\n  stringr::str_replace_all('\"#00B0F0\"', '\"#00B0F000\"') %>%\n    # Cirles\n  stringr::str_replace_all('\"#0066FF\"', '\"#0066FF00\"') %>%   \n  stringr::str_replace_all('\"#FF0000\"', '\"#FF000000\"') %>% \n    # Label\n  stringr::str_replace_all('User', '') %>% \n  #### GROUP: SNS Populations\n    # Boxes & Symbols\n  stringr::str_replace_all('\"#C55A11\"', '\"#C55A1100\"') %>% \n    # Cirles\n  stringr::str_replace_all('\"#800000\"', '\"#80000000\"') %>%   \n  stringr::str_replace_all('\"#9900FF\"', '\"#9900FF00\"') %>% \n  #### GROUP: Links\n  stringr::str_replace_all('\"#7F7F7F\"', '\"#7F7F7F00\"') %>% \n  #### GROUP: SNS Population Textboxes\n    # Shape\n  stringr::str_replace_all('\"#7030A0\"', '\"#7030A000\"') %>%  \n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff00ff\"',\n    'style=\"fill:#ff00ff00\"') %>%\n    # Text\n    ## Box 1\n  stringr::str_replace_all('box1h1', '') %>% \n  stringr::str_replace_all('box1text1', '') %>% \n  stringr::str_replace_all('box1text2', '') %>%\n  stringr::str_replace_all('box1text3', '') %>%\n  stringr::str_replace_all('box1text4', '') %>% \n    ## Box 2 \n  stringr::str_replace_all('box2h1', '') %>% \n  stringr::str_replace_all('box2text1', '') %>% \n  stringr::str_replace_all('box2text2', '') %>% \n  stringr::str_replace_all('box2text3', '') %>% \n #### Convert back to .svg\n  svgtools::read_svg(.) \n  \n# Save output\nsvgtools::write_svg(\n  graph_dbd_02, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_dbd_02.svg\"))\n\n\n\ngraph_dbd_03\n\n# Create graph\ngraph_dbd_03 <- graph_base_dbd %>% \n  toString() %>%\n  #### GROUP: Population\n   # Circles\n  stringr::str_replace_all('\"#00B050\"', '\"#04316A\"') %>% \n   # Label\n  stringr::str_replace(\"symbol1label\", \"Population\") %>%\n  #### GROUP: Construct\n    # Shape\n  stringr::str_replace_all('\"#FFFF00\"', '\"#04316A\"') %>% # CHANGE color \n    # Label\n  stringr::str_replace('symbol2label', 'Konstrukt') %>% # CHANGE text\n #### Group: Symbol text bubbles\n    # Shape\n  stringr::str_replace_all('stroke=\"#7F6000\"','stroke=\"#e6002e\"') %>% # CHANGE stroke\n  stringr::str_replace_all('fill=\"#7F6000\"', 'fill=\"#ffffff\"') %>% # CHANGE fill\n    # Text\n  stringr::str_replace(\"bubble1label1\", \"Alle Erwachsenen\") %>% # CHANGE text\n  stringr::str_replace(\"bubble1label2\", \"in Deutschland\") %>% # CHANGE text\n  stringr::str_replace(\"bubble2label1\", \"Unterstützung der\") %>% # CHANGE text\n  stringr::str_replace(\"bubble2label2\", \"COVID19-Maßnamen\") %>% # CHANGE text\n  #### GROUP: Survey\n    # Symbol\n  stringr::str_replace_all('\"#70AD47\"', '\"#70AD4700\"') %>% \n    # Label\n  stringr::str_replace_all('label_box_3', '') %>% \n  #### GROUP: Free Text(Survey)\n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff8080\"',\n    'style=\"fill:#ff808000\"') %>%  \n    # Text\n  stringr::str_replace(\"surveylabel1\", \"\") %>% \n  stringr::str_replace(\"surveylabel2\", \"\") %>% \n  stringr::str_replace(\"surveylabel3\", \"\") %>% \n  #### GROUP: SNS Twitter\n    # Box & Symbol\n  stringr::str_replace_all('\"#00B0F0\"', '\"#00B0F000\"') %>%\n    # Cirles\n  stringr::str_replace_all('\"#0066FF\"', '\"#0066FF00\"') %>%   \n  stringr::str_replace_all('\"#FF0000\"', '\"#FF000000\"') %>% \n    # Label\n  stringr::str_replace_all('User', '') %>% \n  #### GROUP: SNS Populations\n    # Boxes & Symbols\n  stringr::str_replace_all('\"#C55A11\"', '\"#C55A1100\"') %>% \n    # Cirles\n  stringr::str_replace_all('\"#800000\"', '\"#80000000\"') %>%   \n  stringr::str_replace_all('\"#9900FF\"', '\"#9900FF00\"') %>% \n  #### GROUP: Links\n  stringr::str_replace_all('\"#7F7F7F\"', '\"#7F7F7F00\"') %>% \n  #### GROUP: SNS Population Textboxes\n    # Shape\n  stringr::str_replace_all('\"#7030A0\"', '\"#7030A000\"') %>%  \n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff00ff\"',\n    'style=\"fill:#ff00ff00\"') %>%\n    # Text\n    ## Box 1\n  stringr::str_replace_all('box1h1', '') %>% \n  stringr::str_replace_all('box1text1', '') %>% \n  stringr::str_replace_all('box1text2', '') %>%\n  stringr::str_replace_all('box1text3', '') %>%\n  stringr::str_replace_all('box1text4', '') %>% \n    ## Box 2 \n  stringr::str_replace_all('box2h1', '') %>% \n  stringr::str_replace_all('box2text1', '') %>% \n  stringr::str_replace_all('box2text2', '') %>% \n  stringr::str_replace_all('box2text3', '') %>% \n #### Convert back to .svg\n  svgtools::read_svg(.) \n  \n# Save output\nsvgtools::write_svg(\n  graph_dbd_03, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_dbd_03.svg\"))\n\n\n\ngraph_dbd_04\n\n# Create graph\ngraph_dbd_04 <- graph_base_dbd %>% \n  toString() %>%\n  #### GROUP: Population\n   # Circles\n  stringr::str_replace_all('\"#00B050\"', '\"#04316A\"') %>% \n   # Label\n  stringr::str_replace(\"symbol1label\", \"Population\") %>%\n  #### GROUP: Construct\n    # Shape\n  stringr::str_replace_all('\"#FFFF00\"', '\"#04316A\"') %>% \n    # Label\n  stringr::str_replace('symbol2label', 'Konstrukt') %>% \n  #### GROUP: Text bubbles (Population and Construct)\n    # Shape\n  stringr::str_replace_all('\"#7F6000\"', '\"#7F600000\"') %>%  # HIDE boxes\n    # Text\n  stringr::str_replace(\"bubble1label1\", \"\") %>% # HIDE text\n  stringr::str_replace(\"bubble1label2\", \"\") %>% # HIDE text\n  stringr::str_replace(\"bubble2label1\", \"\") %>% # HIDE text\n  stringr::str_replace(\"bubble2label2\", \"\") %>% # HIDE text\n  #### GROUP: Survey\n    # Symbol\n  stringr::str_replace_all('\"#70AD47\"', '\"#E6002E\"') %>% # CHANGE color\n    # Label\n  stringr::str_replace_all('label_box_3', 'Survey') %>% # CHANGE label\n  #### GROUP: Free Text(Survey)\n    # Bulletpoints\n  stringr::str_replace_all('style=\"fill:#ff8080\"','style=\"fill:#E6002E\"') %>% # CHANGE color\n    # Text\n  stringr::str_replace(\"surveylabel1\", \"Teuer\") %>% # CHANGE text\n  stringr::str_replace(\"surveylabel2\", \"Zeitverzörgert\") %>% # CHANGE text\n  stringr::str_replace(\"surveylabel3\", '\"Kleine\" Samples') %>% # CHANGE text \n  #### GROUP: SNS Twitter\n    # Box & Symbol\n  stringr::str_replace_all('\"#00B0F0\"', '\"#00B0F000\"') %>%\n    # Cirles\n  stringr::str_replace_all('\"#0066FF\"', '\"#0066FF00\"') %>%   \n  stringr::str_replace_all('\"#FF0000\"', '\"#FF000000\"') %>% \n    # Label\n  stringr::str_replace_all('User', '') %>% \n  #### GROUP: SNS Populations\n    # Boxes & Symbols\n  stringr::str_replace_all('\"#C55A11\"', '\"#C55A1100\"') %>% \n    # Cirles\n  stringr::str_replace_all('\"#800000\"', '\"#80000000\"') %>%   \n  stringr::str_replace_all('\"#9900FF\"', '\"#9900FF00\"') %>% \n  #### GROUP: Links\n  stringr::str_replace_all('\"#7F7F7F\"', '\"#7F7F7F00\"') %>% \n  #### GROUP: SNS Population Textboxes\n    # Shape\n  stringr::str_replace_all('\"#7030A0\"', '\"#7030A000\"') %>%  \n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff00ff\"',\n    'style=\"fill:#ff00ff00\"') %>%\n    # Text\n    ## Box 1\n  stringr::str_replace_all('box1h1', '') %>% \n  stringr::str_replace_all('box1text1', '') %>% \n  stringr::str_replace_all('box1text2', '') %>%\n  stringr::str_replace_all('box1text3', '') %>%\n  stringr::str_replace_all('box1text4', '') %>% \n    ## Box 2 \n  stringr::str_replace_all('box2h1', '') %>% \n  stringr::str_replace_all('box2text1', '') %>% \n  stringr::str_replace_all('box2text2', '') %>% \n  stringr::str_replace_all('box2text3', '') %>% \n #### Convert back to .svg\n  svgtools::read_svg(.) \n  \n# Save output\nsvgtools::write_svg(\n  graph_dbd_04, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_dbd_04.svg\"))\n\n\n\ngraph_dbd_05\n\n# Create graph\ngraph_dbd_05 <- graph_base_dbd %>% \n  toString() %>%\n  #### GROUP: Population\n   # Circles\n  stringr::str_replace_all('\"#00B050\"', '\"#04316A\"') %>% \n   # Label\n  stringr::str_replace(\"symbol1label\", \"Population\") %>%\n  #### GROUP: Construct\n    # Shape\n  stringr::str_replace_all('\"#FFFF00\"', '\"#04316A\"') %>% \n    # Label\n  stringr::str_replace('symbol2label', 'Konstrukt') %>% \n  #### GROUP: Text bubbles (Population and Construct)\n    # Shape\n  stringr::str_replace_all('\"#7F6000\"', '\"#7F600000\"') %>%  \n    # Text\n  stringr::str_replace(\"bubble1label1\", \"\") %>% \n  stringr::str_replace(\"bubble1label2\", \"\") %>%  \n  stringr::str_replace(\"bubble2label1\", \"\") %>% \n  stringr::str_replace(\"bubble2label2\", \"\") %>% \n  #### GROUP: Survey\n    # Symbol\n  stringr::str_replace_all('\"#70AD47\"', '\"#04316A\"') %>% # CHANGE color\n    # Label\n  stringr::str_replace_all('label_box_3', 'Survey') %>% # CHANGE label\n  #### GROUP: Free Text(Survey)\n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff8080\"',\n    'style=\"fill:#ff808000\"') %>%  \n    # Text\n  stringr::str_replace(\"surveylabel1\", \"\") %>% # HIDE text\n  stringr::str_replace(\"surveylabel2\", \"\") %>% # HIDE text\n  stringr::str_replace(\"surveylabel3\", \"\") %>% # HIDE text \n  #### GROUP: Links (moved to avoid replacement of color)\n  stringr::str_replace_all('\"#7F7F7F\"', '\"#7F7F7F00\"') %>% \n  #### GROUP: SNS Twitter\n    # Box & Symbol\n  stringr::str_replace_all('\"#00B0F0\"', '\"#E6002E\"') %>% # CHANGE color\n    # Cirles\n  stringr::str_replace_all('\"#0066FF\"', '\"#7F7F7F\"') %>% # CHANGE color  \n  stringr::str_replace_all('\"#FF0000\"', '\"#04316A\"') %>% # CHANGE color\n    # Label\n  stringr::str_replace_all('User', '') %>% \n  #### GROUP: SNS Populations\n    # Boxes & Symbols\n  stringr::str_replace_all('\"#C55A11\"', '\"#E6002E\"') %>% # CHANGE color\n    # Cirles\n  stringr::str_replace_all('\"#800000\"', '\"#7F7F7F\"') %>% # CHANGE color  \n  stringr::str_replace_all('\"#9900FF\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP: SNS Population Textboxes\n    # Shape\n  stringr::str_replace_all('\"#7030A0\"', '\"#7030A000\"') %>%  \n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff00ff\"',\n    'style=\"fill:#ff00ff00\"') %>%\n    # Text\n    ## Box 1\n  stringr::str_replace_all('box1h1', '') %>% \n  stringr::str_replace_all('box1text1', '') %>% \n  stringr::str_replace_all('box1text2', '') %>%\n  stringr::str_replace_all('box1text3', '') %>%\n  stringr::str_replace_all('box1text4', '') %>% \n    ## Box 2 \n  stringr::str_replace_all('box2h1', '') %>% \n  stringr::str_replace_all('box2text1', '') %>% \n  stringr::str_replace_all('box2text2', '') %>% \n  stringr::str_replace_all('box2text3', '') %>% \n #### Convert back to .svg\n  svgtools::read_svg(.) \n  \n# Save output\nsvgtools::write_svg(\n  graph_dbd_05, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_dbd_05.svg\"))\n\n\n\ngraph_dbd_06\n\n# Create graph\ngraph_dbd_06 <- graph_base_dbd %>% \n  toString() %>%\n  #### GROUP: Population\n   # Circles\n  stringr::str_replace_all('\"#00B050\"', '\"#04316A\"') %>% \n   # Label\n  stringr::str_replace(\"symbol1label\", \"Population\") %>%\n  #### GROUP: Construct\n    # Shape\n  stringr::str_replace_all('\"#FFFF00\"', '\"#04316A\"') %>% \n    # Label\n  stringr::str_replace('symbol2label', 'Konstrukt') %>% \n  #### GROUP: Text bubbles (Population and Construct)\n    # Shape\n  stringr::str_replace_all('\"#7F6000\"', '\"#7F600000\"') %>%  \n    # Text\n  stringr::str_replace(\"bubble1label1\", \"\") %>% \n  stringr::str_replace(\"bubble1label2\", \"\") %>%  \n  stringr::str_replace(\"bubble2label1\", \"\") %>% \n  stringr::str_replace(\"bubble2label2\", \"\") %>% \n  #### GROUP: Survey\n    # Symbol\n  stringr::str_replace_all('\"#70AD47\"', '\"#04316A\"') %>%  \n    # Label\n  stringr::str_replace_all('label_box_3', 'Survey') %>% \n  #### GROUP: Free Text(Survey)\n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff8080\"',\n    'style=\"fill:#ff808000\"') %>%  \n    # Text\n  stringr::str_replace(\"surveylabel1\", \"\") %>% \n  stringr::str_replace(\"surveylabel2\", \"\") %>% \n  stringr::str_replace(\"surveylabel3\", \"\") %>% \n  #### GROUP: Links (moved to avoid replacement of color)\n  stringr::str_replace_all('\"#7F7F7F\"', '\"#E6002E\"') %>% # CHANGE color\n  #### GROUP: SNS Twitter\n    # Box & Symbol\n  stringr::str_replace_all('\"#00B0F0\"', '\"#04316A\"') %>% # CHANGE color\n    # Cirles\n  stringr::str_replace_all('\"#0066FF\"', '\"#7F7F7F\"') %>% #   \n  stringr::str_replace_all('\"#FF0000\"', '\"#04316A\"') %>% # \n    # Label\n  stringr::str_replace_all('User', '') %>% \n  #### GROUP: SNS Populations\n    # Boxes & Symbols\n  stringr::str_replace_all('\"#C55A11\"', '\"#04316A\"') %>% # CHANGE color\n    # Cirles\n  stringr::str_replace_all('\"#800000\"', '\"#7F7F7F\"') %>% #   \n  stringr::str_replace_all('\"#9900FF\"', '\"#04316A\"') %>% # \n  #### GROUP: SNS Population Textboxes\n    # Shape\n  stringr::str_replace_all('\"#7030A0\"', '\"#7030A000\"') %>%  \n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff00ff\"',\n    'style=\"fill:#ff00ff00\"') %>%\n    # Text\n    ## Box 1\n  stringr::str_replace_all('box1h1', '') %>% \n  stringr::str_replace_all('box1text1', '') %>% \n  stringr::str_replace_all('box1text2', '') %>%\n  stringr::str_replace_all('box1text3', '') %>%\n  stringr::str_replace_all('box1text4', '') %>% \n    ## Box 2 \n  stringr::str_replace_all('box2h1', '') %>% \n  stringr::str_replace_all('box2text1', '') %>% \n  stringr::str_replace_all('box2text2', '') %>% \n  stringr::str_replace_all('box2text3', '') %>% \n #### Convert back to .svg\n  svgtools::read_svg(.) \n  \n# Save output\nsvgtools::write_svg(\n  graph_dbd_06, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_dbd_06.svg\"))\n\n\n\ngraph_dbd_07\n\n# Create graph\ngraph_dbd_07 <- graph_base_dbd %>% \n  toString() %>%\n  #### GROUP: Population\n   # Circles\n  stringr::str_replace_all('\"#00B050\"', '\"#04316A\"') %>% \n   # Label\n  stringr::str_replace(\"symbol1label\", \"Population\") %>%\n  #### GROUP: Construct\n    # Shape\n  stringr::str_replace_all('\"#FFFF00\"', '\"#04316A\"') %>% \n    # Label\n  stringr::str_replace('symbol2label', 'Konstrukt') %>% \n  #### GROUP: Text bubbles (Population and Construct)\n    # Shape\n  stringr::str_replace_all('\"#7F6000\"', '\"#7F600000\"') %>%  \n    # Text\n  stringr::str_replace(\"bubble1label1\", \"\") %>% \n  stringr::str_replace(\"bubble1label2\", \"\") %>%  \n  stringr::str_replace(\"bubble2label1\", \"\") %>% \n  stringr::str_replace(\"bubble2label2\", \"\") %>% \n  #### GROUP: Survey\n    # Symbol\n  stringr::str_replace_all('\"#70AD47\"', '\"#04316A\"') %>%  \n    # Label\n  stringr::str_replace_all('label_box_3', 'Survey') %>% \n  #### GROUP: Free Text(Survey)\n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff8080\"',\n    'style=\"fill:#ff808000\"') %>%  \n    # Text\n  stringr::str_replace(\"surveylabel1\", \"\") %>% \n  stringr::str_replace(\"surveylabel2\", \"\") %>% \n  stringr::str_replace(\"surveylabel3\", \"\") %>% \n  #### GROUP: Links (moved to avoid replacement of color)\n  # stringr::str_replace_all('\"#7F7F7F\"', '\"#7F7F7F\"') %>% # CHANGE color\n  #### GROUP: SNS Twitter\n    # Box & Symbol\n  stringr::str_replace_all('\"#00B0F0\"', '\"#04316A\"') %>% # \n    # Cirles\n  stringr::str_replace_all('\"#0066FF\"', '\"#7F7F7F\"') %>% #   \n  stringr::str_replace_all('\"#FF0000\"', '\"#04316A\"') %>% # \n    # Label\n  stringr::str_replace_all('User', '') %>% \n  #### GROUP: SNS Populations\n    # Boxes & Symbols\n  stringr::str_replace_all('\"#C55A11\"', '\"#04316A\"') %>% # \n    # Cirles\n  stringr::str_replace_all('\"#800000\"', '\"#7F7F7F\"') %>% #   \n  stringr::str_replace_all('\"#9900FF\"', '\"#04316A\"') %>% # \n  #### GROUP: SNS Population Textboxes\n    # Shape\n  stringr::str_replace_all('stroke=\"#7030A0\"','stroke=\"#E6002E\"') %>% # CHANGE stroke\n  stringr::str_replace_all('fill=\"#7030A0\"','fill=\"#FFFFFF\"') %>% # CHANGE fill\n    # Bulletpoints\n  stringr::str_replace_all('style=\"fill:#ff00ff\"','style=\"fill:#E6002E\"') %>% # CHANGE color\n    # Text\n    ## Box 1\n  stringr::str_replace_all('box1h1', 'Traces') %>% # CHANGE text\n  stringr::str_replace_all('box1text1', 'Tweet') %>% # CHANGE text\n  stringr::str_replace_all('box1text2', 'Retweet') %>% # CHANGE text\n  stringr::str_replace_all('box1text3', 'Like') %>% # CHANGE text\n  stringr::str_replace_all('box1text4', 'Reply') %>% # CHANGE text \n    ## Box 2 \n  stringr::str_replace_all('box2h1', 'Traces') %>% # CHANGE text\n  stringr::str_replace_all('box2text1', 'Queries') %>% # CHANGE text\n  stringr::str_replace_all('box2text2', 'Visits') %>% # CHANGE text\n  stringr::str_replace_all('box2text3', 'Standort') %>% # CHANGE text\n #### Convert back to .svg\n  svgtools::read_svg(.) \n  \n# Save output\nsvgtools::write_svg(\n  graph_dbd_07, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_dbd_07.svg\"))\n\n\n\ngraph_dbd_08\n\n# Create graph\ngraph_dbd_08 <- graph_base_dbd %>% \n  toString() %>%\n  #### GROUP: Population\n   # Circles\n  stringr::str_replace_all('\"#00B050\"', '\"#04316A\"') %>% \n   # Label\n  stringr::str_replace(\"symbol1label\", \"Population\") %>%\n  #### GROUP: Construct\n    # Shape\n  stringr::str_replace_all('\"#FFFF00\"', '\"#04316A\"') %>% \n    # Label\n  stringr::str_replace('symbol2label', 'Konstrukt') %>% \n  #### GROUP: Text bubbles (Population and Construct)\n    # Shape\n  stringr::str_replace_all('\"#7F6000\"', '\"#7F600000\"') %>%  \n    # Text\n  stringr::str_replace(\"bubble1label1\", \"\") %>% \n  stringr::str_replace(\"bubble1label2\", \"\") %>%  \n  stringr::str_replace(\"bubble2label1\", \"\") %>% \n  stringr::str_replace(\"bubble2label2\", \"\") %>% \n  #### GROUP: Survey\n    # Symbol\n  stringr::str_replace_all('\"#70AD47\"', '\"#70AD4700\"') %>% # HIDE symbol\n    # Label\n  stringr::str_replace_all('label_box_3', '') %>% # HIDE label\n  #### GROUP: Free Text(Survey)\n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff8080\"',\n    'style=\"fill:#ff808000\"') %>%  \n    # Text\n  stringr::str_replace(\"surveylabel1\", \"\") %>% \n  stringr::str_replace(\"surveylabel2\", \"\") %>% \n  stringr::str_replace(\"surveylabel3\", \"\") %>% \n  #### GROUP: Links (moved to avoid replacement of color)\n  stringr::str_replace_all('\"#7F7F7F\"', '\"#7F7F7F00\"') %>% # HIDE links\n  #### GROUP: SNS Twitter\n    # Box & Symbol\n  stringr::str_replace_all('\"#00B0F0\"', '\"#04316A\"') %>% # \n    # Cirles\n  stringr::str_replace_all('\"#0066FF\"', '\"#7F7F7F\"') %>% #   \n  stringr::str_replace_all('\"#FF0000\"', '\"#04316A\"') %>% # \n    # Label\n  stringr::str_replace_all('User', '') %>% \n  #### GROUP: SNS Populations\n    # Boxes & Symbols\n  stringr::str_replace_all('\"#C55A11\"', '\"#04316A\"') %>% # \n    # Cirles\n  stringr::str_replace_all('\"#800000\"', '\"#7F7F7F\"') %>% #   \n  stringr::str_replace_all('\"#9900FF\"', '\"#04316A\"') %>% # \n  #### GROUP: SNS Population Textboxes\n    # Shape\n  stringr::str_replace_all('\"#7030A0\"', '\"#7030A000\"') %>% # Hide shape\n    # Bulletpoints\n  stringr::str_replace_all('style=\"fill:#ff00ff\"', 'style=\"fill:#ff00ff00\"') %>% # Hide color\n    # Text\n    ## Box 1\n  stringr::str_replace_all('box1h1', '') %>% # Hide text\n  stringr::str_replace_all('box1text1', '') %>% # Hide text\n  stringr::str_replace_all('box1text2', '') %>% # Hide text\n  stringr::str_replace_all('box1text3', '') %>% # Hide text\n  stringr::str_replace_all('box1text4', '') %>% # Hide text\n    ## Box 2 \n  stringr::str_replace_all('box2h1', '') %>% # Hide text\n  stringr::str_replace_all('box2text1', '') %>% # Hide text\n  stringr::str_replace_all('box2text2', '') %>% # Hide text\n  stringr::str_replace_all('box2text3', '') %>% # Hide text\n #### Convert back to .svg\n  svgtools::read_svg(.) \n  \n# Save output\nsvgtools::write_svg(\n  graph_dbd_08, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_dbd_08.svg\"))\n\n\n\ngraph_dbd_09\n\n# Create graph\ngraph_dbd_09 <- graph_base_dbd %>% \n  toString() %>%\n  #### GROUP: Population\n   # Circles\n  stringr::str_replace_all('\"#00B050\"', '\"#04316A20\"') %>% # APLHA circles\n   # Label\n  stringr::str_replace(\"symbol1label\", \"Population\") %>%\n  #### GROUP: Construct\n    # Shape\n  stringr::str_replace_all('\"#FFFF00\"', '\"#04316A\"') %>% \n    # Label\n  stringr::str_replace('symbol2label', 'Konstrukt') %>% \n  #### GROUP: Text bubbles (Population and Construct)\n    # Shape\n  stringr::str_replace_all('stroke=\"#7F6000\"','stroke=\"#E6002E\"') %>% # CHANGE stroke\n  stringr::str_replace_all('fill=\"#7F6000\"', 'fill=\"#ffffff\"') %>% # CHANGE fill\n    # Text\n  stringr::str_replace(\"bubble1label1\", \"Alle deutschsprachigen\") %>% # CHANGE text\n  stringr::str_replace(\"bubble1label2\", \"Twitter-Nutzer*Innen\") %>% # CHANGE text \n  stringr::str_replace(\"bubble2label1\", \"Empfänglichkeit\") %>% # CHANGE text\n  stringr::str_replace(\"bubble2label2\", \"für Fake News\") %>% # CHANGE text \n  #### GROUP: Survey\n    # Symbol\n  stringr::str_replace_all('\"#70AD47\"', '\"#70AD4700\"') %>% \n    # Label\n  stringr::str_replace_all('label_box_3', '') %>%\n  #### GROUP: Free Text(Survey)\n    # Bulletpoints\n  stringr::str_replace_all(\n    'style=\"fill:#ff8080\"',\n    'style=\"fill:#ff808000\"') %>%  \n    # Text\n  stringr::str_replace(\"surveylabel1\", \"\") %>% \n  stringr::str_replace(\"surveylabel2\", \"\") %>% \n  stringr::str_replace(\"surveylabel3\", \"\") %>% \n  #### GROUP: Links (moved to avoid replacement of color)\n  stringr::str_replace_all('\"#7F7F7F\"', '\"#7F7F7F00\"') %>% \n  #### GROUP: SNS Twitter\n    # Box & Symbol\n  # stringr::str_replace_all('\"#00B0F0\"', '\"#04316A\"') %>% # \n    # Cirles\n  stringr::str_replace_all('\"#0066FF\"', '\"#E6002E\"') %>% #   \n  stringr::str_replace_all('\"#FF0000\"', '\"#0066FF\"') %>% # \n    # Label\n  stringr::str_replace_all('User', '') %>% \n  #### GROUP: SNS Populations\n    # Boxes & Symbols\n  stringr::str_replace_all('\"#C55A11\"', '\"#04316A\"') %>% # \n    # Cirles\n  stringr::str_replace_all('\"#800000\"', '\"#7F7F7F\"') %>% #   \n  stringr::str_replace_all('\"#9900FF\"', '\"#04316A\"') %>% # \n  #### GROUP: SNS Population Textboxes\n    # Shape\n  stringr::str_replace_all('\"#7030A0\"', '\"#7030A000\"') %>% \n    # Bulletpoints\n  stringr::str_replace_all('style=\"fill:#ff00ff\"', 'style=\"fill:#ff00ff00\"') %>% \n    # Text\n    ## Box 1\n  stringr::str_replace_all('box1h1', '') %>% \n  stringr::str_replace_all('box1text1', '') %>% \n  stringr::str_replace_all('box1text2', '') %>% \n  stringr::str_replace_all('box1text3', '') %>% \n  stringr::str_replace_all('box1text4', '') %>% \n    ## Box 2 \n  stringr::str_replace_all('box2h1', '') %>% \n  stringr::str_replace_all('box2text1', '') %>% \n  stringr::str_replace_all('box2text2', '') %>% \n  stringr::str_replace_all('box2text3', '') %>% \n #### Convert back to .svg\n  svgtools::read_svg(.) \n  \n# Save output\nsvgtools::write_svg(\n  graph_dbd_09, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_dbd_09.svg\"))"
  },
  {
    "objectID": "content/03-dbd-data_collection/slides/scripts/graph_creation.html#api",
    "href": "content/03-dbd-data_collection/slides/scripts/graph_creation.html#api",
    "title": "graph_creation",
    "section": "API",
    "text": "API\n\nGet base graphic\n\ngraph_base_api <- svgtools::read_svg(\n  here(\"content/03-dbd-data_collection/slides/graphs/graph_api-01_base.svg\")) \n\n\n\ngraph_api_01\n\ngraph_api_01 <- graph_base_api %>% \n  toString() %>%\n  #### GROUP 1: programmer\n  stringr::str_replace_all('\"#FF0000\"', '\"#E6002E\"') %>% # CHANGE color\n  #### GROUP 2: server\n  stringr::str_replace_all('\"#FFC000\"', '\"#FFC00000\"') %>% # CHANGE color\n  #### GROUP 3: reference\n  stringr::str_replace_all('\"#92D050\"', '\"#92D05000\"') %>% # CHANGE color\n  #### GROUP 4: developer\n  stringr::str_replace_all('\"#00B050\"', '\"#00B05000\"') %>% # CHANGE color \n  #### GROUP 5: client\n  stringr::str_replace_all('\"#00B0F0\"', '\"#00B0F000\"') %>% # CHANGE color\n  #### GROUP 6: cloud\n  stringr::str_replace_all('\"#0070C0\"', '\"#0070C000\"') %>% # CHANGE color\n  #### Convert back to .svg\n  svgtools::read_svg(.) \n\n# Save output\nsvgtools::write_svg(\n  graph_api_01, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_api_01.svg\"))\n\n\n\ngraph_api_02\n\ngraph_api_02 <- graph_base_api %>% \n  toString() %>%\n  #### GROUP 1: programmer\n  stringr::str_replace_all('\"#FF0000\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP 2: server\n  stringr::str_replace_all('\"#FFC000\"', '\"#E6002E\"') %>% # CHANGE color\n  #### GROUP 3: reference\n  stringr::str_replace_all('\"#92D050\"', '\"#92D05000\"') %>% # CHANGE color\n  #### GROUP 4: developer\n  stringr::str_replace_all('\"#00B050\"', '\"#00B05000\"') %>% # CHANGE color \n  #### GROUP 5: client\n  stringr::str_replace_all('\"#00B0F0\"', '\"#00B0F000\"') %>% # CHANGE color\n  #### GROUP 6: cloud\n  stringr::str_replace_all('\"#0070C0\"', '\"#0070C000\"') %>% # CHANGE color\n  #### Convert back to .svg\n  svgtools::read_svg(.) \n\n# Save output\nsvgtools::write_svg(\n  graph_api_02, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_api_02.svg\"))\n\n\n\ngraph_api_03\n\ngraph_api_03 <- graph_base_api %>% \n  toString() %>%\n  #### GROUP 1: programmer\n  stringr::str_replace_all('\"#FF0000\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP 2: server\n  stringr::str_replace_all('\"#FFC000\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP 3: reference\n  stringr::str_replace_all('\"#92D050\"', '\"#E6002E\"') %>% # CHANGE color\n  #### GROUP 4: developer\n  stringr::str_replace_all('\"#00B050\"', '\"#00B05000\"') %>% # CHANGE color \n  #### GROUP 5: client\n  stringr::str_replace_all('\"#00B0F0\"', '\"#00B0F000\"') %>% # CHANGE color\n  #### GROUP 6: cloud\n  stringr::str_replace_all('\"#0070C0\"', '\"#0070C000\"') %>% # CHANGE color\n  #### Convert back to .svg\n  svgtools::read_svg(.) \n\n# Save output\nsvgtools::write_svg(\n  graph_api_03, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_api_03.svg\"))\n\n\n\ngraph_api_04\n\ngraph_api_04 <- graph_base_api %>% \n  toString() %>%\n  #### GROUP 1: programmer\n  stringr::str_replace_all('\"#FF0000\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP 2: server\n  stringr::str_replace_all('\"#FFC000\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP 3: reference\n  stringr::str_replace_all('\"#92D050\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP 4: developer\n  stringr::str_replace_all('\"#00B050\"', '\"#E6002E\"') %>% # CHANGE color \n  #### GROUP 5: client\n  stringr::str_replace_all('\"#00B0F0\"', '\"#00B0F000\"') %>% # CHANGE color\n  #### GROUP 6: cloud\n  stringr::str_replace_all('\"#0070C0\"', '\"#0070C000\"') %>% # CHANGE color\n  #### Convert back to .svg\n  svgtools::read_svg(.) \n\n# Save output\nsvgtools::write_svg(\n  graph_api_04, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_api_04.svg\"))\n\n\n\ngraph_api_05\n\ngraph_api_05 <- graph_base_api %>% \n  toString() %>%\n  #### GROUP 1: programmer\n  stringr::str_replace_all('\"#FF0000\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP 2: server\n  stringr::str_replace_all('\"#FFC000\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP 3: reference\n  stringr::str_replace_all('\"#92D050\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP 4: developer\n  stringr::str_replace_all('\"#00B050\"', '\"#04316A\"') %>% # CHANGE color \n  #### GROUP 5: client\n  stringr::str_replace_all('\"#00B0F0\"', '\"#E6002E\"') %>% # CHANGE color\n  #### GROUP 6: cloud\n  stringr::str_replace_all('\"#0070C0\"', '\"#0070C000\"') %>% # CHANGE color\n  #### Convert back to .svg\n  svgtools::read_svg(.) \n\n# Save output\nsvgtools::write_svg(\n  graph_api_05, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_api_05.svg\"))\n\n\n\ngraph_api_06\n\ngraph_api_06 <- graph_base_api %>% \n  toString() %>%\n  #### GROUP 1: programmer\n  stringr::str_replace_all('\"#FF0000\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP 2: server\n  stringr::str_replace_all('\"#FFC000\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP 3: reference\n  stringr::str_replace_all('\"#92D050\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP 4: developer\n  stringr::str_replace_all('\"#00B050\"', '\"#04316A\"') %>% # CHANGE color \n  #### GROUP 5: client\n  stringr::str_replace_all('\"#00B0F0\"', '\"#04316A\"') %>% # CHANGE color\n  #### GROUP 6: cloud\n  stringr::str_replace_all('\"#0070C0\"', '\"#E6002E\"') %>% # CHANGE color\n  #### Convert back to .svg\n  svgtools::read_svg(.) \n\n# Save output\nsvgtools::write_svg(\n  graph_api_06, \n  here(\"content/03-dbd-data_collection/slides/graphs/graph_api_06.svg\"))"
  },
  {
    "objectID": "content/04-api_access-twitter/04-exercise.html",
    "href": "content/04-api_access-twitter/04-exercise.html",
    "title": "Showcase",
    "section": "",
    "text": "library(academictwitteR) # Collecting the data\nlibrary(tidyverse) # Preparation of the data\nlibrary(quanteda) # Text mining\nlibrary(quanteda.textstats) # Text statistics\nlibrary(quanteda.textplots) # Visualisation of text data\nlibrary(here)\nlibrary(sjmisc)\nlibrary(lubridate)\nlibrary(ggthemes)\nlibrary(ggpubr)"
  },
  {
    "objectID": "content/04-api_access-twitter/04-exercise.html#set-personal-bearer-token",
    "href": "content/04-api_access-twitter/04-exercise.html#set-personal-bearer-token",
    "title": "Showcase",
    "section": "Set personal bearer token",
    "text": "Set personal bearer token\n\npersonal_bearer_token <- \"INSERT BEARER TOKEN HERE\""
  },
  {
    "objectID": "content/04-api_access-twitter/04-exercise.html#mining-tweets-hashtags",
    "href": "content/04-api_access-twitter/04-exercise.html#mining-tweets-hashtags",
    "title": "Showcase",
    "section": "Mining tweets: hashtag(s)",
    "text": "Mining tweets: hashtag(s)\n\nData collection\n\nget_all_tweets(\n    query = \"#Karneval\", \n    start_tweets = \"2022-11-11T00:00:00Z\",\n    end_tweets = \"2022-11-13T12:00:00Z\",\n    file = \"karneval\",\n    data_path = \"data/raw_karneval/\",\n    n = 100000,\n    #bearer_token = personal_bearer_token\n  )\n\n\nRead data from disc\n\ntweets_karneval <- bind_tweets(\n  data_path = here(\"content/04-api_access-twitter/data/raw_karneval\"),\n  output_format = \"tidy\") %>% \n    mutate(\n    datetime = ymd_hms(created_at),\n    date = date(datetime),\n    hour = hour(datetime),\n    min  = minute(datetime),\n    hms  = hms::as_hms(datetime),\n    hm   = hms::parse_hm(hms)\n  )\n\n\n\n\nData analysis\n\nOverview of dataset\n\ntweets_karneval %>% glimpse\n\nRows: 2,637\nColumns: 37\n$ tweet_id               <chr> \"1590981439457464321\", \"1590981390123892740\", \"…\n$ user_username          <chr> \"truthonearth3\", \"Auno94\", \"HaiAriane\", \"ElCobr…\n$ text                   <chr> \"RT @faqyoutoo: Ab heute 11:11 h dürfen wir uns…\n$ created_at             <chr> \"2022-11-11T08:15:08.000Z\", \"2022-11-11T08:14:5…\n$ lang                   <chr> \"de\", \"de\", \"de\", \"und\", \"und\", \"und\", \"und\", \"…\n$ possibly_sensitive     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE,…\n$ author_id              <chr> \"1486309822832287746\", \"2745840416\", \"995739344…\n$ source                 <chr> \"Twitter for Android\", \"Twitter for Android\", \"…\n$ conversation_id        <chr> \"1590981439457464321\", \"1590981390123892740\", \"…\n$ in_reply_to_user_id    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ user_protected         <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ user_description       <chr> \"I♥️Trump\\nAnd Trump ♥️US.\", \"Zertifizierter Alma…\n$ user_profile_image_url <chr> \"https://pbs.twimg.com/profile_images/158736121…\n$ user_pinned_tweet_id   <chr> \"1591132482271080448\", \"1346065459297267712\", N…\n$ user_verified          <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ user_name              <chr> \"truthonearth\", \"Auno\", \"Pirschelbär\", \"Michael…\n$ user_created_at        <chr> \"2022-01-26T12:07:54.000Z\", \"2014-08-19T17:42:0…\n$ user_location          <chr> NA, \"Köln, Deutschland\", NA, \"Bonn, Germany\", N…\n$ user_url               <chr> NA, \"https://t.co/lodH0xUg0C\", NA, NA, NA, NA, …\n$ retweet_count          <int> 2, 0, 0, 178, 178, 178, 178, 178, 178, 178, 178…\n$ like_count             <int> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ quote_count            <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ user_tweet_count       <int> 840, 5851, 495, 110593, 5045, 1351, 932, 29967,…\n$ user_list_count        <int> 0, 0, 0, 77, 0, 1, 1, 0, 0, 1, 1, 5, 3, 73, 18,…\n$ user_followers_count   <int> 268, 197, 18, 1260, 54, 109, 71, 372, 126, 145,…\n$ user_following_count   <int> 1102, 1124, 118, 476, 436, 403, 411, 748, 1339,…\n$ sourcetweet_type       <chr> \"retweeted\", NA, NA, \"retweeted\", \"retweeted\", …\n$ sourcetweet_id         <chr> \"1590967777762086912\", NA, NA, \"159097615399271…\n$ sourcetweet_text       <chr> \"Ab heute 11:11 h dürfen wir uns wieder verklei…\n$ sourcetweet_lang       <chr> \"de\", NA, NA, \"und\", \"und\", \"und\", \"und\", \"und\"…\n$ sourcetweet_author_id  <chr> \"946619921454247937\", NA, NA, \"529561909\", \"529…\n$ datetime               <dttm> 2022-11-11 08:15:08, 2022-11-11 08:14:57, 2022…\n$ date                   <date> 2022-11-11, 2022-11-11, 2022-11-11, 2022-11-11…\n$ hour                   <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,…\n$ min                    <int> 15, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 12,…\n$ hms                    <time> 08:15:08, 08:14:57, 08:14:29, 08:14:29, 08:13:…\n$ hm                     <time> 08:15:00, 08:14:00, 08:14:00, 08:14:00, 08:13:…\n\n\n\n\nLanguage of tweets\n\nfrq(tweets_karneval$lang, sort.frq = \"desc\")\n\nx <character> \n# total N=2637 valid N=2637 mean=6.85 sd=5.06\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nde    | 2180 | 82.67 |   82.67 |  82.67\nund   |  201 |  7.62 |    7.62 |  90.29\nen    |  112 |  4.25 |    4.25 |  94.54\nqme   |   43 |  1.63 |    1.63 |  96.17\nes    |   18 |  0.68 |    0.68 |  96.85\ntr    |   10 |  0.38 |    0.38 |  97.23\nfr    |    9 |  0.34 |    0.34 |  97.57\nnl    |    9 |  0.34 |    0.34 |  97.91\npl    |    8 |  0.30 |    0.30 |  98.22\nin    |    7 |  0.27 |    0.27 |  98.48\nno    |    7 |  0.27 |    0.27 |  98.75\nqht   |    6 |  0.23 |    0.23 |  98.98\nca    |    5 |  0.19 |    0.19 |  99.17\nda    |    4 |  0.15 |    0.15 |  99.32\ncs    |    3 |  0.11 |    0.11 |  99.43\neu    |    3 |  0.11 |    0.11 |  99.54\nfi    |    3 |  0.11 |    0.11 |  99.66\ncy    |    2 |  0.08 |    0.08 |  99.73\net    |    2 |  0.08 |    0.08 |  99.81\nja    |    2 |  0.08 |    0.08 |  99.89\nlt    |    2 |  0.08 |    0.08 |  99.96\nis    |    1 |  0.04 |    0.04 | 100.00\n<NA>  |    0 |  0.00 |    <NA> |   <NA>\n\n\n\n\nTweets over time\n\ntweets_karneval %>% \n  ggplot(aes(hour)) +\n  geom_bar() +\n  facet_grid(cols = vars(date)) +\n  theme_pubr()\n\n\n\n\n\n\nMost frequent time (HH:MM) of sending tweets\n\ntweets_karneval %>%\n  frq(hm,\n      sort.frq = \"desc\", \n      min.frq = 10)\n\nhm <numeric> \n# total N=2637 valid N=2637 mean=48463.12 sd=16629.59\n\nValue    |    N | Raw % | Valid % | Cum. %\n------------------------------------------\n10:11:00 |   59 |  2.24 |    2.24 |   2.24\n10:12:00 |   14 |  0.53 |    0.53 |   2.77\n10:13:00 |   12 |  0.46 |    0.46 |   3.22\n16:54:00 |   12 |  0.46 |    0.46 |   3.68\n10:10:00 |   10 |  0.38 |    0.38 |   4.06\n11:29:00 |   10 |  0.38 |    0.38 |   4.44\n16:53:00 |   10 |  0.38 |    0.38 |   4.82\nn < 10   | 2510 | 95.18 |   95.18 | 100.00\n<NA>     |    0 |  0.00 |    <NA> |   <NA>\n\n\n\n\nUser with the most tweets\n\ntweets_karneval %>% \n  frq(user_username,\n      sort.frq = \"desc\", \n      min.frq = 5)\n\nuser_username <character> \n# total N=2637 valid N=2637 mean=976.88 sd=562.18\n\nValue           |    N | Raw % | Valid % | Cum. %\n-------------------------------------------------\nGun17170309     |   65 |  2.46 |    2.46 |   2.46\nberlinaffaires  |   28 |  1.06 |    1.06 |   3.53\nBeratungszimmer |   19 |  0.72 |    0.72 |   4.25\nbartl_katrin    |   16 |  0.61 |    0.61 |   4.85\nrammwars        |   15 |  0.57 |    0.57 |   5.42\nfree_cart9      |   12 |  0.46 |    0.46 |   5.88\nploetschkoepp   |   12 |  0.46 |    0.46 |   6.33\nKespelHelau     |   11 |  0.42 |    0.42 |   6.75\nkryptomania84   |   10 |  0.38 |    0.38 |   7.13\nneussentweetet  |   10 |  0.38 |    0.38 |   7.51\nRaissoussama1   |   10 |  0.38 |    0.38 |   7.89\nRaissRaiss2     |    9 |  0.34 |    0.34 |   8.23\nexpress24       |    8 |  0.30 |    0.30 |   8.53\npolizei_nrw_k   |    8 |  0.30 |    0.30 |   8.84\nhedonist_redux  |    7 |  0.27 |    0.27 |   9.10\nPfantomaus      |    7 |  0.27 |    0.27 |   9.37\nrudibruns       |    7 |  0.27 |    0.27 |   9.63\nWDRaktuell      |    7 |  0.27 |    0.27 |   9.90\nGermanAtPompey  |    6 |  0.23 |    0.23 |  10.13\nIamIllgner      |    6 |  0.23 |    0.23 |  10.35\nMadameK_tweetet |    6 |  0.23 |    0.23 |  10.58\nStreamDESupport |    6 |  0.23 |    0.23 |  10.81\ntonight_news    |    6 |  0.23 |    0.23 |  11.04\nTrendsanalysed  |    6 |  0.23 |    0.23 |  11.26\narnohb112       |    5 |  0.19 |    0.19 |  11.45\ncarsten_fiedler |    5 |  0.19 |    0.19 |  11.64\ndieBasisKoeln   |    5 |  0.19 |    0.19 |  11.83\nDieFRAKTIONKoln |    5 |  0.19 |    0.19 |  12.02\nHeike_T_        |    5 |  0.19 |    0.19 |  12.21\nHowie591        |    5 |  0.19 |    0.19 |  12.40\njoergprante     |    5 |  0.19 |    0.19 |  12.59\nJoker_6278      |    5 |  0.19 |    0.19 |  12.78\nKSTA            |    5 |  0.19 |    0.19 |  12.97\nlibertaeredeju  |    5 |  0.19 |    0.19 |  13.16\nMarktzyniker    |    5 |  0.19 |    0.19 |  13.35\nMieterV         |    5 |  0.19 |    0.19 |  13.54\nmuesztaerrieh   |    5 |  0.19 |    0.19 |  13.73\nTrixiMaus502    |    5 |  0.19 |    0.19 |  13.92\nwieichfm        |    5 |  0.19 |    0.19 |  14.11\nn < 5           | 2265 | 85.89 |   85.89 | 100.00\n<NA>            |    0 |  0.00 |    <NA> |   <NA>\n\n\n\n\nDisplay tweets from most freuquent users\n\ntweets_karneval %>% \n  filter(user_username == \"Gun17170309\") %$% \n  glue::glue(\n    \"Tweet-ID: {tweet_id} \n    Inhalt:\n    {text}\\n\\n\") %>% \n  head()\n\nTweet-ID: 1591125966809878528 \nInhalt:\n#FünfteJahreszeit #Fasching #Faschingsbeginn #fastnacht #Karneval  \n💙💚💛🧡❤️🤎💜💙\nhttps://t.co/peg7JWAKdN\nKarnevals-Klassiker: Rheinische Krapfen🤡 https://t.co/Am7A84C4Dj\n\nTweet-ID: 1591120408102961153 \nInhalt:\nRT @Gun17170309: #FünfteJahreszeit #Fasching #Faschingsbeginn #fastnacht #Karneval  \n💙💚💛🧡❤️🤎💜💙\nhttps://t.co/vZxzhaeUNY\nFasching Berliner😋 h…\n\nTweet-ID: 1591120391904587777 \nInhalt:\nRT @Gun17170309: #FünfteJahreszeit #Fasching #Faschingsbeginn #fastnacht #Karneval  \n💙💚💛🧡❤️🤎💜💙\nhttps://t.co/6XKwts10gk\nTätä-Tätä – Das Karn…\n\nTweet-ID: 1591120367321419776 \nInhalt:\nRT @Gun17170309: #FünfteJahreszeit #Fasching #Faschingsbeginn #fastnacht #Karneval  \n💙💚💛🧡❤️🤎💜💙\nSo schnell gehen Berliner, Kreppel, Krapfenr…\n\nTweet-ID: 1591120292436643840 \nInhalt:\n#FünfteJahreszeit #Fasching #Faschingsbeginn #fastnacht #Karneval  \n💙💚💛🧡❤️🤎💜💙\nSo schnell gehen Berliner, Kreppel, Krapfenräppel, Pfannkuchen, Krapfen,\nin Frankreich: „Boule de Berlin“\nin England „Jelly Doughnut“\n. . . einfach selber backen, lecker!\nhttps://t.co/mw3W1iynuL https://t.co/WQ45iMavLn\n\nTweet-ID: 1591119195928166400 \nInhalt:\nRT @Gun17170309: https://t.co/mw3W1igegD \n#FünfteJahreszeit #Fasching #Faschingsbeginn #fastnacht #Karneval  \n💙💚💛🧡❤️🤎💜💙\n\n\n\ntweets_karneval %>% \n  filter(user_username == \"berlinaffaires\") %$% \n  glue::glue(\n    \"Tweet-ID: {tweet_id} \n    Inhalt:\n    {text}\\n\\n\") %>% \n  head()\n\nTweet-ID: 1591076515110060033 \nInhalt:\nRT @berlinaffaires: #HauptstadtStudio #SanktMartin #Karneval #Fastnacht #Fasching #ElfteElfte der Himmel über Berlin\nLuftrettung Christoph…\n\nTweet-ID: 1591076479663624192 \nInhalt:\n#HauptstadtStudio #SanktMartin #Karneval #Fastnacht #Fasching #ElfteElfte der Himmel über Berlin\nLuftrettung Christoph Callsign fliegt immer noch hin und her #DHXCA #CHX31 #H135 #MASH #HelicopterSpotting https://t.co/OETymAoOL1\n\nTweet-ID: 1591055201598701568 \nInhalt:\nRT @berlinaffaires: #HauptstadtStudio #SanktMartin #Karneval #Fastnacht #Fasching #ElfteElfte der Himmel über Berlin\nLuftrettung Christoph…\n\nTweet-ID: 1591055143579004928 \nInhalt:\n#HauptstadtStudio #SanktMartin #Karneval #Fastnacht #Fasching #ElfteElfte der Himmel über Berlin\nLuftrettung Christoph Callsign #DHXCA #CHX31 #H135 #MASH #HelicopterSpotting #Funkturm https://t.co/jv22RwAgS9\n\nTweet-ID: 1591043114181218306 \nInhalt:\nRT @DresdenNews: #Karneval im #PieschenerRathaus: Am 11.11 übernehmen um 11:11 Uhr die #Närrinnen und #Narren der Initiative Karneval in #P…\n\nTweet-ID: 1591042750408929280 \nInhalt:\nRT @bernhardboeth: @hessenschau Was haben wir den ganzen Vormittag auf diesen Moment hingefiebert #Fasching #Fastnacht #Karneval https://t.…"
  },
  {
    "objectID": "content/04-api_access-twitter/04-exercise.html#mining-tweets-profiles",
    "href": "content/04-api_access-twitter/04-exercise.html#mining-tweets-profiles",
    "title": "Showcase",
    "section": "Mining tweets: profile(s)",
    "text": "Mining tweets: profile(s)\n\nData collection\n\nget_all_tweets(\n    users = c(\"elonmusk\"),\n    start_tweets = \"2020-11-11T00:00:00Z\",\n    end_tweets = \"2022-11-13T12:00:00Z\",\n    file = \"elonmusk\",\n    data_path = here(\"content/04-api_access-twitter/data/raw_elonmusk/\"),\n    n = 100000,\n    bearer_token = personal_bearer_token\n  )\n\n\nRead data from disc\n\ntweets_musk <- bind_tweets(\n  data_path = here(\"content/04-api_access-twitter/data/raw_elonmusk\"),\n  # data_path = \"data/raw_karneval\",\n  output_format = \"tidy\") %>% \n    mutate(\n    datetime = ymd_hms(created_at),\n    date = date(datetime),\n    hour = hour(datetime),\n    min  = minute(datetime),\n    hms  = hms::as_hms(datetime),\n    hm   = hms::parse_hm(hms)\n  )\n\n\n\n\nData analysis\n\nOverview of dataset\n\ntweets_musk %>% glimpse\n\nRows: 7,255\nColumns: 37\n$ tweet_id               <chr> \"1336809767574982658\", \"1336808486022258688\", \"…\n$ user_username          <chr> \"elonmusk\", \"elonmusk\", \"elonmusk\", \"elonmusk\",…\n$ text                   <chr> \"Fuel header tank pressure was low during landi…\n$ conversation_id        <chr> \"1336808486022258688\", \"1336808486022258688\", \"…\n$ author_id              <chr> \"44196397\", \"44196397\", \"44196397\", \"44196397\",…\n$ in_reply_to_user_id    <chr> \"44196397\", NA, \"4914384040\", \"3101588527\", \"34…\n$ source                 <chr> \"Twitter for iPhone\", \"Twitter for iPhone\", \"Tw…\n$ possibly_sensitive     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ created_at             <chr> \"2020-12-09T23:07:39.000Z\", \"2020-12-09T23:02:3…\n$ lang                   <chr> \"en\", \"en\", \"und\", \"en\", \"en\", \"en\", \"en\", \"en\"…\n$ user_created_at        <chr> \"2009-06-02T20:12:29.000Z\", \"2009-06-02T20:12:2…\n$ user_description       <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ user_profile_image_url <chr> \"https://pbs.twimg.com/profile_images/159096873…\n$ user_name              <chr> \"Elon Musk\", \"Elon Musk\", \"Elon Musk\", \"Elon Mu…\n$ user_protected         <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ user_verified          <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,…\n$ user_location          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ user_url               <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ user_pinned_tweet_id   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ retweet_count          <int> 6987, 9775, 349, 348, 1055, 7732, 10719, 324, 5…\n$ like_count             <int> 96961, 106142, 15267, 13204, 22581, 0, 170964, …\n$ quote_count            <int> 2027, 2953, 93, 111, 177, 0, 5106, 92, 143, 12,…\n$ user_tweet_count       <int> 20290, 20290, 20290, 20290, 20290, 20290, 20290…\n$ user_list_count        <int> 102027, 102027, 102027, 102027, 102027, 102027,…\n$ user_followers_count   <int> 115405919, 115405919, 115405919, 115405919, 115…\n$ user_following_count   <int> 130, 130, 130, 130, 130, 130, 130, 130, 130, 13…\n$ sourcetweet_type       <chr> NA, \"quoted\", NA, NA, NA, \"retweeted\", NA, NA, …\n$ sourcetweet_id         <chr> NA, \"1336777137391456256\", NA, NA, NA, \"1336349…\n$ sourcetweet_text       <chr> NA, \"Watch Starship high-altitude test live → h…\n$ sourcetweet_lang       <chr> NA, \"en\", NA, NA, NA, \"en\", NA, NA, NA, NA, \"en…\n$ sourcetweet_author_id  <chr> NA, \"34743251\", NA, NA, NA, \"34743251\", NA, NA,…\n$ datetime               <dttm> 2020-12-09 23:07:39, 2020-12-09 23:02:34, 2020…\n$ date                   <date> 2020-12-09, 2020-12-09, 2020-12-09, 2020-12-08…\n$ hour                   <int> 23, 23, 18, 18, 16, 16, 16, 2, 2, 0, 22, 20, 18…\n$ min                    <int> 7, 2, 13, 5, 57, 55, 44, 51, 50, 16, 16, 13, 14…\n$ hms                    <time> 23:07:39, 23:02:34, 18:13:21, 18:05:28, 16:57:…\n$ hm                     <time> 23:07:00, 23:02:00, 18:13:00, 18:05:00, 16:57:…\n\n\n\n\nTweets over time\n\ntweets_musk %>% \n  ggplot(aes(date)) +\n  geom_bar() +\n  theme_pubr()\n\n\n\n\n\n\nTweets with the most likes\n\ntweets_musk %>% \n  filter(is.na(sourcetweet_type)) %>% \n  arrange(-like_count) %>% \n  select(text, created_at, like_count) %>% \n  head(10)\n\n# A tibble: 10 × 3\n   text                                                          creat…¹ like_…²\n   <chr>                                                         <chr>     <int>\n 1 \"Next I’m buying Coca-Cola to put the cocaine back in\"        2022-0… 4767770\n 2 \"I hope that even my worst critics remain on Twitter, becaus… 2022-0… 3221746\n 3 \"Let’s make Twitter maximum fun!\"                             2022-0… 2641791\n 4 \"\\U0001f680\\U0001f4ab♥️ Yesss!!! ♥️\\U0001f4ab\\U0001f680 https:… 2022-0… 2599811\n 5 \"Listen, I can’t do miracles ok https://t.co/z7dvLMUXy8\"      2022-0… 2572036\n 6 \"the bird is freed\"                                           2022-1… 2497902\n 7 \"Comedy is now legal on Twitter\"                              2022-1… 2400045\n 8 \"https://t.co/kGncG7Hs3M\"                                     2022-1… 1893281\n 9 \"If I die under mysterious circumstances, it’s been nice kno… 2022-0… 1891028\n10 \"The extreme antibody reaction from those who fear free spee… 2022-0… 1647281\n# … with abbreviated variable names ¹​created_at, ²​like_count\n\n\n\n\nTweets with the most retweets\n\ntweets_musk %>% \n  filter(is.na(sourcetweet_type)) %>% \n  arrange(-retweet_count) %>% \n  select(text, created_at, retweet_count) %>% \n  head(10)\n\n# A tibble: 10 × 3\n   text                                                          creat…¹ retwe…²\n   <chr>                                                         <chr>     <int>\n 1 \"Next I’m buying Coca-Cola to put the cocaine back in\"        2022-0…  679688\n 2 \"I hope that even my worst critics remain on Twitter, becaus… 2022-0…  366807\n 3 \"the bird is freed\"                                           2022-1…  357937\n 4 \"\\U0001f680\\U0001f4ab♥️ Yesss!!! ♥️\\U0001f4ab\\U0001f680 https:… 2022-0…  346717\n 5 \"Comedy is now legal on Twitter\"                              2022-1…  261224\n 6 \"Listen, I can’t do miracles ok https://t.co/z7dvLMUXy8\"      2022-0…  212059\n 7 \"https://t.co/Q9OjlJhi7f\"                                     2022-0…  207978\n 8 \"Let’s make Twitter maximum fun!\"                             2022-0…  193913\n 9 \"The extreme antibody reaction from those who fear free spee… 2022-0…  191983\n10 \"Entering Twitter HQ – let that sink in! https://t.co/D68z4K… 2022-1…  190507\n# … with abbreviated variable names ¹​created_at, ²​retweet_count\n\n\n\n\nProportion of tweets\n\ntweets_musk %>% \n  frq(sourcetweet_type)\n\nsourcetweet_type <character> \n# total N=7255 valid N=489 mean=1.74 sd=0.44\n\nValue     |    N | Raw % | Valid % | Cum. %\n-------------------------------------------\nquoted    |  125 |  1.72 |   25.56 |  25.56\nretweeted |  364 |  5.02 |   74.44 | 100.00\n<NA>      | 6766 | 93.26 |    <NA> |   <NA>\n\n\n\n\nLanguate of tweets\n\ntweets_musk %>% \n  frq(lang)\n\nlang <character> \n# total N=7255 valid N=7255 mean=14.37 sd=9.86\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nar    |    2 |  0.03 |    0.03 |   0.03\nart   |   12 |  0.17 |    0.17 |   0.19\nbg    |    1 |  0.01 |    0.01 |   0.21\nca    |   10 |  0.14 |    0.14 |   0.34\ncs    |    1 |  0.01 |    0.01 |   0.36\ncy    |    1 |  0.01 |    0.01 |   0.37\nda    |    6 |  0.08 |    0.08 |   0.45\nde    |   24 |  0.33 |    0.33 |   0.79\nel    |    2 |  0.03 |    0.03 |   0.81\nen    | 5915 | 81.53 |   81.53 |  82.34\nes    |   15 |  0.21 |    0.21 |  82.55\net    |    6 |  0.08 |    0.08 |  82.63\neu    |    3 |  0.04 |    0.04 |  82.67\nfr    |   20 |  0.28 |    0.28 |  82.95\nhi    |    1 |  0.01 |    0.01 |  82.96\nht    |    2 |  0.03 |    0.03 |  82.99\nhu    |    2 |  0.03 |    0.03 |  83.02\nin    |    8 |  0.11 |    0.11 |  83.13\nis    |    1 |  0.01 |    0.01 |  83.14\nit    |    6 |  0.08 |    0.08 |  83.23\nja    |    5 |  0.07 |    0.07 |  83.29\nlt    |    3 |  0.04 |    0.04 |  83.34\nnl    |    4 |  0.06 |    0.06 |  83.39\nno    |    1 |  0.01 |    0.01 |  83.40\npl    |    5 |  0.07 |    0.07 |  83.47\npt    |    7 |  0.10 |    0.10 |  83.57\nqam   |   40 |  0.55 |    0.55 |  84.12\nqht   |    1 |  0.01 |    0.01 |  84.14\nqme   |   81 |  1.12 |    1.12 |  85.25\nqst   |    5 |  0.07 |    0.07 |  85.32\nro    |    3 |  0.04 |    0.04 |  85.36\nru    |    7 |  0.10 |    0.10 |  85.46\nsl    |    2 |  0.03 |    0.03 |  85.49\ntl    |   59 |  0.81 |    0.81 |  86.30\ntr    |    5 |  0.07 |    0.07 |  86.37\nuk    |    1 |  0.01 |    0.01 |  86.38\nund   |  817 | 11.26 |   11.26 |  97.64\nvi    |    1 |  0.01 |    0.01 |  97.66\nzh    |    1 |  0.01 |    0.01 |  97.67\nzxx   |  169 |  2.33 |    2.33 | 100.00\n<NA>  |    0 |  0.00 |    <NA> |   <NA>"
  },
  {
    "objectID": "content/04-api_access-twitter/04-exercise.html#text-mining",
    "href": "content/04-api_access-twitter/04-exercise.html#text-mining",
    "title": "Showcase",
    "section": "Text mining",
    "text": "Text mining\n\nPreprocessing\n\nremove_html <- \"&amp;|&lt;|&gt;\"\n\ntweets_en <- tweets_musk %>% \n  filter(lang == \"en\",\n         is.na(sourcetweet_type)) %>% \n  select(tweet_id, text, user_username) %>% \n  mutate(text = str_remove_all(text, remove_html))\n\n\ntweets_en_corpus <- corpus(tweets_en,\n                           docid_field = \"tweet_id\",\n                           text_field = \"text\")\n\n\ntweets_en_tokens <- \n  tokens(tweets_en_corpus,\n         remove_punct = TRUE,\n         remove_numbers = TRUE,\n         remove_symbols = TRUE,\n         remove_url = TRUE) %>% \n  tokens_tolower() %>% \n  tokens_remove(stopwords(\"english\"))\n\n\ntweets_en_dfm <- dfm(tweets_en_tokens)\n\n\n\nAnalysis\n\nTop Hashtags\n\ntag_dfm <- dfm_select(tweets_en_dfm, pattern = \"#*\")\ntoptag <- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n[1] \"#2\"                  \"#mars\"               \"#resistanceisfutile\"\n[4] \"#1\"                  \"#freespeech\"        \n\n\n\n\nTop Mentions\n\nuser_dfm <- dfm_select(tweets_en_dfm, pattern = \"@*\")\ntopuser <- names(topfeatures(user_dfm, 50))\nhead(topuser, 10)\n\n [1] \"@wholemarsblog\"  \"@spacex\"         \"@teslaownerssv\"  \"@ppathole\"      \n [5] \"@tesla\"          \"@erdayastronaut\" \"@billym2k\"       \"@teslarati\"     \n [9] \"@sawyermerritt\"  \"@evafoxu\"       \n\n\n\n\nExclude Hashtags & Metions\n\ntweets_en_clean <- tweets_en_dfm %>% \n  dfm_remove(pattern = \"@*\") %>% \n  dfm_remove(pattern = \"#*\")\n\n\n\nTop 10 features\n\nterm_freq_en <- textstat_frequency(tweets_en_clean)\nhead(term_freq_en, n = 10)\n\n   feature frequency rank docfreq group\n1    tesla       354    1     328   all\n2     just       227    2     225   all\n3     good       215    3     209   all\n4    great       187    4     183   all\n5     much       184    5     180   all\n6     like       172    6     168   all\n7      can       169    7     165   all\n8   people       167    8     156   all\n9  twitter       156    9     147   all\n10     one       146   10     144   all\n\n\n\n\nWordcloud with Top 50 features\n\ntextplot_wordcloud(tweets_en_clean, max_words = 50)"
  },
  {
    "objectID": "content/04-api_access-twitter/04-slides.html",
    "href": "content/04-api_access-twitter/04-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you find the slides for the fourth session:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome."
  },
  {
    "objectID": "content/04-api_access-twitter/slides/index.html#seminarplan",
    "href": "content/04-api_access-twitter/slides/index.html#seminarplan",
    "title": "API-Access – Twitter",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\nSitzung\nDatum\nThema\nReferent*Innen\n\n\n\n\n1\n26.10.2022\nKick-Off Session\nChristoph Adrian\n\n\n2\n02.11.2022\nDBD: Einführung und Überblick\nChristoph Adrian\n\n\n3\n09.11.2022\nDBD: Datenerhebung\nChristoph Adrian\n\n\n4\n16.11.2022\nAPI-Access (I): Twitter\nFalk\n\n\n5\n23.11.2022\nAPI-Access (II): YouTube\nDenisov\n\n\n6\n30.11.2022\nAPI-Access (II): Reddit\nLandauer\n\n\n7\n07.12.2022\nWebscraping: TikTok\nBrand & Kocher\n\n\n8\n14.12.2022\nExkurs: DBD Analyse mit R\nChristoph Adrian\n\n\n\n\nWEIHNACHTSPAUSE\n\n\n\n9\n12.01.2023\nESM: m-path\nDörr\n\n\n10\n19.01.2023\nData Donations\nHofmann & Wierzbicki\n\n\n11\n26.01.2023\nPUFFER\n\n\n\n12\n02.02.2023\nGuest Lecture: Linking DBD & Survey data\nJohannes Breuer\n\n\n13\n09.02.2023\nSemesterabschluss & Evaluation\nChristoph Adrian\n\n\n\n\n▶️"
  },
  {
    "objectID": "content/04-api_access-twitter/slides/index.html#vielen-dank-für-ihre-fragen",
    "href": "content/04-api_access-twitter/slides/index.html#vielen-dank-für-ihre-fragen",
    "title": "API-Access – Twitter",
    "section": "Vielen Dank für Ihre Fragen!",
    "text": "Vielen Dank für Ihre Fragen!\n\n\nWas genau kann unter “N-gram analysis” verstanden werden?\n\n\n\n\nUm semantische Muster der Daten bei der Twitter-Forschung zu erkennen, wendet man häufig Text-Mining-Methoden an. Wie wird dabei mit ironischen Aussagen oder Umgangssprache umgegangen? Sind solche Analysemethoden somit überhaupt verlässlich oder kann man diesbezüglich mit einer hohen Fehleranfälligkeit rechnen?\n\n\n\n\nViele Studien verwenden Twitter in der Forschung. Dabei werden verschiedene Methoden eingesetzt. Welche Methode (aus Table 4 des Pflichtpapers) würden Sie am meisten empfehlen? Welche ist am aussagekräftigsten? Oder ist das Kontext-bezogen und eine allgemeine Aussage kann nicht getätigt werden?\n\n\n\n\nInteressant wäre es in diesem Kontext vielleicht nicht nur Twitter, sondern eine andere Social-Media-Plattform zu betrachten, zum Bsp. Facebook. Würden die dominierenden Themen der Facebook-basierten Forschung ähnlich wie bei Twitter ausfallen? Wie sieht wohl die Entwicklung der Themen in den letzten Jahren aus?\n\n\n\n\nInteressant wäre es in diesem Kontext vielleicht nicht nur Twitter, sondern eine andere Social-Media-Plattform zu betrachten, zum Bsp. Facebook. Würden die dominierenden Themen der Facebook-basierten Forschung ähnlich wie bei Twitter ausfallen? Wie sieht wohl die Entwicklung der Themen in den letzten Jahren aus?\n\n\n\n\nTwitter wird unter anderem für „Opinion Mining“ verwendet. Jedoch kann es gerade bei polarisierenden Themen vorkommen, dass sich vor allem Personen mit extremer Meinung äußern und die Personen mittig im Spektrum zurückhalten. Wie kann man einen solchen Bias ausgleichen?\n\n\n\n\nEs kann nur ein kleiner Bruchteil der Twitter-Kommunikation per API abgefragt werden, der zudem noch veraltet ist. Inwieweit sind Trend-Analysen überhaupt möglich? Gibt es Möglichkeiten Twitter-Kommunikation zu “streamen”?\n\n\n\n\nEine gute Studie lebt von den Kriterien der Validität, Objektivität und Reliabilität. Inwieweit können diese Kriterien bei einer Twitter-Analyse erfüllt werden?\n\n\n\n\nWas sind die Gründe für den Rückgang der Forschung mit Twitter-Daten rund um das Thema Marketing?"
  },
  {
    "objectID": "content/04-api_access-twitter/slides/index.html#suchanfrage-erstellen-daten-sammeln",
    "href": "content/04-api_access-twitter/slides/index.html#suchanfrage-erstellen-daten-sammeln",
    "title": "API-Access – Twitter",
    "section": "Suchanfrage erstellen & Daten sammeln",
    "text": "Suchanfrage erstellen & Daten sammeln\nTwitter case study - Teil 1: Suchen & Sammeln\n\n\n\n\n\n\nFür die Nutzung des Tweet Downloader benötigen Sie einen Bearer Token. Diesen finden Sie auf MS Teams.\n\n\n\nWäheln Sie eine der folgenden Szenarien aus und erstellen Sie eine Suchanfrage mit Hilfe des Twitter Query Builder:\n\nAlle Tweets eines bestimmten Accounts seit dem 01.10.2022\nAlle Tweets (ohne Retweets) mit dem Hashtag #karneval oder #wokwm am letzten Wochenende\n\n… und ziehen sich die Tweets als .csv & .json mit Hilfe des Tweet Downloader (benötigt Chrome, Edge oder Opera)"
  },
  {
    "objectID": "content/04-api_access-twitter/slides/index.html#format-der-daten-potentielle-analysen",
    "href": "content/04-api_access-twitter/slides/index.html#format-der-daten-potentielle-analysen",
    "title": "API-Access – Twitter",
    "section": "Format der Daten & potentielle Analyse(n)",
    "text": "Format der Daten & potentielle Analyse(n)\nTwitter case study - Teil 2: Sichten & Konzipieren\n\nInspizieren Sie anschließend die Datensätze und schauen Sie, welche Informationen Ihnen geliefert werden.\n\nWie unterscheiden sich die Datenformate?\n\n\n\n\nAuf Basis der gesichteten Daten …\n\nWie würden Sie die Daten analyiseren bzw. welche Aspekte würden Sie sich anschauen?\nWelche potentielle Fragestellungen könnten Sie spontan entwickeln?"
  },
  {
    "objectID": "content/04-api_access-twitter/slides/index.html#setup-und-datensammlung-mit-r",
    "href": "content/04-api_access-twitter/slides/index.html#setup-und-datensammlung-mit-r",
    "title": "API-Access – Twitter",
    "section": "Setup und Datensammlung mit R",
    "text": "Setup und Datensammlung mit R\n#Karneval im Fokus: Hintergrund und Datensammelung\n\n\nAlle Tweets herunterladen, die\n\n#Karneval enthalten\nzwischen 00:00 am 11.11.2022 und 12:00 am 13.12.2022 erschienen sind\n\nWeiterverarbeitung und Auswertung der Daten mit R, Fokus auf Paket academictwitteR (Barrie & Ho, 2021)\n\nSammlung der Daten mit academictwitteR:\n\nget_all_tweets(\n  query = \"#Karneval\", \n  start_tweets = \"2022-11-11T00:00:00Z\",\n  end_tweets = \"2022-11-13T12:00:00Z\",\n  file = \"karneval\",\n  data_path = \"data/raw_karneval/\",\n  n = 100000\n  )"
  },
  {
    "objectID": "content/04-api_access-twitter/slides/index.html#kurzer-blick-in-die-daten",
    "href": "content/04-api_access-twitter/slides/index.html#kurzer-blick-in-die-daten",
    "title": "API-Access – Twitter",
    "section": "Kurzer Blick in die Daten",
    "text": "Kurzer Blick in die Daten\n#Karneval im Fokus: Datenübersicht\n\ntweets_karneval %>% glimpse()\n\nRows: 2,637\nColumns: 37\n$ tweet_id               <chr> \"1590981439457464321\", \"1590981390123892740\", \"…\n$ user_username          <chr> \"truthonearth3\", \"Auno94\", \"HaiAriane\", \"ElCobr…\n$ text                   <chr> \"RT @faqyoutoo: Ab heute 11:11 h dürfen wir uns…\n$ created_at             <chr> \"2022-11-11T08:15:08.000Z\", \"2022-11-11T08:14:5…\n$ lang                   <chr> \"de\", \"de\", \"de\", \"und\", \"und\", \"und\", \"und\", \"…\n$ possibly_sensitive     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE,…\n$ author_id              <chr> \"1486309822832287746\", \"2745840416\", \"995739344…\n$ source                 <chr> \"Twitter for Android\", \"Twitter for Android\", \"…\n$ conversation_id        <chr> \"1590981439457464321\", \"1590981390123892740\", \"…\n$ in_reply_to_user_id    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ user_protected         <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ user_description       <chr> \"I♥️Trump\\nAnd Trump ♥️US.\", \"Zertifizierter Alma…\n$ user_profile_image_url <chr> \"https://pbs.twimg.com/profile_images/158736121…\n$ user_pinned_tweet_id   <chr> \"1591132482271080448\", \"1346065459297267712\", N…\n$ user_verified          <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ user_name              <chr> \"truthonearth\", \"Auno\", \"Pirschelbär\", \"Michael…\n$ user_created_at        <chr> \"2022-01-26T12:07:54.000Z\", \"2014-08-19T17:42:0…\n$ user_location          <chr> NA, \"Köln, Deutschland\", NA, \"Bonn, Germany\", N…\n$ user_url               <chr> NA, \"https://t.co/lodH0xUg0C\", NA, NA, NA, NA, …\n$ retweet_count          <int> 2, 0, 0, 178, 178, 178, 178, 178, 178, 178, 178…\n$ like_count             <int> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ quote_count            <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ user_tweet_count       <int> 840, 5851, 495, 110593, 5045, 1351, 932, 29967,…\n$ user_list_count        <int> 0, 0, 0, 77, 0, 1, 1, 0, 0, 1, 1, 5, 3, 73, 18,…\n$ user_followers_count   <int> 268, 197, 18, 1260, 54, 109, 71, 372, 126, 145,…\n$ user_following_count   <int> 1102, 1124, 118, 476, 436, 403, 411, 748, 1339,…\n$ sourcetweet_type       <chr> \"retweeted\", NA, NA, \"retweeted\", \"retweeted\", …\n$ sourcetweet_id         <chr> \"1590967777762086912\", NA, NA, \"159097615399271…\n$ sourcetweet_text       <chr> \"Ab heute 11:11 h dürfen wir uns wieder verklei…\n$ sourcetweet_lang       <chr> \"de\", NA, NA, \"und\", \"und\", \"und\", \"und\", \"und\"…\n$ sourcetweet_author_id  <chr> \"946619921454247937\", NA, NA, \"529561909\", \"529…\n$ datetime               <dttm> 2022-11-11 08:15:08, 2022-11-11 08:14:57, 2022…\n$ date                   <date> 2022-11-11, 2022-11-11, 2022-11-11, 2022-11-11…\n$ hour                   <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,…\n$ min                    <int> 15, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 12,…\n$ hms                    <time> 08:15:08, 08:14:57, 08:14:29, 08:14:29, 08:13:…\n$ hm                     <time> 08:15:00, 08:14:00, 08:14:00, 08:14:00, 08:13:…"
  },
  {
    "objectID": "content/04-api_access-twitter/slides/index.html#deutsch-hashtags-englisch",
    "href": "content/04-api_access-twitter/slides/index.html#deutsch-hashtags-englisch",
    "title": "API-Access – Twitter",
    "section": "Deutsch, Hashtags, Englisch …",
    "text": "Deutsch, Hashtags, Englisch …\n#Karneval im Fokus: Sprache der Tweets\n\n\n\ntable(tweets_karneval$lang)\n\n\n  ca   cs   cy   da   de   en   es   et   eu   fi   fr   in   is   ja   lt   nl \n   5    3    2    4 2180  112   18    2    3    3    9    7    1    2    2    9 \n  no   pl  qht  qme   tr  und \n   7    8    6   43   10  201 \n\n\n\n\nfrq(tweets_karneval$lang, sort.frq = \"desc\")\n\nx <character> \n# total N=2637 valid N=2637 mean=6.85 sd=5.06\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nde    | 2180 | 82.67 |   82.67 |  82.67\nund   |  201 |  7.62 |    7.62 |  90.29\nen    |  112 |  4.25 |    4.25 |  94.54\nqme   |   43 |  1.63 |    1.63 |  96.17\nes    |   18 |  0.68 |    0.68 |  96.85\ntr    |   10 |  0.38 |    0.38 |  97.23\nfr    |    9 |  0.34 |    0.34 |  97.57\nnl    |    9 |  0.34 |    0.34 |  97.91\npl    |    8 |  0.30 |    0.30 |  98.22\nin    |    7 |  0.27 |    0.27 |  98.48\nno    |    7 |  0.27 |    0.27 |  98.75\nqht   |    6 |  0.23 |    0.23 |  98.98\nca    |    5 |  0.19 |    0.19 |  99.17\nda    |    4 |  0.15 |    0.15 |  99.32\ncs    |    3 |  0.11 |    0.11 |  99.43\neu    |    3 |  0.11 |    0.11 |  99.54\nfi    |    3 |  0.11 |    0.11 |  99.66\ncy    |    2 |  0.08 |    0.08 |  99.73\net    |    2 |  0.08 |    0.08 |  99.81\nja    |    2 |  0.08 |    0.08 |  99.89\nlt    |    2 |  0.08 |    0.08 |  99.96\nis    |    1 |  0.04 |    0.04 | 100.00\n<NA>  |    0 |  0.00 |    <NA> |   <NA>\n\n\n\n\n\nqme = only hashtags\nund = unidentified"
  },
  {
    "objectID": "content/04-api_access-twitter/slides/index.html#am-11.11.-um-1011",
    "href": "content/04-api_access-twitter/slides/index.html#am-11.11.-um-1011",
    "title": "API-Access – Twitter",
    "section": "Am 11.11. um 10:11?",
    "text": "Am 11.11. um 10:11?\n#Karneval im Fokus: Tweets im Zeitverlauf\n\ntweets_karneval %>% \n  ggplot(aes(hour)) +\n  geom_bar() +\n  facet_grid(cols = vars(date)) +\n  theme_pubr()"
  },
  {
    "objectID": "content/04-api_access-twitter/slides/index.html#utc-statt-utc1",
    "href": "content/04-api_access-twitter/slides/index.html#utc-statt-utc1",
    "title": "API-Access – Twitter",
    "section": "UTC statt UTC+1",
    "text": "UTC statt UTC+1\n#Karneval im Fokus: Stärkste Konzentration von Tweets\n\ntweets_karneval %>%\n  frq(hm,\n      sort.frq = \"desc\", \n      min.frq = 10)\n\nhm <numeric> \n# total N=2637 valid N=2637 mean=48463.12 sd=16629.59\n\nValue    |    N | Raw % | Valid % | Cum. %\n------------------------------------------\n10:11:00 |   59 |  2.24 |    2.24 |   2.24\n10:12:00 |   14 |  0.53 |    0.53 |   2.77\n10:13:00 |   12 |  0.46 |    0.46 |   3.22\n16:54:00 |   12 |  0.46 |    0.46 |   3.68\n10:10:00 |   10 |  0.38 |    0.38 |   4.06\n11:29:00 |   10 |  0.38 |    0.38 |   4.44\n16:53:00 |   10 |  0.38 |    0.38 |   4.82\nn < 10   | 2510 | 95.18 |   95.18 | 100.00\n<NA>     |    0 |  0.00 |    <NA> |   <NA>"
  },
  {
    "objectID": "content/04-api_access-twitter/slides/index.html#ein-blick-auf-die-senderinnen",
    "href": "content/04-api_access-twitter/slides/index.html#ein-blick-auf-die-senderinnen",
    "title": "API-Access – Twitter",
    "section": "Ein Blick auf die Sender:Innen",
    "text": "Ein Blick auf die Sender:Innen\n#Karneval im Fokus: User mit den meisten Tweets\n\n\n\ntweets_karneval$user_username %>% \n  frq(sort.frq = \"desc\", \n      min.frq = 10)\n\nx <character> \n# total N=2637 valid N=2637 mean=976.88 sd=562.18\n\nValue           |    N | Raw % | Valid % | Cum. %\n-------------------------------------------------\nGun17170309     |   65 |  2.46 |    2.46 |   2.46\nberlinaffaires  |   28 |  1.06 |    1.06 |   3.53\nBeratungszimmer |   19 |  0.72 |    0.72 |   4.25\nbartl_katrin    |   16 |  0.61 |    0.61 |   4.85\nrammwars        |   15 |  0.57 |    0.57 |   5.42\nfree_cart9      |   12 |  0.46 |    0.46 |   5.88\nploetschkoepp   |   12 |  0.46 |    0.46 |   6.33\nKespelHelau     |   11 |  0.42 |    0.42 |   6.75\nkryptomania84   |   10 |  0.38 |    0.38 |   7.13\nneussentweetet  |   10 |  0.38 |    0.38 |   7.51\nRaissoussama1   |   10 |  0.38 |    0.38 |   7.89\nn < 10          | 2429 | 92.11 |   92.11 | 100.00\n<NA>            |    0 |  0.00 |    <NA> |   <NA>\n\n\n\n\n\ntweets_karneval %>% \n  filter(user_username == \"Gun17170309\") %$% \n  glue::glue(\n    \"Tweet-ID: {tweet_id} \n    Inhalt:\n    {text}\\n\\n\") %>% \n  head()\n\nTweet-ID: 1591125966809878528 \nInhalt:\n#FünfteJahreszeit #Fasching #Faschingsbeginn #fastnacht #Karneval  \n💙💚💛🧡❤️🤎💜💙\nhttps://t.co/peg7JWAKdN\nKarnevals-Klassiker: Rheinische Krapfen🤡 https://t.co/Am7A84C4Dj\n\nTweet-ID: 1591120408102961153 \nInhalt:\nRT @Gun17170309: #FünfteJahreszeit #Fasching #Faschingsbeginn #fastnacht #Karneval  \n💙💚💛🧡❤️🤎💜💙\nhttps://t.co/vZxzhaeUNY\nFasching Berliner😋 h…\n\nTweet-ID: 1591120391904587777 \nInhalt:\nRT @Gun17170309: #FünfteJahreszeit #Fasching #Faschingsbeginn #fastnacht #Karneval  \n💙💚💛🧡❤️🤎💜💙\nhttps://t.co/6XKwts10gk\nTätä-Tätä – Das Karn…\n\nTweet-ID: 1591120367321419776 \nInhalt:\nRT @Gun17170309: #FünfteJahreszeit #Fasching #Faschingsbeginn #fastnacht #Karneval  \n💙💚💛🧡❤️🤎💜💙\nSo schnell gehen Berliner, Kreppel, Krapfenr…\n\nTweet-ID: 1591120292436643840 \nInhalt:\n#FünfteJahreszeit #Fasching #Faschingsbeginn #fastnacht #Karneval  \n💙💚💛🧡❤️🤎💜💙\nSo schnell gehen Berliner, Kreppel, Krapfenräppel, Pfannkuchen, Krapfen,\nin Frankreich: „Boule de Berlin“\nin England „Jelly Doughnut“\n. . . einfach selber backen, lecker!\nhttps://t.co/mw3W1iynuL https://t.co/WQ45iMavLn\n\nTweet-ID: 1591119195928166400 \nInhalt:\nRT @Gun17170309: https://t.co/mw3W1igegD \n#FünfteJahreszeit #Fasching #Faschingsbeginn #fastnacht #Karneval  \n💙💚💛🧡❤️🤎💜💙\n\n\n\n\ntweets_karneval %>% \n  filter(user_username == \"berlinaffaires\") %$% \n  glue::glue(\n    \"Tweet-ID: {tweet_id} \n    Inhalt:\n    {text}\\n\\n\") %>% \n  head()\n\nTweet-ID: 1591076515110060033 \nInhalt:\nRT @berlinaffaires: #HauptstadtStudio #SanktMartin #Karneval #Fastnacht #Fasching #ElfteElfte der Himmel über Berlin\nLuftrettung Christoph…\n\nTweet-ID: 1591076479663624192 \nInhalt:\n#HauptstadtStudio #SanktMartin #Karneval #Fastnacht #Fasching #ElfteElfte der Himmel über Berlin\nLuftrettung Christoph Callsign fliegt immer noch hin und her #DHXCA #CHX31 #H135 #MASH #HelicopterSpotting https://t.co/OETymAoOL1\n\nTweet-ID: 1591055201598701568 \nInhalt:\nRT @berlinaffaires: #HauptstadtStudio #SanktMartin #Karneval #Fastnacht #Fasching #ElfteElfte der Himmel über Berlin\nLuftrettung Christoph…\n\nTweet-ID: 1591055143579004928 \nInhalt:\n#HauptstadtStudio #SanktMartin #Karneval #Fastnacht #Fasching #ElfteElfte der Himmel über Berlin\nLuftrettung Christoph Callsign #DHXCA #CHX31 #H135 #MASH #HelicopterSpotting #Funkturm https://t.co/jv22RwAgS9\n\nTweet-ID: 1591043114181218306 \nInhalt:\nRT @DresdenNews: #Karneval im #PieschenerRathaus: Am 11.11 übernehmen um 11:11 Uhr die #Närrinnen und #Narren der Initiative Karneval in #P…\n\nTweet-ID: 1591042750408929280 \nInhalt:\nRT @bernhardboeth: @hessenschau Was haben wir den ganzen Vormittag auf diesen Moment hingefiebert #Fasching #Fastnacht #Karneval https://t.…"
  },
  {
    "objectID": "content/04-api_access-twitter/slides/index.html#literatur",
    "href": "content/04-api_access-twitter/slides/index.html#literatur",
    "title": "API-Access – Twitter",
    "section": "Literatur",
    "text": "Literatur\n\n\nBarrie, C., & Ho, J. (2021). academictwitteR: An r package to access the twitter academic research product track v2 API endpoint. Journal of Open Source Software, 6(62), 3272. https://doi.org/10.21105/joss.03272\n\n\n\n\n\nDigital Behavioral Data"
  },
  {
    "objectID": "content/05-api_access-youtube/05-exercise.html#background",
    "href": "content/05-api_access-youtube/05-exercise.html#background",
    "title": "Showcase",
    "section": "Background",
    "text": "Background\nPractical application of the YouTube Data Tool (YTDT) using the example of Mai Thi Nguyen-Kim (  |  ) and her -Channel maiLab.\nYou can also open this showcase in other interactive and executable environments:"
  },
  {
    "objectID": "content/05-api_access-youtube/05-exercise.html#exercise-1",
    "href": "content/05-api_access-youtube/05-exercise.html#exercise-1",
    "title": "Showcase",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nUse the Channel Search site/function of the YTDT to find the (correct) channel ID for the  Channel maiLab.\nTherefore, enter “maiLab” in the field Search query and download the results as .csv.\nOpen the file and search extract the correct channel ID.\nHint: If in doubt, use Channel Info function to check if the selected ID matches the channel description.\n\n\n# Load packages\nlibrary(readr)\nlibrary(tidyverse)\n\n# Import data\nchannel_list <- read_csv(\n  here(\"content/05-api_access-youtube/data/channelsearch_channels50_2022_11_17-09_54_22.csv\"))\n\n# Preview data \nchannel_list %>% glimpse()\n\nRows: 50\nColumns: 10\n$ position        <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ id              <chr> \"UCyHDQ5C6z1NDmJ4g6SerW8g\", \"UC146qqkUMTrn4nfSSOTNwiA\"…\n$ title           <chr> \"maiLab\", \"musstewissen Chemie\", \"mailab\", \"MAILab_메…\n$ description     <chr> \"Holt euch einen Tee, Freunde der Sonne, macht es euch…\n$ publishedAt     <dttm> 2016-09-08 14:13:08, 2016-09-23 09:24:14, 2020-03-19 …\n$ defaultLanguage <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ country         <chr> \"DE\", \"DE\", \"DE\", \"KR\", NA, NA, NA, NA, NA, \"DE\", NA, …\n$ viewCount       <dbl> 127882489, 18039348, 21013, 5101, 11, 2, 2, 5004, 230,…\n$ subscriberCount <dbl> 1480000, 190000, 146, 26, 0, 1, 1, 9, 9, 66300, 0, 2, …\n$ videoCount      <dbl> 186, 45, 9, 104, 3, 1, 1, 24, 12, 51, 1, 6, 31, 1, 2, …\n\n\n\n# Get channel description with R\nchannel_list %>%\n  filter(title == \"maiLab\") %>%\n  select(id, title, description)\n\n# A tibble: 1 × 3\n  id                       title  description                                   \n  <chr>                    <chr>  <chr>                                         \n1 UCyHDQ5C6z1NDmJ4g6SerW8g maiLab Holt euch einen Tee, Freunde der Sonne, macht…"
  },
  {
    "objectID": "content/05-api_access-youtube/05-exercise.html#exercise-2",
    "href": "content/05-api_access-youtube/05-exercise.html#exercise-2",
    "title": "Showcase",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nWith help of the Video List site/function of the YTDT, get a list of all published videos of the channel maiLab.\nTherefore, use the extracted channel id and download the results as .csv.\nImport/preview the data.\n\n\n# Import data: video list\nvideo_list <- read_csv(\n  here(\"content/05-api_access-youtube/data/videolist_channel186_2022_11_17-10_20_11.csv\"))\n\n# Preview data \nvideo_list %>% glimpse()\n\nRows: 186\nColumns: 23\n$ position           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ channelId          <chr> \"UCyHDQ5C6z1NDmJ4g6SerW8g\", \"UCyHDQ5C6z1NDmJ4g6SerW…\n$ channelTitle       <chr> \"maiLab\", \"maiLab\", \"maiLab\", \"maiLab\", \"maiLab\", \"…\n$ videoId            <chr> \"IK5BZdnqMDU\", \"Mt50U4_ueR0\", \"-NMs56pQ9EE\", \"-9OvN…\n$ publishedAt        <dttm> 2022-09-18 16:00:18, 2022-06-09 04:30:04, 2022-05-…\n$ publishedAtSQL     <dttm> 2022-09-18 16:00:18, 2022-06-09 04:30:04, 2022-05-…\n$ videoTitle         <chr> \"Das Ende der Homöopathie | MAITHINK X\", \"Affenpock…\n$ videoDescription   <chr> \"Der vielleicht größte Abwasserskandal aller Zeiten…\n$ tags               <chr> \"Mai Thi Nguyen-Kim,Mai Thi,mai,nguyen,mailab,lab,m…\n$ videoCategoryId    <dbl> 28, 28, 28, 28, 22, 28, 22, 22, 22, 27, 22, 22, 28,…\n$ videoCategoryLabel <chr> \"Science & Technology\", \"Science & Technology\", \"Sc…\n$ duration           <chr> \"PT31M57S\", \"PT13M39S\", \"PT15M21S\", \"PT12M6S\", \"PT1…\n$ durationSec        <dbl> 1917, 819, 921, 726, 832, 1664, 306, 1065, 1325, 12…\n$ dimension          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ definition         <chr> \"hd\", \"hd\", \"hd\", \"hd\", \"hd\", \"hd\", \"hd\", \"hd\", \"hd…\n$ caption            <lgl> FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, T…\n$ thumbnail_maxres   <chr> \"https://i.ytimg.com/vi/IK5BZdnqMDU/maxresdefault.j…\n$ licensedContent    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ viewCount          <dbl> 1729562, 993841, 929302, 2125579, 3021542, 895038, …\n$ likeCount          <dbl> 83334, 56794, 64304, 123191, 236388, 77980, 73251, …\n$ dislikeCount       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ favoriteCount      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ commentCount       <dbl> 18716, 9141, 8204, 38823, 111200, 7905, 3831, 5723,…"
  },
  {
    "objectID": "content/05-api_access-youtube/05-exercise.html#different-location-parameters",
    "href": "content/05-api_access-youtube/05-exercise.html#different-location-parameters",
    "title": "Showcase",
    "section": "Different location parameters",
    "text": "Different location parameters\n\n# Load additional packages\nlibrary(sjmisc) \n\n# Get distribution parameters for selected variables\nvideo_list %>% \n  select(durationSec, viewCount, likeCount, favoriteCount, commentCount) %>% \n  descr()\n\n\n## Basic descriptive statistics\n\n           var    type         label   n NA.prc      mean        sd       se\n   durationSec numeric   durationSec 186      0    612.92    402.74    29.53\n     viewCount numeric     viewCount 186      0 687563.73 763588.15 55989.00\n     likeCount numeric     likeCount 186      0  31172.50  36815.40  2699.44\n favoriteCount numeric favoriteCount 186      0      0.00      0.00     0.00\n  commentCount numeric  commentCount 186      0   5598.28  11238.77   824.07\n       md   trimmed                   range       iqr skew\n    530.5    579.42          1825 (92-1917)    633.25 0.64\n 467371.5 562670.19 6671382 (21298-6692680) 782761.75 3.36\n  20970.5  24734.81    267376 (1063-268439)  37023.75 3.10\n      0.0      0.00                 0 (0-0)      0.00  NaN\n   2068.0   3106.24      111166 (34-111200)   5718.00 5.54\n\n\n\nMore detailed distribution for each variable\n\nvideo_list %>% \n  plot_frq(durationSec, viewCount, likeCount, commentCount, type = \"density\")\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]"
  },
  {
    "objectID": "content/05-api_access-youtube/05-exercise.html#in-depth-analysis",
    "href": "content/05-api_access-youtube/05-exercise.html#in-depth-analysis",
    "title": "Showcase",
    "section": "In-depth analysis",
    "text": "In-depth analysis\nBased on the findings of the previous section, let us take a closer look. Interestingly, although most of the varialbes have a left-sloping distribution, there are isolated outliers on the “right” edge.\nTherefore, the next goal is to find out which video(s) they are.\n\nTop 5 videos with the highest view count\n\nvideo_list %>% \n  arrange(-viewCount) %>% \n  select(videoTitle, publishedAt, viewCount, likeCount, commentCount) %>% \n  head()\n\n# A tibble: 6 × 5\n  videoTitle                     publishedAt         viewCount likeCount comme…¹\n  <chr>                          <dttm>                  <dbl>     <dbl>   <dbl>\n1 Corona geht gerade erst los    2020-04-02 07:00:00   6692680    268439   48174\n2 Impfpflicht ist OK             2021-11-14 07:00:11   3021542    236388  111200\n3 So endet Corona                2021-01-28 06:15:04   3020999    170219   28363\n4 Virologen-Vergleich            2020-04-19 07:00:22   2654200     92957   33773\n5 Rezo wissenschaftlich geprüft  2019-05-24 04:30:25   2605822    142990   16995\n6 Was ist jetzt mit dem Kokosöl? 2018-08-30 04:30:01   2362469     63198    6833\n# … with abbreviated variable name ¹​commentCount\n\n\n\n\nTop 5 videos with the highest comment count\n\nvideo_list %>% \n  arrange(-commentCount) %>% \n  select(videoTitle, publishedAt, viewCount, likeCount, commentCount) %>% \n  head()\n\n# A tibble: 6 × 5\n  videoTitle                         publishedAt         viewC…¹ likeC…² comme…³\n  <chr>                              <dttm>                <dbl>   <dbl>   <dbl>\n1 Impfpflicht ist OK                 2021-11-14 07:00:11 3021542  236388  111200\n2 Corona geht gerade erst los        2020-04-02 07:00:00 6692680  268439   48174\n3 7 kritische Fragen zur Impfung     2021-02-11 06:15:02 2068956   80075   38982\n4 Corona-Endlosschleife | Kommen wi… 2021-11-25 07:48:24 2125579  123191   38823\n5 Corona hat meine Meinung geändert  2020-10-08 04:30:01 1602957   74811   36543\n6 Virologen-Vergleich                2020-04-19 07:00:22 2654200   92957   33773\n# … with abbreviated variable names ¹​viewCount, ²​likeCount, ³​commentCount"
  },
  {
    "objectID": "content/05-api_access-youtube/05-slides.html",
    "href": "content/05-api_access-youtube/05-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you find the slides for the fifth session:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome."
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#seminarplan",
    "href": "content/05-api_access-youtube/slides/index.html#seminarplan",
    "title": "API-Access – YouTube",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\nSitzung\nDatum\nThema\nReferent*Innen\n\n\n\n\n1\n26.10.2022\nKick-Off Session\nChristoph Adrian\n\n\n2\n02.11.2022\nDBD: Einführung und Überblick\nChristoph Adrian\n\n\n3\n09.11.2022\nDBD: Datenerhebung\nChristoph Adrian\n\n\n4\n16.11.2022\nAPI-Access (I): Twitter\nFalk\n\n\n5\n23.11.2022\nAPI-Access (II): YouTube\nDenisov\n\n\n6\n30.11.2022\nAPI-Access (II): Reddit\nLandauer\n\n\n7\n07.12.2022\nWebscraping: TikTok\nBrand & Kocher\n\n\n8\n14.12.2022\nExkurs: DBD Analyse mit R\nChristoph Adrian\n\n\n\n\nWEIHNACHTSPAUSE\n\n\n\n9\n12.01.2023\nESM: m-path\nDörr\n\n\n10\n19.01.2023\nData Donations\nHofmann & Wierzbicki\n\n\n11\n26.01.2023\nPUFFER\n\n\n\n12\n02.02.2023\nGuest Lecture: Linking DBD & Survey data\nJohannes Breuer\n\n\n13\n09.02.2023\nSemesterabschluss & Evaluation\nChristoph Adrian\n\n\n\n\n▶️"
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#what-is-it-an-who-made-it",
    "href": "content/05-api_access-youtube/slides/index.html#what-is-it-an-who-made-it",
    "title": "API-Access – YouTube",
    "section": "What is it an who made it",
    "text": "What is it an who made it\nHintergrundinformationen YouTube Data Tools (Rieder, 2015)\n\nSammlung von einfachen Modulen zur Extraktion von Daten aus der YouTube-Plattform über die YouTube-API v3.\nKeine voll entwickelte Analysesoftware, sondern ein Mittel für Forschende, um Daten in Standarddateiformaten zu sammeln und in anderen Softwarepaketen weiter zu analysieren.\ngeschrieben, entwickelt und gepflegt von Bernhard Rieder (Universität Amsterdam)"
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#and-now-you",
    "href": "content/05-api_access-youtube/slides/index.html#and-now-you",
    "title": "API-Access – YouTube",
    "section": "And now … you!",
    "text": "And now … you!\nBeispielhafte Nutzung\n\nSuchen Sie sich einen YouTube-Kanal aus, auf dem die folgenden Analysen basieren sollen\nNutzen Sie die Channel Search des YTDT um die “korrekte” Channel ID zu identifizieren.\nLaden Sie mit Hilfe der Video List Seite/Funktion des YTDT eine Liste aller veröffentlichten Videos Ihres ausgewählten YouTube-Kanals herunter.\n\nOptional:\n\nSuchen Sie mit Hilfe der heruntergeladenen Liste der veröffentlichten Videos das Video mit den meisten Likes/Comments.\nDownloaden Sie die Kommentare des Videos mit Hilfe der Video Info des YTDT."
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#wissenschaftsjournalismus-auf-youtube",
    "href": "content/05-api_access-youtube/slides/index.html#wissenschaftsjournalismus-auf-youtube",
    "title": "API-Access – YouTube",
    "section": "Wissenschaftsjournalismus auf YouTube",
    "text": "Wissenschaftsjournalismus auf YouTube\nHIntergrund zu  maiLab\n\n\n\nseit 2016 bestehende YouTube-Kanal (früher schönschlau) von Mai Thi Nguyen-Kim\nist Teil des funk-Netzwerks\nbehandelt diverse Themen aus den Bereichen Natur- und Gesellschaftswissenschaften"
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#zusammenspiel-von-ytdt-r",
    "href": "content/05-api_access-youtube/slides/index.html#zusammenspiel-von-ytdt-r",
    "title": "API-Access – YouTube",
    "section": "Zusammenspiel von YTDT & R",
    "text": "Zusammenspiel von YTDT & R\n maiLab im Fokus: Datenimport mit R (1)\n\n# Load packages\nlibrary(readr)\nlibrary(tidyverse)\n\n# Import data\nchannel_list <- read_csv(\n  \"https://raw.githubusercontent.com/chrdrn/dbd_binder/main/data/05-youtube/channelsearch_channels50_2022_11_17-09_54_22.csv\")\n\n# Preview data \nchannel_list %>% glimpse()\n\nRows: 50\nColumns: 10\n$ position        <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ id              <chr> \"UCyHDQ5C6z1NDmJ4g6SerW8g\", \"UC146qqkUMTrn4nfSSOTNwiA\"…\n$ title           <chr> \"maiLab\", \"musstewissen Chemie\", \"mailab\", \"MAILab_메…\n$ description     <chr> \"Holt euch einen Tee, Freunde der Sonne, macht es euch…\n$ publishedAt     <dttm> 2016-09-08 14:13:08, 2016-09-23 09:24:14, 2020-03-19 …\n$ defaultLanguage <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ country         <chr> \"DE\", \"DE\", \"DE\", \"KR\", NA, NA, NA, NA, NA, \"DE\", NA, …\n$ viewCount       <dbl> 127882489, 18039348, 21013, 5101, 11, 2, 2, 5004, 230,…\n$ subscriberCount <dbl> 1480000, 190000, 146, 26, 0, 1, 1, 9, 9, 66300, 0, 2, …\n$ videoCount      <dbl> 186, 45, 9, 104, 3, 1, 1, 24, 12, 51, 1, 6, 31, 1, 2, …"
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#mit-channel-id-zur-video-list",
    "href": "content/05-api_access-youtube/slides/index.html#mit-channel-id-zur-video-list",
    "title": "API-Access – YouTube",
    "section": "Mit Channel ID zur Video List",
    "text": "Mit Channel ID zur Video List\n maiLab im Fokus: Datenimport mit R (2)\n\n# Import data: video list\nvideo_list <- read_csv(\n  \"https://raw.githubusercontent.com/chrdrn/dbd_binder/main/data/05-youtube/videolist_channel186_2022_11_17-10_20_11.csv\"\n)\n\n# Preview data \nvideo_list %>% glimpse()\n\nRows: 186\nColumns: 23\n$ position           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ channelId          <chr> \"UCyHDQ5C6z1NDmJ4g6SerW8g\", \"UCyHDQ5C6z1NDmJ4g6SerW…\n$ channelTitle       <chr> \"maiLab\", \"maiLab\", \"maiLab\", \"maiLab\", \"maiLab\", \"…\n$ videoId            <chr> \"IK5BZdnqMDU\", \"Mt50U4_ueR0\", \"-NMs56pQ9EE\", \"-9OvN…\n$ publishedAt        <dttm> 2022-09-18 16:00:18, 2022-06-09 04:30:04, 2022-05-…\n$ publishedAtSQL     <dttm> 2022-09-18 16:00:18, 2022-06-09 04:30:04, 2022-05-…\n$ videoTitle         <chr> \"Das Ende der Homöopathie | MAITHINK X\", \"Affenpock…\n$ videoDescription   <chr> \"Der vielleicht größte Abwasserskandal aller Zeiten…\n$ tags               <chr> \"Mai Thi Nguyen-Kim,Mai Thi,mai,nguyen,mailab,lab,m…\n$ videoCategoryId    <dbl> 28, 28, 28, 28, 22, 28, 22, 22, 22, 27, 22, 22, 28,…\n$ videoCategoryLabel <chr> \"Science & Technology\", \"Science & Technology\", \"Sc…\n$ duration           <chr> \"PT31M57S\", \"PT13M39S\", \"PT15M21S\", \"PT12M6S\", \"PT1…\n$ durationSec        <dbl> 1917, 819, 921, 726, 832, 1664, 306, 1065, 1325, 12…\n$ dimension          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ definition         <chr> \"hd\", \"hd\", \"hd\", \"hd\", \"hd\", \"hd\", \"hd\", \"hd\", \"hd…\n$ caption            <lgl> FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, T…\n$ thumbnail_maxres   <chr> \"https://i.ytimg.com/vi/IK5BZdnqMDU/maxresdefault.j…\n$ licensedContent    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ viewCount          <dbl> 1729562, 993841, 929302, 2125579, 3021542, 895038, …\n$ likeCount          <dbl> 83334, 56794, 64304, 123191, 236388, 77980, 73251, …\n$ dislikeCount       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ favoriteCount      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ commentCount       <dbl> 18716, 9141, 8204, 38823, 111200, 7905, 3831, 5723,…"
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#explorative-analyse-der-video-list",
    "href": "content/05-api_access-youtube/slides/index.html#explorative-analyse-der-video-list",
    "title": "API-Access – YouTube",
    "section": "Explorative Analyse der Video List",
    "text": "Explorative Analyse der Video List\n maiLab im Fokus: Datenauswertung mit R (1)\n\n# Load additional packages\nlibrary(sjmisc) \n\n# Get distribution parameters for selected variables\nvideo_list %>% \n  select(durationSec, viewCount, likeCount, favoriteCount, commentCount) %>% \n  descr() %>%\n  select(!c(var, type)) \n\n\n## Basic descriptive statistics\n\n         label   n NA.prc      mean        sd       se       md   trimmed\n   durationSec 186      0    612.92    402.74    29.53    530.5    579.42\n     viewCount 186      0 687563.73 763588.15 55989.00 467371.5 562670.19\n     likeCount 186      0  31172.50  36815.40  2699.44  20970.5  24734.81\n favoriteCount 186      0      0.00      0.00     0.00      0.0      0.00\n  commentCount 186      0   5598.28  11238.77   824.07   2068.0   3106.24\n                   range       iqr skew\n          1825 (92-1917)    633.25 0.64\n 6671382 (21298-6692680) 782761.75 3.36\n    267376 (1063-268439)  37023.75 3.10\n                 0 (0-0)      0.00  NaN\n      111166 (34-111200)   5718.00 5.54"
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#videoveröffentlichungen-im-zeitverlauf",
    "href": "content/05-api_access-youtube/slides/index.html#videoveröffentlichungen-im-zeitverlauf",
    "title": "API-Access – YouTube",
    "section": "Videoveröffentlichungen im Zeitverlauf",
    "text": "Videoveröffentlichungen im Zeitverlauf\n maiLab im Fokus: Datenauswertung mit R (2)\n\n# Load additional packages\nlibrary(sjPlot) \n\n# Create plot\nvideo_list %>% \n  mutate(year  = as.factor(year(publishedAt))) %>% \n  plot_frq(\n    year,\n    title = \"Video uploads on `maiLab` by year\")"
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#top-5-videos-mit-den-meisten-aufrufen",
    "href": "content/05-api_access-youtube/slides/index.html#top-5-videos-mit-den-meisten-aufrufen",
    "title": "API-Access – YouTube",
    "section": "Top 5 Videos mit den meisten Aufrufen",
    "text": "Top 5 Videos mit den meisten Aufrufen\n maiLab im Fokus: Datenauswertung mit R (3)\n\nvideo_list %>% \n  arrange(-viewCount) %>% \n  select(videoTitle, publishedAt, viewCount, likeCount, commentCount) %>% \n  head()\n\n# A tibble: 6 × 5\n  videoTitle                     publishedAt         viewCount likeCount comme…¹\n  <chr>                          <dttm>                  <dbl>     <dbl>   <dbl>\n1 Corona geht gerade erst los    2020-04-02 07:00:00   6692680    268439   48174\n2 Impfpflicht ist OK             2021-11-14 07:00:11   3021542    236388  111200\n3 So endet Corona                2021-01-28 06:15:04   3020999    170219   28363\n4 Virologen-Vergleich            2020-04-19 07:00:22   2654200     92957   33773\n5 Rezo wissenschaftlich geprüft  2019-05-24 04:30:25   2605822    142990   16995\n6 Was ist jetzt mit dem Kokosöl? 2018-08-30 04:30:01   2362469     63198    6833\n# … with abbreviated variable name ¹​commentCount"
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#top-5-videos-mit-den-meisten-kommentaren",
    "href": "content/05-api_access-youtube/slides/index.html#top-5-videos-mit-den-meisten-kommentaren",
    "title": "API-Access – YouTube",
    "section": "Top 5 Videos mit den meisten Kommentaren",
    "text": "Top 5 Videos mit den meisten Kommentaren\n maiLab im Fokus: Datenauswertung mit R (4)\n\nvideo_list %>% \n  arrange(-commentCount) %>% \n  select(videoTitle, publishedAt, viewCount, likeCount, commentCount) %>% \n  head()\n\n# A tibble: 6 × 5\n  videoTitle                         publishedAt         viewC…¹ likeC…² comme…³\n  <chr>                              <dttm>                <dbl>   <dbl>   <dbl>\n1 Impfpflicht ist OK                 2021-11-14 07:00:11 3021542  236388  111200\n2 Corona geht gerade erst los        2020-04-02 07:00:00 6692680  268439   48174\n3 7 kritische Fragen zur Impfung     2021-02-11 06:15:02 2068956   80075   38982\n4 Corona-Endlosschleife | Kommen wi… 2021-11-25 07:48:24 2125579  123191   38823\n5 Corona hat meine Meinung geändert  2020-10-08 04:30:01 1602957   74811   36543\n6 Virologen-Vergleich                2020-04-19 07:00:22 2654200   92957   33773\n# … with abbreviated variable names ¹​viewCount, ²​likeCount, ³​commentCount"
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#interactive-and-executable-environments",
    "href": "content/05-api_access-youtube/slides/index.html#interactive-and-executable-environments",
    "title": "API-Access – YouTube",
    "section": "Interactive and executable environments",
    "text": "Interactive and executable environments\nQuarto, Binder & Google Colab\nVerschiedene Möglichkeiten, um den Code bzw. das heutige Beispiel (“live”) zu reproduzieren:\n\nQuarto-Dokument auf Kurs-Homepage\nBinder: Repositories mit Jupyter-Notebooks in ausführbaren Umgebungen öffnen und bearbeiten\n\nGoogle Colab: gehosteter Jupyter-Notebook-Dienst, der keine EInrichtung erfordertund gleichzeitig kostenlosen Zugang zu Rechenressourcen einschließlich GPUs bietet"
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#vielen-dank-für-ihre-fragen",
    "href": "content/05-api_access-youtube/slides/index.html#vielen-dank-für-ihre-fragen",
    "title": "API-Access – YouTube",
    "section": "Vielen Dank für Ihre Fragen!",
    "text": "Vielen Dank für Ihre Fragen!\n\n\nIst es denkbar, dass YouTube eines Tages von anderen sozialen Plattformen wie Instagram oder Spotify überholt wird, an Bekanntheit verliert oder sogar ganz von der Bildfläche verschwindet?\n\n\n\n\nYoutube-Influencer verdienen etwa das Doppelte mit Werbeanzeigen auf YouTube, als wenn sie Werbeanzeigen auf Facebook oder Instagram schalten würden (2016). Kann das im Jahr 2022 immer noch bestätigt werden? Sind Werbeanzeigen auf YouTube erfolgreicher und haben eine weitere Reichweite im Vergleich mit Instagram?\n\n\n\n\nEs wird das Tool Webometric erwähnt, ein automatisiertes Instrument für die Sammlung von Social Media Daten über die API der Plattform YouTube. Wie sieht Webometric genau aus und wie werden die Daten über dieses Tool extrahiert?\n\n\n\n\nEs wird oft darüber geredet, dass das „Recommendation System” fehlerhaft sei und bestimmte Videos trotz mangelnde Performance trotzdem empfohlen werden oder in den Trends landen (z.B Amerikanische Late Night Shows). Wie kann in der Forschung verhindert werden, dass Resultate entstehen die nicht den tatsächlichen Verhaltensdaten der Konsumenten entstehen."
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#lets-discuss",
    "href": "content/05-api_access-youtube/slides/index.html#lets-discuss",
    "title": "API-Access – YouTube",
    "section": "Let’s discuss",
    "text": "Let’s discuss\n\n\nSophie Bishop meint es wäre interessant die Forschung zum Algorithmus von YouTube dahingehend zu erweitern, dass man untersucht was Nutzer denken, wie der Algorithmus funktioniert. Wie würde man bei dieser Fragestellung vorgehen bzw. wie könnte man das untersuchen?\n\n\n\n\nInwiefern kann YouTube als Soziales Netzwerk verstanden werden, wenn nur ein kleiner Bruchteil seiner User sich durch Upload von Content am Informationsaustausch beteiligt und der Großteil “nur” konsumiert?\n\n\n\n\nYouTube besteht als Social Media Platform länger als die meisten anderen Plattformen (Twitter, Instagram, TikTok, etc.) und besitzt so gut wie weltweite Bekanntheit. Warum ist die Forschung in Richtung YouTube trotzdem so gering?"
  },
  {
    "objectID": "content/05-api_access-youtube/slides/index.html#literatur",
    "href": "content/05-api_access-youtube/slides/index.html#literatur",
    "title": "API-Access – YouTube",
    "section": "Literatur",
    "text": "Literatur\n\n\nRieder, B. (2015). YouTube data tools. https://tools.digitalmethods.net/netvizz/youtube/index.php\n\n\n\n\n\nDigital Behavioral Data"
  },
  {
    "objectID": "content/06-api_access-reddit/06-exercise.html#background",
    "href": "content/06-api_access-reddit/06-exercise.html#background",
    "title": "Showcase",
    "section": "Background",
    "text": "Background\nThs showcase has two different goals:\n\nExemplary presentation of the collection of  Reddit data using the RedditExtractoR package\nImporting and analyzing the data collected with the 🐈🐈 4CAT 🐈🐈 tool.\n\nYou can also open this showcase in other interactive and executable environments:"
  },
  {
    "objectID": "content/06-api_access-reddit/06-exercise.html#data-collection-with-redditextractor",
    "href": "content/06-api_access-reddit/06-exercise.html#data-collection-with-redditextractor",
    "title": "Showcase",
    "section": "Data collection with RedditExtractor",
    "text": "Data collection with RedditExtractor\nReddit Extractor is an R package for extracting data out of Reddit. It allows you to:\n\nfind subreddits based on a search query\nfind a user and their Reddit history\nfind URLs to threads of interest and retrieve comments out of these threads\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nUnfortunately, the functions of the RedditExtractoR package can NOT be executed during the Markdown creation process and must be reproduced “live”.\nTherefore, there will be no output for the next two paragraphs.\n\n\n\nExample: Find subreddits\nSimilar to the example from the seminar, the function find_subreddits identifies all subreddits that contain the keyword news either in their name or in their attributes.\n\nlibrary(tidyverse)\nlibrary(RedditExtractoR)\n\n# Get list of subreddits\nnews <- find_subreddits(\"news\")\n\n# Quick preview of the dataset\nnews %>% glimpse()\n\n# Arrange subreddits by subscribers\nnews %>% \n  arrange(-subscribers) %>% \n  tibble() %>% head()\n\n\n\nExample: Find thread URLs\n\n# Get list of top thread urls\nnews_top_urls <- find_thread_urls(\n  subreddit = \"news\", \n  sort_by = \"top\", \n  period = \"month\"\n)\n\n# Quick preview of dataset\nnews_top_urls %>% glimpse()\nnews_top_urls %>% tibble()"
  },
  {
    "objectID": "content/06-api_access-reddit/06-exercise.html#analysis-of-collected-4cat-data",
    "href": "content/06-api_access-reddit/06-exercise.html#analysis-of-collected-4cat-data",
    "title": "Showcase",
    "section": "Analysis of collected 🐈🐈 4CAT 🐈🐈 data",
    "text": "Analysis of collected 🐈🐈 4CAT 🐈🐈 data\n\nData import from \n\n# load packages\nlibrary(readr)\n\n# get data from github\nmusk <- read_csv(\n  \"https://raw.githubusercontent.com/chrdrn/dbd_binder/main/data/06-reddit/4cat-reddit_news-musk_complete.csv\")\n\nmusk_entities <- read_csv(\n  \"https://raw.githubusercontent.com/chrdrn/dbd_binder/main/data/06-reddit/4cat-reddit_news-musk-named_entities.csv\")\n\n# quick preview\nmusk %>% glimpse()\n\nRows: 4,838\nColumns: 16\n$ thread_id      <chr> \"yugsz0\", \"yt59ku\", \"yulq2v\", \"yulq2v\", \"yulq2v\", \"yulq…\n$ id             <chr> \"iw9mhr7\", \"iw9tzrz\", \"iwa0egr\", \"iwa10h3\", \"iwa1gry\", …\n$ timestamp      <dttm> 2022-11-14 00:13:59, 2022-11-14 01:11:53, 2022-11-14 0…\n$ body           <chr> \"Nick Cannon and Elon Musk need to put a damn condom on…\n$ subject        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ author         <chr> \"fbe0d753750a9f008871e6e829b727bf26cc2bdcdc71f340\", \"89…\n$ author_flair   <chr> \"aadb59c4da75af6c9fb8d5cb4c310ce59888aab7f96ffc15\", \"aa…\n$ post_flair     <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ domain         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ url            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ image_file     <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ image_md5      <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ subreddit      <chr> \"worldnews\", \"news\", \"news\", \"news\", \"news\", \"news\", \"n…\n$ parent         <chr> \"t3_yugsz0\", \"t1_iw4y0aj\", \"t3_yulq2v\", \"t3_yulq2v\", \"t…\n$ score          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ unix_timestamp <dbl> 1668384839, 1668388313, 1668391383, 1668391676, 1668391…\n\nmusk_entities %>% glimpse()\n\nRows: 3,633\nColumns: 3\n$ word   <chr> \"musk\", \"twitter\", \"twitter\", \"elon musk\", \"tesla\", \"trump\", \"t…\n$ entity <chr> \"PERSON\", \"PERSON\", \"PRODUCT\", \"PRODUCT\", \"ORG\", \"ORG\", \"GPE\", …\n$ count  <dbl> 1147, 861, 479, 404, 345, 273, 222, 218, 205, 187, 154, 132, 13…\n\n\n\n\nMessages including ‘musk’ over time\n\n\n\n\n\n\nTip\n\n\n\nThe following graphics (and especially their labels) may appear very small. To view the graphics in their original size, right-click on the images and select “Open image/graphic in new tab”.\n\n\n\nTotal\n\nlibrary(lubridate)\nlibrary(sjPlot)\n\n# Display \nmusk %>% \n  mutate(date  = as.factor(date(timestamp))) %>% \n  plot_frq(\n    date,\n    title = \"Post including 'musk' on Reddit\") +\n  labs(subtitle = \"Subreddits 'news' & 'worldnews' between 14-11 and 26-11-2022\")\n\n\n\n\n\n\nBy subreddit\n\nlibrary(magrittr)\n\nmusk %>% \n  mutate(\n    date  = as.factor(date(timestamp)),\n    across(subreddit, as.factor)\n    ) %$% \n  plot_grpfrq(\n    date,\n    subreddit,\n    title = \"Post including 'musk' on Reddit\") +\n  labs(subtitle = \"Between 14-11 and 26-11-2022\")"
  },
  {
    "objectID": "content/06-api_access-reddit/06-slides.html",
    "href": "content/06-api_access-reddit/06-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you find the slides for the sixth session:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome."
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#seminarplan",
    "href": "content/06-api_access-reddit/slides/index.html#seminarplan",
    "title": "API-Access – Reddit",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\nSitzung\nDatum\nThema\nReferent*Innen\n\n\n\n\n1\n26.10.2022\nKick-Off Session\nChristoph Adrian\n\n\n2\n02.11.2022\nDBD: Einführung und Überblick\nChristoph Adrian\n\n\n3\n09.11.2022\nDBD: Datenerhebung\nChristoph Adrian\n\n\n4\n16.11.2022\nAPI-Access (I): Twitter\nFalk\n\n\n5\n23.11.2022\nAPI-Access (II): YouTube\nDenisov\n\n\n6\n30.11.2022\nAPI-Access (II): Reddit\nLandauer\n\n\n7\n07.12.2022\nWebscraping: TikTok\nBrand & Kocher\n\n\n8\n14.12.2022\nExkurs: DBD Analyse mit R\nChristoph Adrian\n\n\n\n\nWEIHNACHTSPAUSE\n\n\n\n9\n12.01.2023\nESM: m-path\nDörr\n\n\n10\n19.01.2023\nData Donations\nHofmann & Wierzbicki\n\n\n11\n26.01.2023\nPUFFER\n\n\n\n12\n02.02.2023\nGuest Lecture: Linking DBD & Survey data\nJohannes Breuer\n\n\n13\n09.02.2023\nSemesterabschluss & Evaluation\nChristoph Adrian\n\n\n\n\n▶️"
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#what-is-it-who-made-it",
    "href": "content/06-api_access-reddit/slides/index.html#what-is-it-who-made-it",
    "title": "API-Access – Reddit",
    "section": "What is it & who made it?",
    "text": "What is it & who made it?\nHintergrundinformationen 4CAT (Peeters & Hagen, 2022)\n\n\n\nTool zur Analyse und Verarbeitung von Daten aus sozialen Online-Plattformen\nZiel ist es, die Erfassung und Analyse von Daten aus diesen Plattformen über eine Webschnittstelle zugänglich zu machen, ohne dass Programmier- oder Web-Scraping-Kenntnisse erforderlich sind."
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#soziale-medien-sind-quelle-für-nachrichten",
    "href": "content/06-api_access-reddit/slides/index.html#soziale-medien-sind-quelle-für-nachrichten",
    "title": "API-Access – Reddit",
    "section": "Soziale Medien sind Quelle für Nachrichten",
    "text": "Soziale Medien sind Quelle für Nachrichten\nMedienkonsum in Wandel\n\n\n\n\nAber: Wie prominent ist Reddit (als Nachrichtenquelle) bei Ihnen/in DE?"
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#und-fa-brands-reddit-alien-noch-nicht-in-deutschland",
    "href": "content/06-api_access-reddit/slides/index.html#und-fa-brands-reddit-alien-noch-nicht-in-deutschland",
    "title": "API-Access – Reddit",
    "section": "Und ? (Noch) Nicht in Deutschland!",
    "text": "Und ? (Noch) Nicht in Deutschland!\nNutzung sozialer Medien als Nachrichtenquelle in Deutschland & USA"
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#and-now-you-reddit-als-newsfeed",
    "href": "content/06-api_access-reddit/slides/index.html#and-now-you-reddit-als-newsfeed",
    "title": "API-Access – Reddit",
    "section": "And now … you: Reddit als Newsfeed",
    "text": "And now … you: Reddit als Newsfeed\nGroup activity: Scrape Reddit mit 4CAT\n\n\n\nZiel der Group Activity\n\n\nDurchsuchen Sie die Subreddits r/news und r/worldnews nach Posts zu einem bestimmten Thema/Begriff und analysieren Sie diese mit Hilfe von 4CAT\n\n\n\n\n\nÜberlegen Sie (in Gruppen), welches Thema Sie untersuchen wollen und mit welchen Begriffen Sie Beiträge zu diesem Thema identifizieren könnten.\n\n\n\n\nÖffnen Sie 4CAT ➡️Create dataset ➡️ Data source: Reddit\n\n\n\n\nGeben Sie news, worldness an beim Feld Subreddit(s)\n\n\n\n\nGeben Sie Ihre(n) Suchbegriff(e) im Feld Message search ein.\n\n\n\n\nWählen Sie (maximal) die letzten zwei Wochen als Date range\n\n\n\n\nBenennen Sie ihrem Datensatz im Feld Dataset name\n\n\n\n\n\nOptional: Probieren Sie verschiedenen Analysemöglichkeiten aus"
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#tools-2-analysen",
    "href": "content/06-api_access-reddit/slides/index.html#tools-2-analysen",
    "title": "API-Access – Reddit",
    "section": "2 Tools, 2 Analysen",
    "text": "2 Tools, 2 Analysen\nBeispiele für Auswertung mit 🐈🐈4CAT🐈🐈 &  RStudio\nZwei (einfache) Szenarien:\n\nVisualisierung der Anzahl der Beiträge im Zeitverlauf\nErstellung einer Wordcloud mit den meistgenannten Named Entities"
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#beiträge-mit-keyword-musk-im-zeitverlauf",
    "href": "content/06-api_access-reddit/slides/index.html#beiträge-mit-keyword-musk-im-zeitverlauf",
    "title": "API-Access – Reddit",
    "section": "Beiträge mit Keyword “musk” im Zeitverlauf",
    "text": "Beiträge mit Keyword “musk” im Zeitverlauf\nVisualisierung mit 🐈🐈4CAT🐈🐈"
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#beiträge-im-zeitverlauf-nach-subreddit",
    "href": "content/06-api_access-reddit/slides/index.html#beiträge-im-zeitverlauf-nach-subreddit",
    "title": "API-Access – Reddit",
    "section": "Beiträge im Zeitverlauf nach Subreddit",
    "text": "Beiträge im Zeitverlauf nach Subreddit\nVisualisierung mit 🐈🐈4CAT🐈🐈"
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#wordcloud-mit-named-entities",
    "href": "content/06-api_access-reddit/slides/index.html#wordcloud-mit-named-entities",
    "title": "API-Access – Reddit",
    "section": "Wordcloud mit Named Entities",
    "text": "Wordcloud mit Named Entities\nVisualisierung mit 🐈🐈4CAT🐈🐈"
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#back-to-fa-brands-r-project-rstudio",
    "href": "content/06-api_access-reddit/slides/index.html#back-to-fa-brands-r-project-rstudio",
    "title": "API-Access – Reddit",
    "section": "Back to  RStudio",
    "text": "Back to  RStudio\nKurzer Überblick über die eingelesenen Datensätze\n\nmusk %>% glimpse()\n\nRows: 4,838\nColumns: 16\n$ thread_id      <chr> \"yugsz0\", \"yt59ku\", \"yulq2v\", \"yulq2v\", \"yulq2v\", \"yulq…\n$ id             <chr> \"iw9mhr7\", \"iw9tzrz\", \"iwa0egr\", \"iwa10h3\", \"iwa1gry\", …\n$ timestamp      <dttm> 2022-11-14 00:13:59, 2022-11-14 01:11:53, 2022-11-14 0…\n$ body           <chr> \"Nick Cannon and Elon Musk need to put a damn condom on…\n$ subject        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ author         <chr> \"fbe0d753750a9f008871e6e829b727bf26cc2bdcdc71f340\", \"89…\n$ author_flair   <chr> \"aadb59c4da75af6c9fb8d5cb4c310ce59888aab7f96ffc15\", \"aa…\n$ post_flair     <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ domain         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ url            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ image_file     <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ image_md5      <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ subreddit      <chr> \"worldnews\", \"news\", \"news\", \"news\", \"news\", \"news\", \"n…\n$ parent         <chr> \"t3_yugsz0\", \"t1_iw4y0aj\", \"t3_yulq2v\", \"t3_yulq2v\", \"t…\n$ score          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ unix_timestamp <dbl> 1668384839, 1668388313, 1668391383, 1668391676, 1668391…\n\nmusk_entities %>% glimpse()\n\nRows: 3,633\nColumns: 3\n$ word   <chr> \"musk\", \"twitter\", \"twitter\", \"elon musk\", \"tesla\", \"trump\", \"t…\n$ entity <chr> \"PERSON\", \"PERSON\", \"PRODUCT\", \"PRODUCT\", \"ORG\", \"ORG\", \"GPE\", …\n$ count  <dbl> 1147, 861, 479, 404, 345, 273, 222, 218, 205, 187, 154, 132, 13…"
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#beiträge-mit-keyword-musk-im-zeitverlauf-1",
    "href": "content/06-api_access-reddit/slides/index.html#beiträge-mit-keyword-musk-im-zeitverlauf-1",
    "title": "API-Access – Reddit",
    "section": "Beiträge mit Keyword “musk” im Zeitverlauf",
    "text": "Beiträge mit Keyword “musk” im Zeitverlauf\nBetrachtung von beiden Subreddits (news & worldnews )\n\nmusk %>% \n  mutate(date  = as.factor(date(timestamp))) %>%\n  plot_frq(date,\n           title = \"Post including 'musk' on Reddit\") +\n  labs(subtitle = \"Subreddits 'news' & 'worldnews' between 14-11 and 26-11-2022\")"
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#beiträge-mit-keyword-musk-im-zeitverlauf-2",
    "href": "content/06-api_access-reddit/slides/index.html#beiträge-mit-keyword-musk-im-zeitverlauf-2",
    "title": "API-Access – Reddit",
    "section": "Beiträge mit Keyword “musk” im Zeitverlauf",
    "text": "Beiträge mit Keyword “musk” im Zeitverlauf\nUnterschieden nach Subreddits (news & worldnews )\n\n\n\n\nmusk %>%\n  mutate(date  = as.factor(date(timestamp)),\n         across(subreddit, as.factor)) %$% \n  plot_grpfrq(date, subreddit,\n              title = \"Post including 'musk' on Reddit\") +\n  labs(subtitle = \"Between 14-11 and 26-11-2022\")"
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#vielen-dank-für-ihre-fragen",
    "href": "content/06-api_access-reddit/slides/index.html#vielen-dank-für-ihre-fragen",
    "title": "API-Access – Reddit",
    "section": "Vielen Dank für Ihre Fragen!",
    "text": "Vielen Dank für Ihre Fragen!\n\n\nDie Reddit API erlaubt nur bis 500 Kommentare pro Post über ein automatisches Tool zu ‘extrahieren’. Manuell ist es jedoch möglich über diese Grenze hinaus zu gehen. Wie kann das sein? Das würde ja bedeuten, dass man die Regeln der Reddit API manuell umgehen kann?\n\n\n\n\nIn dem Paper wird darauf hingewiesen, dass die Verwendung der API von Reddit ein möglicher Verstoß gegen die Nutzungsvereinbarungen ist. Wann würde so ein Fall eintreten bzw. wann spricht man von einem Verstoß und was wäre dann die Bestrafung?\n\n\nWie kann es sein, dass Reddit eine kostenlose API zur verfügung stellt, die Nutzung dieser dann aber teilweise nicht legal sein kann?\n\n\n\n\nEs wird erwähnt, dass viele Nutzer mehrere Accounts haben und dementsprechend Diskussionen in bestimmten Subreddits stärker beeinflussen können. Besteht die Möglichkeit solche Accounts zu erkennen und angemessen in der Studie zu berücksichtigen ohne die Datenschutzrichtlinien zu verletzen?"
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#lets-discuss",
    "href": "content/06-api_access-reddit/slides/index.html#lets-discuss",
    "title": "API-Access – Reddit",
    "section": "Let’s discuss",
    "text": "Let’s discuss\n\n\nViele Social Media Plattformen, wie Instagram, Facebook und Twitter beschränken ihren API Zugang. Weshalb legt Reddit den API Zugang im Gegensatz zu den anderen Plattformen so offen dar?\n\n\n\n\nViele der Reddit-Daten basierenden Publikationen enthielten nur unvollständige oder zweideutige Beschreibungen ihrer Datensätze. Auch die Methoden der Datenerhebung bleiben teilweise verschleiert. Warum ist das so und welche ethischen Gründe liegen hier möglicherweise zu Grunde?\n\n\n\n\nWie stark sind Reddit-Daten (evtl. auch im Vergleich zu Twitter) generalisierbar, wenn die Struktur von Reddit doch Diskussionen und Gespräche in gewisser Weise leitet (z.B. durch Sichtbarkeit von Themen)?"
  },
  {
    "objectID": "content/06-api_access-reddit/slides/index.html#literatur",
    "href": "content/06-api_access-reddit/slides/index.html#literatur",
    "title": "API-Access – Reddit",
    "section": "Literatur",
    "text": "Literatur\n\n\nPeeters, S., & Hagen, S. (2022). The 4CAT Capture and Analysis Toolkit: A Modular Tool for Transparent and Traceable Social Media Research. Computational Communication Research, 4(2), 571–589. https://doi.org/10.5117/ccr2022.2.007.hage\n\n\n\n\n\nDigital Behavioral Data"
  },
  {
    "objectID": "content/07-webscraping-tiktok/07-exercise.html#background",
    "href": "content/07-webscraping-tiktok/07-exercise.html#background",
    "title": "Showcase",
    "section": "Background",
    "text": "Background\nThis showcase is intended to illustrate different analysis possibilities of  TikTok data downloaded with the Zeeschuimer browser extension.\nYou can also open this showcase in other interactive and executable environments:"
  },
  {
    "objectID": "content/07-webscraping-tiktok/07-exercise.html#data-analysis",
    "href": "content/07-webscraping-tiktok/07-exercise.html#data-analysis",
    "title": "Showcase",
    "section": "Data analysis",
    "text": "Data analysis\n\nTikToks that are tagged with the hashtag statistics\ncollected via Zeeschuimer with .csv export via 🐈🐈 4CAT 🐈🐈\n\n\nData import from \n\n# load packages\nlibrary(readr)\n\nstatistics <- read_csv(\n  here(\"content/07-webscraping-tiktok/data/tiktok-search-statistics.csv\"), \n  col_types = cols(author_followers = col_number()))\n\n# quick preview\nstatistics %>% glimpse()\n\nRows: 941\nColumns: 24\n$ id               <dbl> 6.813800e+18, 6.832737e+18, 6.845368e+18, 6.927122e+1…\n$ thread_id        <dbl> 6.813800e+18, 6.832737e+18, 6.845368e+18, 6.927122e+1…\n$ author           <chr> \"onlyjayus\", \"onlyjayus\", \"ryguy238\", \"mason.whaley\",…\n$ author_full      <chr> \"actuallyitsbella\", \"actuallyitsbella\", \"ryan p\", \"Ma…\n$ author_id        <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ author_followers <dbl> 18500000, 18500000, 28500, 151500, 15700, 319500, 693…\n$ body             <chr> \"You’re sexy. The math proves it #fyp #love #dating #…\n$ timestamp        <dttm> 2020-04-09 19:44:39, 2020-05-30 20:28:05, 2020-07-03…\n$ unix_timestamp   <dbl> 1586461479, 1590870485, 1593811442, 1612846242, 16420…\n$ is_duet          <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ music_name       <chr> \"SexyBack\", \"original sound\", \"original sound\", \"orig…\n$ music_id         <dbl> 6.696418e+18, 6.832737e+18, 6.845368e+18, 6.927122e+1…\n$ music_url        <chr> \"https://sf16-ies-music-va.tiktokcdn.com/obj/tos-usea…\n$ video_url        <chr> \"https://v16-webapp.tiktok.com/eac3d9cb4566d4ac017e3f…\n$ tiktok_url       <chr> \"https://tiktok.com/@onlyjayus/video/6813800162259897…\n$ thumbnail_url    <chr> \"https://p16-sign-va.tiktokcdn.com/tos-maliva-p-0068/…\n$ likes            <dbl> 1200000, 910000, 901000, 794300, 740300, 701400, 6490…\n$ comments         <dbl> 7746, 11900, 3020, 36900, 8179, 8150, 34800, 7592, 28…\n$ shares           <dbl> 23000, 16600, 1755, 64000, 6397, 1685, 93800, 51300, …\n$ plays            <dbl> 6700000, 3300000, 5100000, 3800000, 2900000, 2500000,…\n$ hashtags         <chr> \"fyp,love,dating,romance,relationship,crush,people,po…\n$ stickers         <chr> NA, NA, NA, \"that one guy\", \"Ok…but I guess Timmy is …\n$ effects          <chr> NA, NA, NA, NA, \"Greenscreen\", NA, \"Disco\", NA, \"TapT…\n$ warning          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n\n\nExploration\n\n\n\n\n\n\nTip\n\n\n\nThe following graphics (and especially their labels) may appear very small. To view the graphics in their original size, right-click on the images and select “Open image/graphic in new tab”.\n\n\n\nPeriode in which the TikToks were posted\n\n# Load packages\nlibrary(lubridate)\nlibrary(sjPlot)\nlibrary(ggpubr)\n\n# Display \nstatistics %>% \n  mutate(date  = as.factor(year(timestamp))) %>% \n  plot_frq(date) +\n  theme_pubr()\n\n\n\n\n\n\nLocation parameters of different statistics\n\nstatistics %>% \n  select(likes:plays) %>% \n  descr()\n\n\n## Basic descriptive statistics\n\n      var    type    label   n NA.prc      mean        sd       se     md\n    likes numeric    likes 941      0  50412.33 110696.23  3608.59  16600\n comments numeric comments 941      0    980.51   2380.29    77.60    351\n   shares numeric   shares 941      0   1349.89   4755.66   155.03    262\n    plays numeric    plays 941      0 384388.52 750096.58 24452.45 153700\n   trimmed                   range    iqr  skew\n  26511.33  1395280 (4720-1400000)  37830  6.14\n    537.56         36900 (0-36900)    791  8.94\n    527.19         93796 (4-93800)    820 12.24\n 220367.46 7985100 (14900-8000000) 309300  5.44\n\n\n\n\nDistribution of likes\n\nstatistics %>% \n  plot_frq(likes, type = \"density\")\n\n\n\n\n\n\nWarning messages displayed\n\nlibrary(sjmisc)\n\nstatistics %>% \n  frq(warning)\n\nwarning <character> \n# total N=941 valid N=24 mean=1.88 sd=0.80\n\nValue                                                                                                    |   N | Raw % | Valid % | Cum. %\n-----------------------------------------------------------------------------------------------------------------------------------------\nLearn more about COVID-19 vaccines                                                                       |   9 |  0.96 |   37.50 |  37.50\nLearn the facts about COVID-19                                                                           |   9 |  0.96 |   37.50 |  75.00\nThe actions in this video are performed by professionals or supervised by professionals. Do not attempt. |   6 |  0.64 |   25.00 | 100.00\n<NA>                                                                                                     | 917 | 97.45 |    <NA> |   <NA>\n\n\n\n\n\nText analysis\n\nCorpus creation\n\n# Create corpus based on variable hashtags\ncrp <- corpus(\n  statistics, \n  docid_field = \"id\",\n  text_field = \"hashtags\")\n\n# Display\ncrp \n\nCorpus consisting of 941 documents and 22 docvars.\n6813800162259897344 :\n\"fyp,love,dating,romance,relationship,crush,people,population...\"\n\n6832736698938576896 :\n\"fyp,blacklivesmatter,tiktokpartner,learnontiktok,police,fact...\"\n\n6845368010048408576 :\n\"skittles,statistics,education,fyp,foryou\"\n\n6927121729257098240 :\n\"hotguy,itwasntme,turbotaxlivepick6,doritosflatlife,foryou,wa...\"\n\n7052705912317840384 :\n\"timotheechalamet,fyp,foryou,timothee,peach,callmebyyourname,...\"\n\n6909087873081905152 :\n\"stitch,statistics,staticstics,fyp,foryoupage,trending\"\n\n[ reached max_ndoc ... 935 more documents ]\n\n\n\n\nTokenization\n\n# Create tokens based on corpus\ntkn <- crp %>% \n  tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE,\n    remove_url = TRUE,\n    remove_separators = TRUE)\n\n# Display\ntkn\n\nTokens consisting of 941 documents and 22 docvars.\n6813800162259897344 :\n [1] \"fyp\"          \"love\"         \"dating\"       \"romance\"      \"relationship\"\n [6] \"crush\"        \"people\"       \"population\"   \"world\"        \"math\"        \n[11] \"stats\"        \"statistics\"  \n\n6832736698938576896 :\n[1] \"fyp\"              \"blacklivesmatter\" \"tiktokpartner\"    \"learnontiktok\"   \n[5] \"police\"           \"facts\"            \"fact\"             \"statistics\"      \n[9] \"usa\"             \n\n6845368010048408576 :\n[1] \"skittles\"   \"statistics\" \"education\"  \"fyp\"        \"foryou\"    \n\n6927121729257098240 :\n [1] \"hotguy\"            \"itwasntme\"         \"turbotaxlivepick6\"\n [4] \"doritosflatlife\"   \"foryou\"            \"wap\"              \n [7] \"statistics\"        \"fyp\"               \"foryoupage\"       \n[10] \"wap\"              \n\n7052705912317840384 :\n[1] \"timotheechalamet\" \"fyp\"              \"foryou\"           \"timothee\"        \n[5] \"peach\"            \"callmebyyourname\" \"statistics\"      \n\n6909087873081905152 :\n[1] \"stitch\"      \"statistics\"  \"staticstics\" \"fyp\"         \"foryoupage\" \n[6] \"trending\"   \n\n[ reached max_ndoc ... 935 more documents ]\n\n\n\n\nCreate Document-Feature-Matrix (DFM)\n\n# Create dfm based on tokens\ndfm <- tkn %>% \n  dfm()\n\n# Display\ndfm\n\nDocument-feature matrix of: 941 documents, 2,941 features (99.71% sparse) and 22 docvars.\n                     features\ndocs                  fyp love dating romance relationship crush people\n  6813800162259897344   1    1      1       1            1     1      1\n  6832736698938576896   1    0      0       0            0     0      0\n  6845368010048408576   1    0      0       0            0     0      0\n  6927121729257098240   1    0      0       0            0     0      0\n  7052705912317840384   1    0      0       0            0     0      0\n  6909087873081905152   1    0      0       0            0     0      0\n                     features\ndocs                  population world math\n  6813800162259897344          1     1    1\n  6832736698938576896          0     0    0\n  6845368010048408576          0     0    0\n  6927121729257098240          0     0    0\n  7052705912317840384          0     0    0\n  6909087873081905152          0     0    0\n[ reached max_ndoc ... 935 more documents, reached max_nfeat ... 2,931 more features ]\n\n\n\n\nWordcloud\n\ndfm %>% \n  textplot_wordcloud(\n    min_size = 1,\n    max_size = 8,\n    max_words = 50,\n    rotation = 0\n  )\n\n\n\n\n\nwithout the searchterm statistics\n\ndfm %>% \n  dfm_remove(pattern = \"statistics\") %>% \n  textplot_wordcloud(\n    min_size = 1,\n    max_size = 8,\n    max_words = 50,\n    rotation = 0,\n    color = \"dodgerblue3\"\n  )"
  },
  {
    "objectID": "content/07-webscraping-tiktok/07-slides.html",
    "href": "content/07-webscraping-tiktok/07-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you find the slides for the seventh session:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome."
  },
  {
    "objectID": "content/07-webscraping-tiktok/slides/index.html#seminarplan",
    "href": "content/07-webscraping-tiktok/slides/index.html#seminarplan",
    "title": "Web-Scraping – TikTok",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\nSitzung\nDatum\nThema\nReferent*Innen\n\n\n\n\n1\n26.10.2022\nKick-Off Session\nChristoph Adrian\n\n\n2\n02.11.2022\nDBD: Einführung und Überblick\nChristoph Adrian\n\n\n3\n09.11.2022\nDBD: Datenerhebung\nChristoph Adrian\n\n\n4\n16.11.2022\nAPI-Access (I): Twitter\nFalk\n\n\n5\n23.11.2022\nAPI-Access (II): YouTube\nDenisov\n\n\n6\n30.11.2022\nAPI-Access (II): Reddit\nLandauer\n\n\n7\n07.12.2022\nWebscraping: TikTok\nBrand & Kocher\n\n\n8\n14.12.2022\nExkurs: DBD Analyse mit R\nChristoph Adrian\n\n\n\n\nWEIHNACHTSPAUSE\n\n\n\n9\n11.01.2023\nESM: m-path\nDörr\n\n\n10\n18.01.2023\nData Donations\nHofmann & Wierzbicki\n\n\n11\n25.01.2023\nPUFFER\n\n\n\n12\n02.02.2023\nGuest Lecture: Linking DBD & Survey data\nJohannes Breuer\n\n\n13\n08.02.2023\nSemesterabschluss & Evaluation\nChristoph Adrian\n\n\n\n\n▶️"
  },
  {
    "objectID": "content/07-webscraping-tiktok/slides/index.html#vielen-dank-für-ihre-fragen",
    "href": "content/07-webscraping-tiktok/slides/index.html#vielen-dank-für-ihre-fragen",
    "title": "Web-Scraping – TikTok",
    "section": "Vielen Dank für Ihre Fragen!",
    "text": "Vielen Dank für Ihre Fragen!\n\n\nIn dem Paper wird darüber berichtet, dass der einzige direkte Weg der Monetarisierung auf TikTok (und auf Douyin) die virtuelle Währung bzw. die virtuellen Geschenke sind. In China ist diese Art des Geschäftsmodell bereits eine sehr etablierte Form der Zahlung, bei uns jedoch noch nicht. Ist es denkbar, dass dieser Trend auch bei uns populär wird?\n\n\n\n\nIn dem Paper wird berichtet, dass TikTok teilweise transparenter als andere nicht-asiatischen Plattformen ist. Was genau tut TikTok dafür um diese transparenz zu schaffen?\n\n\n\n\nIn Indien ist TikTok bereits verboten. Auch die USA und Australien drohen mit Maßnahmen gegen TikTok. Was hätte dies für ByteDance zur Folge?\n\n\n\n\nIst die Walkthrough-Methode eine gängige Methode der Datenerhebung auf TikTok und wie läuft diese genau ab? Gibt es andere Methoden, die vielleicht weniger aufwändig sind?"
  },
  {
    "objectID": "content/07-webscraping-tiktok/slides/index.html#lets-discuss",
    "href": "content/07-webscraping-tiktok/slides/index.html#lets-discuss",
    "title": "Web-Scraping – TikTok",
    "section": "Let’s discuss",
    "text": "Let’s discuss\n\n\nDurch die Ideologie (platform governance) einer Plattform kann es kritisch sein, wenn einzelne Plattformen besonders groß werden. Gleichzeitig ist die Existenz von mehreren kleineren Plattformen ebenfalls kritisch, da sich Nischengruppen bilden würden, die nur eine einzelne Perspektive betrachten. (Vgl. CCP Propaganda )\nWas könnte eine Lösung für dieses Problem sein?\n\n\n\n\nLaut dem paper wird durch verschiedene video einstellungen von TikTok bereits content “geshaped”. Sind biases auf TikTok dadurch stärker als bei anderen Plattformen?"
  },
  {
    "objectID": "content/07-webscraping-tiktok/slides/index.html#what-is-it-who-made-it",
    "href": "content/07-webscraping-tiktok/slides/index.html#what-is-it-who-made-it",
    "title": "Web-Scraping – TikTok",
    "section": "What is it & who made it?",
    "text": "What is it & who made it?\nHintergrundinformationen Zeeschuimer (Peeters, 2022)\n\n\n\nBrowsererweiterung, die während des Besuchs einer Social-Media-Website Daten über die Elemente sammelt, die in der Weboberfläche einer Plattform zu sehen sind\nDerzeit werden die folgenden Plattformen unterstützt:\n\n über https://www.tiktok.com\n über https://www.instagram.com\n\nErgänzung zu 4CAT (Peeters & Hagen, 2022)\n\n\n\n\n\n\n\n\n\n\n\nDie Zielgruppe sind Forscher, die systematisch Inhalte auf Social-Media-Plattformen untersuchen wollen, die sich dem herkömmlichen Scraping oder der API-basierten Datenerfassung widersetzen.\n\nSie können z. B. TikTok durchsuchen und später eine Liste aller Beiträge in der Reihenfolge exportieren, in der Sie sie gesehen haben. Die Daten können als JSON-Datei exportiert oder zur Analyse und Speicherung in eine 4CAT-Instanz exportiert werden. Zeeschuimer ist in erster Linie als Ergänzung zu 4CAT gedacht, aber Sie können seine Ausgabe auch in Ihre eigene Analysepipeline integrieren.\nDie Plattformunterstützung erfordert regelmäßige Wartung, um mit den Änderungen auf den Plattformen Schritt zu halten. Wenn etwas nicht funktioniert, freuen wir uns über Probleme und Pull Request\nDie Erweiterung stört Sie nicht beim normalen Surfen und lädt niemals automatisch Daten hoch, sondern nur, wenn Sie sie ausdrücklich dazu auffordern."
  },
  {
    "objectID": "content/07-webscraping-tiktok/slides/index.html#and-now-you-design-your-own-fa-brands-tiktok-research",
    "href": "content/07-webscraping-tiktok/slides/index.html#and-now-you-design-your-own-fa-brands-tiktok-research",
    "title": "Web-Scraping – TikTok",
    "section": "And now … you: Design your own  research",
    "text": "And now … you: Design your own  research\nGroup activity: Fragestellung ➡️ Datenerhebung ➡️Vergleich\n\n\n\nZiel der Group Activity\n\n\n\nFühren Sie in Zweiergruppen eine kleine Case Study durch\nÜberlegen Sie sich eine Fragestellung, für die Sie mit Hilfe von Zeeschuimer-Daten erheben und (potentiell) auswerten können\nPräsentieren Sie kurz Ihr Projekt mit Hilfe von maximal 2 Powerpoint-Slides\n\n\n\n\n\n\nÜberlegen Sie (in Gruppen), welches Thema Sie untersuchen wollen, d.h.\n\nwelchen Begriffen bzw. Hashtags Sie Beiträge zu diesem Thema identifizieren könnten\noder von welchem Account Sie sich die Beiträge anschauen wollen\n\n\n\n\n\nNutzen Sie die Zeeschuimer-Browsererweiterung und laden Sie (jede Person in der Gruppe) die für Ihre case study relevanten Daten herunter.\n\n\n\n\nErkunden Sie Ihren Datensatz und überlegen Sie sich potentielle Auswertungsstrategien.\n\n\n\n\nPosten Sie eine maximal 2 Folien umfassende Powerpoint-Präsentation auf MS Teams, mit\n\nIhrer Fragestellung\neiner kurzen Beschreibung Ihrer Erhebung\ndie nächsten (Analyse-)Schritte\n\n\n\n\nOptional: Vergleichen Sie die von Ihnen heruntergeladenen Datensätze.\n\nSind die gleichen Beiträge enthalten?\nWie hoch ist die Übereinstimmung (ungefähr)?"
  },
  {
    "objectID": "content/07-webscraping-tiktok/slides/index.html#ein-kurzer-überblick",
    "href": "content/07-webscraping-tiktok/slides/index.html#ein-kurzer-überblick",
    "title": "Web-Scraping – TikTok",
    "section": "Ein kurzer Überblick",
    "text": "Ein kurzer Überblick\nStruktur des  TikTok-Datensatzes\n\nstatistics %>% glimpse()\n\nRows: 941\nColumns: 24\n$ id               <dbl> 6.813800e+18, 6.832737e+18, 6.845368e+18, 6.927122e+1…\n$ thread_id        <dbl> 6.813800e+18, 6.832737e+18, 6.845368e+18, 6.927122e+1…\n$ author           <chr> \"onlyjayus\", \"onlyjayus\", \"ryguy238\", \"mason.whaley\",…\n$ author_full      <chr> \"actuallyitsbella\", \"actuallyitsbella\", \"ryan p\", \"Ma…\n$ author_id        <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ author_followers <dbl> 18500000, 18500000, 28500, 151500, 15700, 319500, 693…\n$ body             <chr> \"You’re sexy. The math proves it #fyp #love #dating #…\n$ timestamp        <dttm> 2020-04-09 19:44:39, 2020-05-30 20:28:05, 2020-07-03…\n$ unix_timestamp   <dbl> 1586461479, 1590870485, 1593811442, 1612846242, 16420…\n$ is_duet          <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ music_name       <chr> \"SexyBack\", \"original sound\", \"original sound\", \"orig…\n$ music_id         <dbl> 6.696418e+18, 6.832737e+18, 6.845368e+18, 6.927122e+1…\n$ music_url        <chr> \"https://sf16-ies-music-va.tiktokcdn.com/obj/tos-usea…\n$ video_url        <chr> \"https://v16-webapp.tiktok.com/eac3d9cb4566d4ac017e3f…\n$ tiktok_url       <chr> \"https://tiktok.com/@onlyjayus/video/6813800162259897…\n$ thumbnail_url    <chr> \"https://p16-sign-va.tiktokcdn.com/tos-maliva-p-0068/…\n$ likes            <dbl> 1200000, 910000, 901000, 794300, 740300, 701400, 6490…\n$ comments         <dbl> 7746, 11900, 3020, 36900, 8179, 8150, 34800, 7592, 28…\n$ shares           <dbl> 23000, 16600, 1755, 64000, 6397, 1685, 93800, 51300, …\n$ plays            <dbl> 6700000, 3300000, 5100000, 3800000, 2900000, 2500000,…\n$ hashtags         <chr> \"fyp,love,dating,romance,relationship,crush,people,po…\n$ stickers         <chr> NA, NA, NA, \"that one guy\", \"Ok…but I guess Timmy is …\n$ effects          <chr> NA, NA, NA, NA, \"Greenscreen\", NA, \"Disco\", NA, \"TapT…\n$ warning          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…"
  },
  {
    "objectID": "content/07-webscraping-tiktok/slides/index.html#erstellung-eines-korpus",
    "href": "content/07-webscraping-tiktok/slides/index.html#erstellung-eines-korpus",
    "title": "Web-Scraping – TikTok",
    "section": "Erstellung eines Korpus",
    "text": "Erstellung eines Korpus\nTextverarbeitung mit quanteda (Benoit et al., 2018)\n\ncrp <- corpus(statistics, \n              docid_field = \"id\",\n              text_field = \"hashtags\")\ncrp\n\nCorpus consisting of 941 documents and 22 docvars.\n6813800162259897344 :\n\"fyp,love,dating,romance,relationship,crush,people,population...\"\n\n6832736698938576896 :\n\"fyp,blacklivesmatter,tiktokpartner,learnontiktok,police,fact...\"\n\n6845368010048408576 :\n\"skittles,statistics,education,fyp,foryou\"\n\n6927121729257098240 :\n\"hotguy,itwasntme,turbotaxlivepick6,doritosflatlife,foryou,wa...\"\n\n7052705912317840384 :\n\"timotheechalamet,fyp,foryou,timothee,peach,callmebyyourname,...\"\n\n6909087873081905152 :\n\"stitch,statistics,staticstics,fyp,foryoupage,trending\"\n\n[ reached max_ndoc ... 935 more documents ]"
  },
  {
    "objectID": "content/07-webscraping-tiktok/slides/index.html#tokenisierung",
    "href": "content/07-webscraping-tiktok/slides/index.html#tokenisierung",
    "title": "Web-Scraping – TikTok",
    "section": "Tokenisierung",
    "text": "Tokenisierung\nTextverarbeitung mit quanteda (Benoit et al., 2018)\n\ntkn <- crp %>% \n  tokens(remove_punct = TRUE,\n         remove_symbols = TRUE,\n         remove_url = TRUE,\n         remove_separators = TRUE)\ntkn\n\nTokens consisting of 941 documents and 22 docvars.\n6813800162259897344 :\n [1] \"fyp\"          \"love\"         \"dating\"       \"romance\"      \"relationship\"\n [6] \"crush\"        \"people\"       \"population\"   \"world\"        \"math\"        \n[11] \"stats\"        \"statistics\"  \n\n6832736698938576896 :\n[1] \"fyp\"              \"blacklivesmatter\" \"tiktokpartner\"    \"learnontiktok\"   \n[5] \"police\"           \"facts\"            \"fact\"             \"statistics\"      \n[9] \"usa\"             \n\n6845368010048408576 :\n[1] \"skittles\"   \"statistics\" \"education\"  \"fyp\"        \"foryou\"    \n\n6927121729257098240 :\n [1] \"hotguy\"            \"itwasntme\"         \"turbotaxlivepick6\"\n [4] \"doritosflatlife\"   \"foryou\"            \"wap\"              \n [7] \"statistics\"        \"fyp\"               \"foryoupage\"       \n[10] \"wap\"              \n\n7052705912317840384 :\n[1] \"timotheechalamet\" \"fyp\"              \"foryou\"           \"timothee\"        \n[5] \"peach\"            \"callmebyyourname\" \"statistics\"      \n\n6909087873081905152 :\n[1] \"stitch\"      \"statistics\"  \"staticstics\" \"fyp\"         \"foryoupage\" \n[6] \"trending\"   \n\n[ reached max_ndoc ... 935 more documents ]"
  },
  {
    "objectID": "content/07-webscraping-tiktok/slides/index.html#erstellung-einer-document-feature-matrix-dfm",
    "href": "content/07-webscraping-tiktok/slides/index.html#erstellung-einer-document-feature-matrix-dfm",
    "title": "Web-Scraping – TikTok",
    "section": "Erstellung einer Document-Feature-Matrix [DFM]",
    "text": "Erstellung einer Document-Feature-Matrix [DFM]\nTextverarbeitung mit quanteda (Benoit et al., 2018)\n\ndfm <- tkn %>%  dfm()\ndfm\n\nDocument-feature matrix of: 941 documents, 2,941 features (99.71% sparse) and 22 docvars.\n                     features\ndocs                  fyp love dating romance relationship crush people\n  6813800162259897344   1    1      1       1            1     1      1\n  6832736698938576896   1    0      0       0            0     0      0\n  6845368010048408576   1    0      0       0            0     0      0\n  6927121729257098240   1    0      0       0            0     0      0\n  7052705912317840384   1    0      0       0            0     0      0\n  6909087873081905152   1    0      0       0            0     0      0\n                     features\ndocs                  population world math\n  6813800162259897344          1     1    1\n  6832736698938576896          0     0    0\n  6845368010048408576          0     0    0\n  6927121729257098240          0     0    0\n  7052705912317840384          0     0    0\n  6909087873081905152          0     0    0\n[ reached max_ndoc ... 935 more documents, reached max_nfeat ... 2,931 more features ]"
  },
  {
    "objectID": "content/07-webscraping-tiktok/slides/index.html#welche-hashtags-werden-genutzt",
    "href": "content/07-webscraping-tiktok/slides/index.html#welche-hashtags-werden-genutzt",
    "title": "Web-Scraping – TikTok",
    "section": "Welche Hashtags werden genutzt?",
    "text": "Welche Hashtags werden genutzt?\nTextvisualisierung mit quanteda.textplots\n\nlibrary(quanteda.textplots)\ndfm %>% textplot_wordcloud(\n  min_size = 1, max_size = 8, max_words = 50,\n  rotation = 0)"
  },
  {
    "objectID": "content/07-webscraping-tiktok/slides/index.html#literatur",
    "href": "content/07-webscraping-tiktok/slides/index.html#literatur",
    "title": "Web-Scraping – TikTok",
    "section": "Literatur",
    "text": "Literatur\n\n\nBenoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., Müller, S., & Matsuo, A. (2018). Quanteda: An r package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774. https://doi.org/10.21105/joss.00774\n\n\nPeeters, S. (2022). Zeeschuimer. Zenodo. https://doi.org/10.5281/ZENODO.7016797\n\n\nPeeters, S., & Hagen, S. (2022). The 4CAT Capture and Analysis Toolkit: A Modular Tool for Transparent and Traceable Social Media Research. Computational Communication Research, 4(2), 571–589. https://doi.org/10.5117/ccr2022.2.007.hage\n\n\n\n\n\nDigital Behavioral Data"
  },
  {
    "objectID": "content/08-text_as_data/08-exercise.html#background",
    "href": "content/08-text_as_data/08-exercise.html#background",
    "title": "Showcase",
    "section": "Background",
    "text": "Background\n\nScraping Amazon Reviewss in R"
  },
  {
    "objectID": "content/08-text_as_data/08-exercise.html#scraping",
    "href": "content/08-text_as_data/08-exercise.html#scraping",
    "title": "Showcase",
    "section": "Scraping",
    "text": "Scraping\n\nCreate function\nbased on stackoverflow post.\n\nscrape_amazon <- function(page_num, review_url) {\n  url_reviews <- paste0(review_url, \"&pageNumber=\", page_num, \"&sortBy=recent\")\n  doc <- read_html(url_reviews)\n  map_dfr(doc %>% html_elements(\"[id^='customer_review']\"), ~ data.frame(\n    review_title = .x %>% html_element(\".review-title\") %>% html_text2(),\n    review_text = .x %>% html_element(\".review-text-content\") %>% html_text2(),\n    review_star = .x %>% html_element(\".review-rating\") %>% html_text2(),\n    date = .x %>% html_element(\".review-date\") %>% html_text2() %>% gsub(\".*vom \", \"\", .),\n    author = .x %>% html_element(\".a-profile-name\") %>% html_text2(),\n    page = page_num\n  )) %>%\n    as_tibble %>%\n    return()\n}\n\n\n\nDefine urls\n\nurl <- list(\n  p01 = \"https://www.amazon.de/LINEAVI-Eiwei%C3%9F-Shake-Kombination-Molkeneiwei%C3%9F-laktosefrei/product-reviews/B018IB02AU/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n  p02 = \"https://www.amazon.de/Detoxkuren%E2%80%A2-Entw%C3%A4sserung-Entschlackung-Stoffwechsel-entschlacken/product-reviews/B072QW5ZN1/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n  p03 = \"https://www.amazon.de/Saint-Nutrition%C2%AE-KETO-BURN-Appetitz%C3%BCgler/product-reviews/B08B67V8G5/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n  p04 = \"https://www.amazon.de/Yokebe-vegetarisch-Mahlzeitersatz-Gewichtsabnahme-hochwertigen/product-reviews/B08GYZ8LRB/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n  p05 = \"https://www.amazon.de/Vihado-Liquid-chlorophyll-drops-alfalfa/product-reviews/B093XNC8QH/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews\"\n)\n\n\np01 (Lineavi): 1.679 Gesamtbewertungen, 782 mit Rezensionen –> 79 pages\np02 (DietySlim): 1.652 Gesamtbewertungen, 268 mit Rezensionen –> 28 pages\np03 (Keto Burn): 3.341 Gesamtbewertungen, 540 mit Rezensionen –> 55 pages\np04 (Yokebe): 1.586 Gesamtbewertungen, 156 mit Rezensionen –> 16 pages\np05 (Vihado): 1.335 Gesamtbewertungen, 396 mit Rezensionen –> 40 pages\n\n\n\nScrape data\n\namazon <- list()\n# p01 \nfor (i in 1:79) {\n  df <- scrape_amazon(page_num = i, review_url = url$p01)\n  amazon$raw$p01[[i]] <- df\n}\n# p02\nfor (i in 1:28) {\n  df <- scrape_amazon(page_num = i, review_url = url$p02)\n  amazon$raw$p02[[i]] <- df\n}\n# p03\nfor (i in 1:55) {\n  df <- scrape_amazon(page_num = i, review_url = url$p03)\n  amazon$raw$p03[[i]] <- df\n}\n# p04\nfor (i in 1:16) {\n  df <- scrape_amazon(page_num = i, review_url = url$p04)\n  amazon$raw$p04[[i]] <- df\n}\n# p05\nfor (i in 1:40) {\n  df <- scrape_amazon(page_num = i, review_url = url$p05)\n  amazon$raw$p05[[i]] <- df\n}\n\n\n\nBind rows\n\nproduct <- names(url)\n# bind rows for each product\nfor (i in product) {\n  amazon$data$raw[[i]] <- amazon$raw[[i]] %>% \n    bind_rows() %>% \n    rownames_to_column(\"id\") %>% \n    mutate(across(id, as.numeric))\n}\n# bind rows of all products\namazon$data$full <- amazon$data$raw %>% \n  bind_rows(.id = \"src\")\n\n\n\nSave data\n\nsaveRDS(\n  amazon,\n  file = here(\"data/08-text_as_data/reviews_only.RDS\"))"
  },
  {
    "objectID": "content/08-text_as_data/08-exercise.html#data-processing",
    "href": "content/08-text_as_data/08-exercise.html#data-processing",
    "title": "Showcase",
    "section": "Data processing",
    "text": "Data processing\n\namazon$data$clean <- amazon$data$full %>% \n  # create doc_id\n  rownames_to_column(., var = \"doc_id\") %>% \n  mutate(\n    # create unique ids (src_id, doc_id)\n    src_id = id, \n    id = paste(src, sprintf(\"%03d\", id), sep = \"_\"),\n    # convert to factor\n    across(c(id, src), as.factor),\n    across(doc_id, as.numeric),\n        # review body\n    review_body = paste(review_title, review_text),\n    body_trimmed = str_replace_all(review_body, \"[\\r\\n]\" , \"\"), # delete line breaks\n    lang_detect = fastText::language_identification(\n      body_trimmed,\n      system.file(\"language_identification/lid.176.ftz\", package = \"fastText\")),\n    lang = lang_detect$iso_lang_1,\n    lang_prob = lang_detect$prob_1,\n    # edit date variable\n    date_raw = date,\n    date_base = str_extract(date_raw, \"\\\\d{1,2}(.*)\\\\d{1,4}\"),\n    ## Change date format to DD.MM.YYYY\n    across(date_base, str_replace, \" Januar \", \"01.\"),\n    across(date_base, str_replace, \" Februar \", \"02.\"),\n    across(date_base, str_replace, \" März \", \"03.\"),\n    across(date_base, str_replace, \" April \", \"04.\"),\n    across(date_base, str_replace, \" Mai \", \"05.\"),\n    across(date_base, str_replace, \" Juni \", \"06.\"),\n    across(date_base, str_replace, \" Juli \", \"07.\"),\n    across(date_base, str_replace, \" August \", \"08.\"),\n    across(date_base, str_replace, \" September \", \"09.\"),\n    across(date_base, str_replace, \" Oktober \", \"10.\"),\n    across(date_base, str_replace, \" November \", \"11.\"),\n    across(date_base, str_replace, \" Dezember \", \"12.\"),\n    ## Convert to date\n    date = as.Date(date_base, format = \"%d.%m.%Y\"),\n    ## create date variables\n    year = as.factor(year(date)),\n    month = as.factor(month(date)),\n    day = as.factor(day(date)),\n    rating  = as.numeric(str_extract(review_star, \"\\\\d{1}(?=,)\")) \n    ) %>% \n  # relcoate variables\n  relocate(starts_with(\"src\"), .after = id) %>%\n  relocate(starts_with(\"date\"), .after = src_id) %>%\n  relocate(year,month,day, .before = review_title) %>%\n  relocate(starts_with(\"lang\"), .after = page) %>% \n  relocate(starts_with(\"review\"), .after = lang_prob) %>%\n  relocate(review_star, .after = body_trimmed)\n\n\nCheck language identification\n\nLanguages\n\namazon$data$clean %>% \n  frq(lang)\n\nlang <character> \n# total N=2142 valid N=2142 mean=2.67 sd=1.52\n\nValue |    N | Raw % | Valid % | Cum. %\n---------------------------------------\nals   |    1 |  0.05 |    0.05 |   0.05\nde    | 1736 | 81.05 |   81.05 |  81.09\nen    |   57 |  2.66 |    2.66 |  83.75\nes    |   39 |  1.82 |    1.82 |  85.57\nfr    |    1 |  0.05 |    0.05 |  85.62\nit    |  292 | 13.63 |   13.63 |  99.25\nnl    |    5 |  0.23 |    0.23 |  99.49\npl    |    1 |  0.05 |    0.05 |  99.53\npt    |    2 |  0.09 |    0.09 |  99.63\nsk    |    2 |  0.09 |    0.09 |  99.72\nsr    |    3 |  0.14 |    0.14 |  99.86\nsv    |    1 |  0.05 |    0.05 |  99.91\ntr    |    2 |  0.09 |    0.09 | 100.00\n<NA>  |    0 |  0.00 |    <NA> |   <NA>\n\n\n\n\nIdentification probability\n\namazon$data$clean %>% \n  group_by(lang) %>% \n  summarise(\n    n = n(), \n    prob = mean(lang_prob)\n  )\n\n# A tibble: 13 × 3\n   lang      n  prob\n   <chr> <int> <dbl>\n 1 als       1 0.861\n 2 de     1736 0.979\n 3 en       57 0.565\n 4 es       39 0.900\n 5 fr        1 0.930\n 6 it      292 0.963\n 7 nl        5 0.566\n 8 pl        1 0.999\n 9 pt        2 0.687\n10 sk        2 0.694\n11 sr        3 0.510\n12 sv        1 0.482\n13 tr        2 0.992\n\n\n\n\n\nSelect only german reviews\n\namazon$data$de <- amazon$data$clean %>% \n  filter(lang == \"de\")"
  },
  {
    "objectID": "content/08-text_as_data/08-exercise.html#exploratory-data-analysis",
    "href": "content/08-text_as_data/08-exercise.html#exploratory-data-analysis",
    "title": "Showcase",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nNumber of reviews by product\n\namazon$data$de %>% \n  frq(src)\n\nsrc <categorical> \n# total N=1736 valid N=1736 mean=2.88 sd=1.46\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\np01   | 438 | 25.23 |   25.23 |  25.23\np02   | 256 | 14.75 |   14.75 |  39.98\np03   | 509 | 29.32 |   29.32 |  69.30\np04   | 143 |  8.24 |    8.24 |  77.53\np05   | 390 | 22.47 |   22.47 | 100.00\n<NA>  |   0 |  0.00 |    <NA> |   <NA>\n\n\n\n\nReviews by year\n\namazon$data$de %>% \n  ggplot(aes(year, fill = src)) +\n  geom_bar() +\n  scale_fill_locuszoom() +\n  theme_pubr()\n\n\n\n\n\n\nRatings by product\n\namazon$data$de %>% \n  mutate(across(rating, as.factor)) %>% \n  ggplot(aes(src, fill = rating)) + \n  geom_bar() +\n  scale_fill_brewer(palette = \"RdYlGn\") +\n  theme_pubr()\namazon$data$de %>% \n  mutate(across(rating, as.factor)) %>% \n  ggplot(aes(src, fill = rating)) + \n  geom_bar(position = \"fill\") +\n  scale_fill_brewer(palette = \"RdYlGn\") +\n  theme_pubr()\n\n\n\n\n\n\nAbsolute\n\n\n\n\n\n\n\nRelative\n\n\n\n\n\n\nRating by product"
  },
  {
    "objectID": "content/08-text_as_data/08-exercise.html#sentiment-analysis",
    "href": "content/08-text_as_data/08-exercise.html#sentiment-analysis",
    "title": "Showcase",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\n\nCreate data (temporary corpora)\n\n\n\n\nLocation parameters\n\namazon$temp$crps_stats %>% \n  select(types, tokens, sentences) %>%\n  descr()\n\n\n## Basic descriptive statistics\n\n       var    type     label    n NA.prc  mean    sd   se md trimmed\n     types integer     types 1736      0 41.92 41.83 1.00 31   34.91\n    tokens integer    tokens 1736      0 56.16 74.06 1.78 36   42.78\n sentences integer sentences 1736      0  3.56  3.39 0.08  3    2.93\n         range iqr skew\n   459 (1-460)  37 3.38\n 1128 (2-1130)  48 5.56\n     37 (1-38)   4 3.39\n\n\n\n\nLength of reviews by source\n\n# add sample size\nsample_size <- amazon$temp$crps_stats %>% \n  group_by(src) %>% \n  summarise(n = n())\namazon$temp$crps_stats %>% \n  # Add sample size as label\n  left_join(sample_size) %>% \n  mutate(src_label = paste0(src, \"\\n\", \"(n = \", n, \")\")) %>% \n  ggboxplot(\"src_label\", \"tokens\", \n            color = \"src\", palette = \"startrek\",\n            orientation = \"horizontal\") +\n  labs(x = \"\", y = \"Number of tokens\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nJoin datasets\n\n\n\n\n\nPolarity by product\n\namazon$data$de_senti %>% \n  ggplot(aes(x = src, y = polarity)) + \n  ggdist::stat_halfeye(\n    aes(fill = src),\n    adjust = .5, \n    width = .6, \n    .width = 0, \n    justification = -.3, \n    point_colour = NA) + \n  geom_boxplot(\n    aes(color = src),\n    width = .25, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    aes(color = src),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter(\n      seed = 1, width = .1\n    )\n  ) + \n  coord_cartesian(xlim = c(1.2, NA), clip = \"off\") +\n  scale_color_startrek() +\n  scale_fill_startrek() +\n  theme_pubr()\n\n\n\n\n\n\nValence by product\n\namazon$data$de_senti %>% \n  ggplot(aes(x = src, y = valence)) + \n  ggdist::stat_halfeye(\n    aes(fill = src),\n    adjust = .5, \n    width = .6, \n    .width = 0, \n    justification = -.3, \n    point_colour = NA) + \n  geom_boxplot(\n    aes(color = src),\n    width = .25, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    aes(color = src),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter(\n      seed = 1, width = .1\n    )\n  ) + \n  coord_cartesian(xlim = c(1.2, NA), clip = \"off\") +\n  scale_color_startrek() +\n  scale_fill_startrek() +\n  theme_pubr()\n\n\n\n\n\n\nRatings by categorical valence\n\namazon$data$de_senti %>% \n  flat_table(rating, val_fct)\n\n       val_fct negativ neutral positiv\nrating                                \n1                  167      40     101\n2                   61      11      37\n3                   47      14      48\n4                   34      10     129\n5                  109      16     912"
  },
  {
    "objectID": "content/08-text_as_data/08-exercise.html#topic-modeling",
    "href": "content/08-text_as_data/08-exercise.html#topic-modeling",
    "title": "Showcase",
    "section": "Topic Modeling",
    "text": "Topic Modeling\n\nPreparation\n\nText Processing\n\n\n\n\n\nGet top features of dfm\n\ntextstat_frequency(\n  amazon$txt$dfm, n = 50)\n\n       feature frequency rank docfreq group\n1          gut       702    1     511   all\n2      produkt       609    2     476   all\n3    geschmack       516    3     419   all\n4     schmeckt       414    4     340   all\n5      tropfen       359    5     199   all\n6        schon       335    6     280   all\n7         dass       328    7     233   all\n8        super       296    8     249   all\n9         mehr       283    9     223   all\n10      wasser       261   10     210   all\n11         mal       257   11     199   all\n12     wirkung       243   12     195   all\n13      leider       232   13     187   all\n14         tag       228   14     174   all\n15       nehme       226   15     209   all\n16       shake       225   16     131   all\n17      wochen       219   17     179   all\n18        seit       210   18     190   all\n19    wirklich       208   19     169   all\n20     lineavi       207   20     127   all\n21     einfach       192   21     163   all\n22       sagen       186   22     171   all\n23   empfehlen       181   23     162   all\n24      besser       177   24     157   all\n25   zufrieden       170   25     155   all\n26       tagen       168   26     144   all\n27       immer       165   27     145   all\n28     kapseln       154   28     116   all\n29    abnehmen       151   29     132   all\n30  abgenommen       147   30     136   all\n31          kg       146   31     109   all\n32       woche       146   31     121   all\n33      shaker       142   33     104   all\n34 chlorophyll       141   34      99   all\n35        ganz       136   35     122   all\n36       fühle       136   35     127   all\n37       finde       133   37     113   all\n38       sport       133   37     116   all\n39     schnell       133   37     121   all\n40      leicht       126   40     116   all\n41       hilft       123   41     106   all\n42       macht       122   42     108   all\n43        satt       119   43      98   all\n44   ernährung       119   43     108   all\n45        kilo       117   45      89   all\n46     weniger       117   45      95   all\n47      ersten       111   47      89   all\n48       gutes       111   47     102   all\n49    bestellt       107   49     102   all\n50        zeit       106   50      98   all\n\n\n\n\nConvert dfm to stm\n\namazon$tpm$dfm <- convert(amazon$txt$dfm, to = \"stm\")\n\n\n\n\nChoose topic number\n\nk = 0\n\ntic(\"choose_k_free\")\namazon$tpm$nullmdl <- \n  stm(\n    documents = dfm_stm$documents,\n    vocab = dfm_stm$vocab, \n    prevalence =~ src,\n    K = 0, \n    seed = 42,\n    max.em.its = 1000,\n    data = dfm_stm$meta,\n    init.type = \"Spectral\",\n    verbose = FALSE\n  )\ntoc(log = TRUE)\n\n\namazon$tpm$nullmdl\n\nA topic model with 44 topics, 1730 documents and a 765 word dictionary.\n\n\n\n\nk = c(3:20)\n\ntopic_range <- c(3:20) # set topic range based on consistency statistics\n# Plan\nfuture::plan(future::multisession, workers = 6) # use multiple cores\n# Fit models\ntic(\"choose_k_intervall\")\namazon$tpm$stm <- tibble(k = topic_range) %>%\n  mutate(mdl = furrr::future_map(k, ~stm::stm(\n    documents = dfm_stm$documents,\n    vocab = dfm_stm$vocab, \n    prevalence =~ src,\n    K = ., \n    seed = 42,\n    max.em.its = 1000,\n    data = dfm_stm$meta,\n    init.type = \"Spectral\",\n    verbose = FALSE),\n    .options = furrr::furrr_options(seed = 42))\n  )\ntoc(log = TRUE)\n\n\n\n\nChoose model\n\n# Create heldout data\namazon$tpm$heldout <- make.heldout(\n  documents = amazon$tpm$dfm$documents,\n  vocab = amazon$tpm$dfm$vocab,\n  seed = 42)\n# create evaluation\namazon$tpm$model_scores <- amazon$tpm$stm %>% \n  mutate(\n    exclusivity = map(mdl, exclusivity),\n    semantic_coherence = map(mdl,semanticCoherence, amazon$tpm$dfm$documents),\n    eval_heldout = map(mdl, eval.heldout, amazon$tpm$heldout$missing),\n    residual = map(mdl, checkResiduals, amazon$tpm$dfm$documents),\n    bound =  map_dbl(mdl, function(x) max(x$convergence$bound)),\n    lfact = map_dbl(mdl, function(x) lfactorial(x$settings$dim$K)),\n    lbound = bound + lfact,\n    iterations = map_dbl(mdl, function(x) length(x$convergence$bound)))\n\n\namazon$tpm$model_scores %>% \n  transmute(\n    k,\n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")) %>% \n  tibble()\n\n# A tibble: 18 × 5\n       k `Lower bound` Residuals `Semantic coherence` `Held-out likelihood`\n   <int>         <dbl>     <dbl>                <dbl>                 <dbl>\n 1     3      -173925.      2.16                -79.2                 -5.77\n 2     4      -171930.      2.21                -85.6                 -5.72\n 3     5      -171599.      2.18                -87.0                 -5.67\n 4     6      -170066.      2.09                -92.3                 -5.53\n 5     7      -170113.      2.16                -96.4                 -5.55\n 6     8      -169722.      2.16               -102.                  -5.48\n 7     9      -169448.      2.06               -101.                  -5.41\n 8    10      -168686.      2.07               -103.                  -5.34\n 9    11      -168933.      1.98                -97.8                 -5.28\n10    12      -168887.      1.91               -108.                  -5.31\n11    13      -168528.      1.89               -109.                  -5.29\n12    14      -167971.      1.92               -110.                  -5.27\n13    15      -167892.      1.75               -114.                  -5.23\n14    16      -168070.      1.69               -115.                  -5.22\n15    17      -167441.      1.73               -113.                  -5.15\n16    18      -167422.      1.65               -116.                  -5.11\n17    19      -167440.      1.62               -123.                  -5.12\n18    20      -167250.      1.59               -119.                  -5.10\n\n\n\namazon$tpm$model_scores %>% \n  transmute(\n    k,\n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")) %>%   \n  pivot_longer(-k, names_to = \"Metric\", values_to = \"Value\") %>% \n  ggplot(\n    aes(k, Value, color = Metric)) +\n  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(x = \"K (number of topics)\",\n       y = NULL,\n       title = \"Model diagnostics by number of topics\")\n\n\n\n\n\nExclusivity by Coherence\n\namazon$tpm$model_scores %>%\n  select(k, exclusivity, semantic_coherence) %>%\n  filter(k %in% c(5, 7, 11)) %>%\n  unnest(cols = c(exclusivity, semantic_coherence)) %>%\n  mutate(k = as.factor(k)) %>%\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(x = \"Semantic coherence\",\n       y = \"Exclusivity\",\n       title = \"Comparing exclusivity and semantic coherence\",\n       subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\")"
  },
  {
    "objectID": "content/08-text_as_data/08-exercise.html#model-understanding",
    "href": "content/08-text_as_data/08-exercise.html#model-understanding",
    "title": "Showcase",
    "section": "Model understanding",
    "text": "Model understanding\n\nSelect model\n\namazon$stm$mdl <- amazon$tpm$stm %>% \n  filter(k == 5) %>% \n  pull(mdl) %>% \n  .[[1]]\n# Get stm statistics\namazon$stm$mdl\n\nA topic model with 5 topics, 1730 documents and a 765 word dictionary.\n\n\n\n\nInterpretation\n\namazon$stm$mdl %>% labelTopics()\n\nTopic 1 Top Words:\n     Highest Prob: tropfen, geschmack, chlorophyll, wasser, wirkung, schmeckt, schon \n     FREX: tropfen, chlorophyll, glas, grün, farbe, flasche, konnten \n     Lift: alfalfa, dosieren, farbe, frisch, geladen, glas, grüne \n     Score: tropfen, chlorophyll, farbe, grün, geladen, medien, wirkung \nTopic 2 Top Words:\n     Highest Prob: produkt, super, nehme, seit, mehr, sagen, wirklich \n     FREX: nehme, gutes, begeistert, seit, wirkt, hilft, überrascht \n     Lift: begeistert, detox, entgiften, entschlacken, gutes, pfirsich, teste \n     Score: produkt, seit, nehme, weiteren, super, gutes, hilft \nTopic 3 Top Words:\n     Highest Prob: gut, dass, tag, tagen, ganz, finde, besser \n     FREX: ganz, finde, möchte, produkte, tagen, anfang, wenig \n     Lift: anfang, soweit, generell, produkte, komisch, versucht, möchte \n     Score: gut, generell, finde, dass, ganz, tag, tagen \nTopic 4 Top Words:\n     Highest Prob: schmeckt, geschmack, shake, lineavi, shaker, pulver, lecker \n     FREX: shake, lineavi, shaker, pulver, lecker, shakes, almased \n     Lift: anleitung, dosen, ersetzt, aktivkost, almased, classic, diätshake \n     Score: lineavi, shaker, shake, almased, pulver, shakes, milch \nTopic 5 Top Words:\n     Highest Prob: schon, wochen, leider, kapseln, abgenommen, sport, abnehmen \n     FREX: kapseln, abgenommen, tabletten, geld, bringt, sport, schlucken \n     Lift: abgenommen, achten, burn, gebracht, gegenteil, geld, geldverschwendung \n     Score: kapseln, tabletten, keto, burn, abgenommen, sport, kilo \n\n\n\nHighest Prob\n\nlabelTopics(amazon$stm$mdl, n = 15)$prob %>%\n  t() %>% \n  as.data.frame() %>% \n  janitor::clean_names() %>% \n  tibble() %>% \n  rename_with(.fn = ~ stringr::str_replace(., \"v\", \"topic_\"), .cols = v1:v5) %>% \n  paged_table()\n\n\n\n  \n\n\n\n\nlabelTopics(amazon$stm$mdl, n = 15)$prob%>%\n  as.data.frame() %>% \n  janitor::clean_names() %>% \n  tibble() %>% \n  rownames_to_column(var = \"topic\") %>% \n  mutate(prob = paste(v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,\n                      v11, v12, v13, v14, v15, \n                      sep = \"; \")) %>% \n  select(topic, prob) %>% \n  kbl() %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n    topic \n    prob \n  \n \n\n  \n    1 \n    tropfen; geschmack; chlorophyll; wasser; wirkung; schmeckt; schon; fühle; glas; gut; fitter; einfach; flasche; grün; immer \n  \n  \n    2 \n    produkt; super; nehme; seit; mehr; sagen; wirklich; zufrieden; hilft; leicht; geschmack; gutes; wirkt; erst; körper \n  \n  \n    3 \n    gut; dass; tag; tagen; ganz; finde; besser; wasser; nehmen; macht; geht; trinke; wenig; allerdings; möchte \n  \n  \n    4 \n    schmeckt; geschmack; shake; lineavi; shaker; pulver; lecker; shakes; satt; mal; immer; schon; almased; diät; einfach \n  \n  \n    5 \n    schon; wochen; leider; kapseln; abgenommen; sport; abnehmen; mal; wirkung; ernährung; empfehlen; kg; kilo; tabletten; woche \n  \n\n\n\n\n\n\n\nFREX\n\nlabelTopics(amazon$stm$mdl, n =15)$frex %>%\n  t() %>% \n  as.data.frame() %>% \n  janitor::clean_names() %>% \n  tibble() %>% \n  rename_with(.fn = ~ stringr::str_replace(., \"v\", \"topic_\"), .cols = v1:v5) %>% \n  paged_table() \n\n\n\n  \n\n\n\n\nlabelTopics(amazon$stm$mdl, n =15)$prob %>%\n  as.data.frame() %>% \n  janitor::clean_names() %>% \n  tibble() %>% \n  rownames_to_column(var = \"topic\") %>% \n  mutate(prob = paste(v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,\n                      v11, v12, v13, v14, v15, \n                      sep = \"; \")) %>% \n  select(topic, prob) %>% \n  kbl() %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n \n  \n    topic \n    prob \n  \n \n\n  \n    1 \n    tropfen; geschmack; chlorophyll; wasser; wirkung; schmeckt; schon; fühle; glas; gut; fitter; einfach; flasche; grün; immer \n  \n  \n    2 \n    produkt; super; nehme; seit; mehr; sagen; wirklich; zufrieden; hilft; leicht; geschmack; gutes; wirkt; erst; körper \n  \n  \n    3 \n    gut; dass; tag; tagen; ganz; finde; besser; wasser; nehmen; macht; geht; trinke; wenig; allerdings; möchte \n  \n  \n    4 \n    schmeckt; geschmack; shake; lineavi; shaker; pulver; lecker; shakes; satt; mal; immer; schon; almased; diät; einfach \n  \n  \n    5 \n    schon; wochen; leider; kapseln; abgenommen; sport; abnehmen; mal; wirkung; ernährung; empfehlen; kg; kilo; tabletten; woche \n  \n\n\n\n\n\n\n\nPrevalence\n\namazon$stm$preval <- list(\n  terms_beta = amazon$stm$mdl %>% tidy(),\n  doc_gamma = amazon$stm$mdl %>% tidy(\"gamma\")\n)\namazon$stm$preval$top_terms <- amazon$stm$preval$terms_beta %>% \n  arrange(., beta) %>%\n  group_by(topic) %>%\n  top_n(10, beta) %>%\n  arrange(-beta) %>%\n  select(topic, term) %>%\n  summarise(terms = list(term)) %>%\n  mutate(terms = map(terms, paste, collapse = \", \")) %>%\n  unnest(cols = c(terms))\n  \namazon$stm$preval$top_topics <- amazon$stm$preval$doc_gamma %>%\n  group_by(., topic) %>%\n  summarise(gamma = mean(gamma)) %>%\n  arrange(desc(gamma))\namazon$stm$preval$tgt <- amazon$stm$preval$top_topics %>% \n  left_join(amazon$stm$preval$top_terms, by = \"topic\") %>%\n  mutate(topic = paste0(\"Topic \", topic),\n         topic = reorder(topic, gamma))\n\n\nVisual\n\namazon$stm$preval$tgt  %>% \n  ggplot(aes(topic, gamma, label = terms, fill = topic)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(hjust = 1.1, nudge_y = 0.0005, size = 3, color = \"white\") +\n  coord_flip() +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.35), labels = scales::percent) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major = element_blank()) +\n  labs(x = NULL, y = expression(gamma)) +\n  scale_fill_jama()"
  },
  {
    "objectID": "content/08-text_as_data/08-exercise.html#topic-analysis",
    "href": "content/08-text_as_data/08-exercise.html#topic-analysis",
    "title": "Showcase",
    "section": "Topic Analysis",
    "text": "Topic Analysis\n\nPreparation\n\nGet gamma statistics for the documents\n\namazon$stm$merge <- list(\n  doc_gamma_wide = amazon$stm$preval$doc_gamma %>% \n    group_by(document) %>% \n    pivot_wider(\n      id_cols = document,\n      names_from = \"topic\", \n      names_prefix = \"gamma_topic_\",\n      values_from = \"gamma\") %>% \n    rename(doc_id_gamma = document),\n  doc_gamma_top = amazon$stm$preval$doc_gamma %>%\n    group_by(document) %>% \n    slice_max(gamma) %>% \n    rename(\n      top_topic = topic,\n      top_gamma = gamma)\n  )\n\n\n\nJoin with original data set\n\namazon$data$tpm <- amazon$data$de_senti %>% \n  filter(\n    doc_id != 108 & doc_id != 790 &\n    doc_id != 914 & doc_id!= 1328 &\n    doc_id != 1349 & doc_id != 1706) %>% \n  bind_cols(amazon$stm$merge$doc_gamma_top) %>% \n  fastDummies::dummy_cols(., select_columns = c(\"top_topic\")) %>% \n  bind_cols(amazon$stm$merge$doc_gamma_wide) %>% \n  select(!doc_id_gamma) %>% \n  mutate(across(top_topic, as.factor))\n\n\n\n\n\n\n\nAnalysis\n\nTopic over years\n\namazon$data$tpm %>% \n  ggplot(aes(year, fill = top_topic)) +\n  geom_bar() +\n  scale_fill_locuszoom() +\n  theme_pubr()\n\n\n\n\n\n\nTopic by product\n\namazon$data$tpm %>% \n  flat_table(top_topic, src)\n\n          src p01 p02 p03 p04 p05\ntop_topic                        \n1               0   8   0   0 372\n2               3 164  28   0  14\n3               1  10   2   0   0\n4             407   0   0 128   0\n5              26  72 477  14   4\n\n\n\n\nTop reviews by topic\n\nPreparation\n\ntop_stories <- list()\nselect_topic <- paste0(\"_\", seq(from = 1, to = 5, by = 1))\nfor (i in select_topic) {\n  i_gamma <- paste0(\"gamma_topic\", i)\n  i_topic <- paste0(\"top_topic\", i)\n  \n  top_stories[[i_gamma]] <- \n    amazon$data$tpm[order(amazon$data$tpm[i_gamma], decreasing = TRUE),] |>\n    select(id, date, src, review_title, review_text, review_star,\n           valence, polarity, pol_fct, val_fct,\n           top_gamma, all_of(i_gamma),\n           top_topic, all_of(i_topic), review_body) |>\n    rename(\n      gamma = all_of(i_gamma),\n      top_topic_d = all_of(i_topic)\n      ) |>\n    dicho(gamma, dich.by = 0.49, suffix = \"_d\") |>\n    mutate(\n      across(top_topic_d, as.factor),\n      gamma_top  = case_when(top_topic_d == 1 ~ gamma, TRUE ~ NA_real_),\n      gamma_main = case_when(gamma_d == 1 ~ gamma, TRUE ~ NA_real_),\n    ) |>\n    relocate(review_body, .after = top_topic) \n  }\n\n\n\nCreate output\n\nlibrary(sjlabelled)\noutput <- list()\ngamma_topics <- amazon$data$tpm %>% \n  select(starts_with(\"gamma_topic_\")) %>% \n  colnames()\nfor (i in gamma_topics) {\n  # Create dataset\n  output[[i]]$data <- top_stories[[i]] |>\n    filter(top_topic_d == 1)\n  \n  # Create crosstable\n  ## top_topic_d == Dummy: Is i the top topic (== highest relative gamma) of the story\n  ## gamma_d     == Dummy: Is i higher than 0.5 (== highest absolute gamma) of the story\n  output[[i]]$crosstable <- top_stories[[i]] |>\n    select(top_topic_d, gamma_d) %>% \n    mutate(\n      across(everything(), add_labels, labels = c(\"No\" = 0, \"Yes\" = 1)),\n      across(top_topic_d, set_label, label = \"Highest realtive gamma\"),\n      across(gamma_d, set_label, label = \"Highest absolute gamma\")\n    ) |>\n    label_to_colnames() |>\n    flat_table()\n  # Create descriptive statistics: location parameters for different gammas\n  ## gamma_top = only articles where i is the top topic (== highest relative beta)\n  ## gamma_main = only articles where i is the main topic (== highest absolute beta)\n  output[[i]]$desc_gamma <- top_stories[[i]] |>\n     select(gamma_top, gamma_main) |>\n     mutate(\n       across(gamma_top, set_label, label = \"Highest realtive gamma\"),\n       across(gamma_main, set_label, label = \"Highest absolute gamma\")\n       ) |>\n       descr(show = c(\"label\", \"n\", \"NA.prc\", \"mean\", \"sd\", \"se\", \"md\", \"range\"))\n }\n\n\nprint_output_descriptives <- function(x) {\n  # Loop\n  for (i in x) {\n    # Headline\n    cat(paste(\"Results for\", i, \"\\n\"))\n    # Mutli-topic abstracts\n    glue(\"## Check for ambiguous topic assignment for review:\nComments: \n- Highest relative gamma: Is {i} the top topic of the abstract?\n- Highest absolute gamma: Is the gamme value for {i} higher than 0.5\n \\n\") |> print()\n    # Crosstable \n   output[[i]]$crosstable |> print()\n    # Location parameters\n   output[[i]]$desc_gamma |> print()\n  }\n}\nprint_output_abstracts <- function(x) {\n  # Loop\n  for (i in x) {\n    for (j in c(1:5)) {\n      glue(\"\n           ***************************************************\n           \n           ID: {output[[i]]$data$id[[j]]} \\n\n           Date: {output[[i]]$data$date[[j]]} \\n\n           Title: {output[[i]]$data$review_title[[j]]} \\n\n           Gamma: {round(output[[i]]$data$gamma[[j]],3)} \\n\n           Review: \\r\n           {output[[i]]$data$review_text[[j]]} \\n\n           Rating: {output[[i]]$data$review_star[[j]]} \\n\n           Valence: {output[[i]]$data$val_fct[[j]]} ({round(output[[i]]$data$valence[[j]],3)}) \\n\n           Polarity: {output[[i]]$data$pol_fct[[j]]} ({round(output[[i]]$data$polarity[[j]],3)}) \n           \\n\\n\") |> print() \n    }\n  }\n}\n\n\n\nOutput by topics\n\nTopic 1Topic 2Topic 3Topic 4Topic 5\n\n\n\nprint_output_descriptives(gamma_topics[1])\n\nResults for gamma_topic_1 \n## Check for ambiguous topic assignment for review:\nComments: \n- Highest relative gamma: Is gamma_topic_1 the top topic of the abstract?\n- Highest absolute gamma: Is the gamme value for gamma_topic_1 higher than 0.5\n \n                       Highest absolute gamma   No  Yes\nHighest realtive gamma                                 \nNo                                            1350    0\nYes                                             93  287\n\n## Basic descriptive statistics\n\n        var                  label   n NA.prc mean   sd se   md\n  gamma_top Highest realtive gamma 380  78.03 0.55 0.09  0 0.55\n gamma_main Highest absolute gamma 287  83.41 0.59 0.07  0 0.58\n            range\n 0.51 (0.27-0.79)\n  0.3 (0.49-0.79)\n\n\n\nprint_output_abstracts(gamma_topics[1])\n\nID: p05_218\nDate: 2021-07-17\nTitle: Ich liebe dieses Grün!!!\nGamma: 0.788\nReview: Die Medien konnten nicht geladen werden. Dass Chlorophyll sehr gesund ist und viele positive Wirkungen hat, ist bekannt. Zumindest in den Kreisen, die sich damit beschäftigen. Grüner Grass-Saft wird als ein wahres Wundermittel angepriesen, nur leider schmeckt er den Wenigsten. Ich kann ihn auch immer nur phasenweise trinken oder mische ihn dann mit Früchten etc. Dauerhaft konnte ich ihn leider noch nicht in mein Leben integrieren. Anders, diese wunderbaren Tropfen. Ich träufle sie morgens in ein Glas Wasser und beobachte erstmal wie sich das intensive Grün verteilt und nach und nach das Wasser einfärbt. Das ist fast wie Meditation für mich, weil es so schön aussieht. Danach trinke ich das Wasser und ja, es schmeckt auch nach Gras, aber nur leicht und es ist überhaupt kein Problem es zu trinken. Ich liebe diese Tropfen. Das wird meine zukünftige Art des Chlorophyll Konsums sein :-)\nRating: 5,0 von 5 Sternen\nValence: positiv (0.018)\nPolarity: positiv (1.099)\n\nID: p05_267\nDate: 2021-07-02\nTitle: Schmecken frisch-grasig\nGamma: 0.764\nReview: Ich habe die Tropfen als Produkttester gratis erhalten. Chlorophyll soll mehrere gesundheitliche Vorteile bringen, aber das kann ich nicht bewerten. Die Tropfen haben eine sehr dunkle, bläulich-grüne Farbe. In einem mittelgroßen Glas Wasser schmeckt man den grasigen geschmack kaum raus. Pur schmecken sie auch nicht schlimm.\nRating: 5,0 von 5 Sternen\nValence: negativ (-0.046)\nPolarity: neutral (0)\n\nID: p05_237\nDate: 2021-07-11\nTitle: Schnelle Lieferung, geschmacklich sehr gut\nGamma: 0.76\nReview: Die Lieferung war sehr schnell. Die Amwendung ist sehr einfach. 20 Tropfen In ein Glas Wasser und trinken. Geschmacklich sehr gut und lässt sich daher problemlos einnehmen. Die Farbe ist ein intensives grün.\nRating: 5,0 von 5 Sternen\nValence: positiv (0.124)\nPolarity: positiv (2.833)\n\nID: p05_205\nDate: 2021-07-22\nTitle: Fühlt sich gut an!\nGamma: 0.757\nReview: Die Medien konnten nicht geladen werden. Der erste Eindruck der Flasche ist schon positiv: zum Schutz vor Licht/Sonnenstrahlung ist die Flasche aus braunen Glas gefertiggt; und zwar aus schönem dunklem braunen Glas, so dass der Füllungsgrad so grade noch erkennbar, dafür die Schutzwirkung umso besser ist. Und die Pipette ist stabil und gut dosierbar. Geruch und Geschmack sind so, wie ich mir den Geschmack von Chlorophyll vorstelle: riecht und schmeckt sehr dezent nach frischem zwischen den Fingern zerriebenen Gras. Farbe ist wunderbar tief-grün, verteilt sich gut im Wasser und lässt sich von daher sehr gut trinken. Ich habe es mir zur Angewohnheit gemacht, morgens auf nüchteren Magen ein Glas Wasser mit der empfohlenen Menge von 20 Tropfen zu trinken (d.h. eine Flasche hält dann ca. 2 bis 2,5 Monate). Den ersten Monat habe ich jetzt fast rum. Ich habe vorher schon ziemlich gesund gelebt … nun ist eine weitere kleine Steigerung spürbar: Verdauung/Stuhl fühlt sich noch besser an, als vorher, und fitter/vitaler fühle ich mich auch. Werde von daher die Tropfen aufbrauchen und eine weitere Flasche anschaffen. Sollten sich weitere Verbesserungen oder ggfs. Verschlechterungen ergeben, werde ich dies hier kommentieren.\nRating: 5,0 von 5 Sternen\nValence: positiv (0.09)\nPolarity: positiv (1.551)\n\nID: p05_091\nDate: 2022-01-13\nTitle: Natur in einer Flasche.\nGamma: 0.755\nReview: Die Medien konnten nicht geladen werden. Es schmeckt nach fast gar nichts und riecht bisschen nach Wald. Man spûrt nach dem Trinken mit bisschen kalten Wasser eine sehr intensive Erfrischung. Kann ich jedem empfehlen\nRating: 5,0 von 5 Sternen\nValence: negativ (-0.001)\nPolarity: positiv (0.511)\n\n\n\n\nprint_output_descriptives(gamma_topics[2])\n\nResults for gamma_topic_2 \n## Check for ambiguous topic assignment for review:\nComments: \n- Highest relative gamma: Is gamma_topic_2 the top topic of the abstract?\n- Highest absolute gamma: Is the gamme value for gamma_topic_2 higher than 0.5\n \n                       Highest absolute gamma   No  Yes\nHighest realtive gamma                                 \nNo                                            1521    0\nYes                                            200    9\n\n## Basic descriptive statistics\n\n        var                  label   n NA.prc mean   sd   se   md\n  gamma_top Highest realtive gamma 209  87.92 0.38 0.07 0.00 0.37\n gamma_main Highest absolute gamma   9  99.48 0.53 0.04 0.01 0.51\n            range\n 0.37 (0.24-0.61)\n 0.12 (0.49-0.61)\n\n\n\nprint_output_abstracts(gamma_topics[2])\n\nID: p03_515\nDate: 2020-08-24\nTitle: Super\nGamma: 0.61\nReview: Ich nutze das Produkt aktuell seit 10 Tagen und bin vollends zufrieden. Der Geschmack ist sehr angenehm, so fällt ein die Einnahme einfacher.Werde das Produkt auf jeden Fall weiterempfehlen!\nRating: 5,0 von 5 Sternen\nValence: positiv (0.267)\nPolarity: positiv (2.565)\n\nID: p03_522\nDate: 2020-08-17\nTitle: gutes Produkt , der Effekt ist Spürbar\nGamma: 0.584\nReview: Ich habe dieses Produkt bestellt da ich momentan verschiedene Fat Burner teste und ähnlich wie bei dem anderen Produkt was ich 30 Tage verwendet habe ist die Wirkung auf leerem Magen stark zu spüren. Verstärktes schwitzen und aufgedreht sein gehört bei Fat burnern für mich dazu , da es eine ankurbelnde Wirkung auf den Metabolismus hat. Gutes Produkt :) Geschmack ist Neutral wie es sein muss.\nRating: 5,0 von 5 Sternen\nValence: positiv (0.145)\nPolarity: positiv (1.099)\n\nID: p03_330\nDate: 2021-02-12\nTitle: Mein Begleiter in der Keto-Diät\nGamma: 0.566\nReview: Ich beschäftige mich nun bereits eine Weile mit dem Ketogenen-Lifestyle und bin auf dieses Produkt gestoßen. Dieses nehme ich nun bereits paar Tage zu mir und merke spürbar, wie es meine Fettverbrennung steigert. Würde raten, dazu genügend Wasser zu trinken da man schon merklich mehr ins Schwitzen kommt. Kaufempfehlung!\nRating: 5,0 von 5 Sternen\nValence: positiv (0.037)\nPolarity: positiv (1.946)\n\nID: p03_270\nDate: 2021-04-21\nTitle: Ab der ersten Tablette wirksam\nGamma: 0.526\nReview: Ab der ersten Tablette hatte ich irgendwie schon kaum mehr ein Hungergefühl, Heißhunger, süßkram usw) auch ist mir aufgefallen das ich viel schneller satt bin als vorher 5 volle Sterne….\nRating: 5,0 von 5 Sternen\nValence: positiv (0.159)\nPolarity: positiv (1.946)\n\nID: p03_466\nDate: 2020-09-29\nTitle: Empfehlungswert\nGamma: 0.511\nReview: Ich empfehle dieses Produkt weiter, da ich es seit Paar Tage benutze, und es wirkt wirklich sehr gut .\nRating: 5,0 von 5 Sternen\nValence: positiv (0.372)\nPolarity: positiv (1.099)\n\n\n\n\nprint_output_descriptives(gamma_topics[3])\n\nResults for gamma_topic_3 \n## Check for ambiguous topic assignment for review:\nComments: \n- Highest relative gamma: Is gamma_topic_3 the top topic of the abstract?\n- Highest absolute gamma: Is the gamme value for gamma_topic_3 higher than 0.5\n \n                       Highest absolute gamma   No  Yes\nHighest realtive gamma                                 \nNo                                            1717    0\nYes                                             13    0\n\n## Basic descriptive statistics\n\n        var                  label  n NA.prc mean   sd   se   md\n  gamma_top Highest realtive gamma 13  99.25 0.32 0.07 0.02 0.29\n gamma_main Highest absolute gamma  0 100.00  NaN   NA   NA   NA\n           range\n 0.2 (0.26-0.46)\n -Inf (Inf--Inf)\n\n\n\nprint_output_abstracts(gamma_topics[3])\n\nID: p03_055\nDate: 2022-02-11\nTitle: Gewöhnungseffekt\nGamma: 0.458\nReview: Nach ein paar Tagen gewöhnt man sich an die Kapseln/ der Körper. Einen ” Leistungs Abfall”, merke ich Mittags nicht mehr ganz so stark, je nach Tag. Abgenommen habe ich nicht. Ernähre mich überwiegend ausgewogen, jeden Tag Obst und Gemüse, ja auch mal was Süsses ( dies in Maßen, nicht in Massen). Der Effekt könnte funktionieren mich noch etwas positiver ausfallen, bzgl des abnehmens. Sport betreibe ich wenig, dafür täglich. Dafür laufe ich täglich mehrfach die Treppen in unserem Haus hoch u runter etc\nRating: 5,0 von 5 Sternen\nValence: negativ (-0.022)\nPolarity: positiv (0.588)\n\nID: p01_134\nDate: 2018-05-02\nTitle: Ja ich habe abgenommen…\nGamma: 0.426\nReview: Nach zwei Geburten und Stress durch Arbeit sowie Trennung von meinem Mann hatte ich mit 1,58 m mittlerweile 70 kg gewogen (hatte sehr viel ohne nachzudenken in mich reingefuttert, vorher war ich bei 55 kg) und fand mich absolut nicht schön und unwohl. Da bin ich auf die Suche nach einem Unterstützer für den Anfang einer Diät gewesen und nach lesen vieler Rezensionen verschiedenster Produkte bei LINEAVI gelandet (3-er Pack). Ich starte voller Tatendrang mit der Turbo-Diät. Ich habe mich strickt dran gehalten und auch nur Wasser dazu getrunken und Gemüsebrühe gegessen. Allerdings hatte ich am dritten Tag mit dem Kreislauf zu tun und Kopfschmerzen, so dass ich sofort auf die 2er-Dosis pro Tag wechselte und täglich eine normale Mahlzeit zu mir nahm. Da ich noch nicht genau wusste, was darf ich in welchem Maße essen, lud ich mir zusätzlich eine App herunter, womit ich meine Werte, hauptsächlich die Kalorien, gut überwachen kann (das mache ich jetzt auch weiter). Da ich eher von der 3-Shakes-Variante gewechselt habe zur 2-Shakes-Varinate habe ich diese bis zum Tag 12 durchgezogen, danach täglich ein Shake. Allerdings muss ich sagen, obwohl ich nie mehr als die vorgeschriebenen Löffel genommen habe, reicht die Portion bei mir nur für 28 Mahlzeiten (ich habe auch keine Berge auf den Löffel gegeben). Allerdings muss ich ganz ehrlich gestehen, ich habe sehr schnell ein Hungergefühl bzw. Magenknurren bekommen, manchmal schon nach einer halben Stunde, manchmal nach zwei (man muss schon ganz schön sein Ziel wollen, um nicht nachzugeben). Ich habe es versucht mit viel Wasser und Tee zu stillen, aber das ist mir nicht recht gelungen, das Knurren blieb. Aber ich habe nicht nachgegeben. Vom Geschmack her muss ich sagen, am Anfang habe ich den Shake mit Sonnenblumenöl angerüht. Das ging gar nicht. Dann habe ich mir Walnussöl besorgt und damit ging es vom Geschmack. Es schmeckt etwas wie roher Kuchenteig, naja muss man mögen. Bisschen ungewohnt ist die Konsistenz. So und jetzt kommt das Ergebnis: Ja ich habe abgenommen, genau genommen 4 kg in bisschen über 14 Tagen, da ich ja den Anfang gestreckt habe. Das finde ich persönlich schön. Allerdings werde ich jetzt nicht weiter das LINEAVI nehmen sondern lieber auf gesunde Ernährung umsteigen und dies mit meiner App überwachen, da diese Trinks doch nicht ganz meins sind. Allerdings für den Einstieg war es für mich sehr gut, aber nichts auf Dauer. Empfehlen kann ich es auf jeden Fall, aber ich bin der Meinung, man muss stark sein um durchzuhalten. Der Shaker der dabei war, ist sehr praktisch und ohne Probleme zu verwenden, fast immer hat sich bei mir alles aufgelöst. Falls ich, was ich jetzt nicht hoffe, wieder einmal mehr Kilos habe als gewünscht, würde ich dieses Produkt zum Einstieg auch verwenden.\nRating: 4,0 von 5 Sternen\nValence: positiv (0.065)\nPolarity: positiv (1.017)\n\nID: p03_237\nDate: 2021-05-28\nTitle: Oke\nGamma: 0.425\nReview: Ich hab’s für eine Freundin gekauft und ich gehe jetzt einfach mal ihre Bewertung ab sie hat mir erzählt dass sie nicht mehr so viel Hunger hat und so aber man wird dadurch nicht Döner Mann nimmt nicht ab es passiert gar nichts meiner Meinung nach ist es eine Verschwendung des Geld dafür auszugeben allerdings wenn du fress Attacken hast und Ernährungsergänzungsmittel brauchst ist es gut aber dann kannst du dir auch im DM für 4,59 € ein Ernährungsergänzungsmittel kaufen das meiner Meinung nach genauso gut ist\nRating: 3,0 von 5 Sternen\nValence: negativ (-0.02)\nPolarity: neutral (0)\n\nID: p02_216\nDate: 2020-06-19\nTitle: Hilft bei mir super gegen Blähbauch und Cellulite\nGamma: 0.359\nReview: Ich bin jetzt in der 3. Woche und habe eine Flasche leergetrunken bzw. die zweite jetzt angefangen.Hier mein Zwischenbericht:Gekauft hatte ich Dietyslim nach dem Motto: klingt nicht schlecht.. mal gucken, ob’s was taugt.Dank dieser Larifari-Einstellung habe ich leider keine Fotos gemacht, mit denen ich die tollen Ergebnisse jetzt hätte vorzeigen können. Denn es gibt zwei auffallende Merkmale seit der Einnahme: die Cellulite an den Beinen und Po ist fast komplett weg und mein Blähbauch ist deutlich reduziert. Es ist nicht so, dass ich extreme Cellulite hatte, aber es gab halt trotzdem unschöne Dellen hier und da, die nun fast vollständig (!) verschwunden sind. Ich glaube, ich hatte das letzte Mal solche glatten und makellosen Beine in meiner Jugend (bin jetzt 47).Mein Blähbauch, der einem 6.Monat-Schwangerschaftsbauch Konkurrenz machte, ist deutlich kleiner geworden. Laut der Inhaltsangabe helfen Papayas bei Blähungen und Verdauungsbeschwerden. Dank dieser Info werde ich dann Papaya-Enzym-Kapseln nochmal extra kaufen, um meinen Blähbauch weiter gezielt zu minimieren.Es tut sich also was in meinem Körper; über weitere Wirkungen kann ich noch nicht berichten. Dass ich jetzt öfters auf Toilette muss, kann ich nicht behaupten. Meine Ernährung habe ich nicht umgestellt. Wie sicher viele andere versuche ich möglichst gesund zu essen, das klappt mehr und mehr weniger gut gut. Abgenommen habe ich minimal, die Hose sitzt etwas lockerer, ich schätze mal, dass es 1-2 kg weniger sind.Den Geschmack fand ich am Anfang nicht so prickelnd. Ich mixe mir das ganze wie folgt: eine große Tasse (Kaffeebecher) zur Hälfte mit kalten Wasser, ein halbes Schnapsglas Dietyslim rein und dann heißes Wasser aufgießen. Schmeckt dann also wie warmer Eistee. Meistens eine Tasse am Morgen und manchmal noch eine Tasse abends.Weil das ganze geschmacklich eine Eisteesorte wäre, die mir nicht so sehr schmeckt, habe ich anfangs echten Eistee, den ich lecker finde, dazugemixt. So ließ sich das ganze ganz gut trinken. Später habe ich den richtigen Eistee wieder nach und nach weggelassen, so dass ich jetzt nur noch Dietyslim (wie am Anfang) pur mit Wasser trinke, aber ich habe mich an den Geschmack gewöhnt habe und finde ihn nicht mehr schlimm.Wer sich gar nicht an den Geschmack gewöhnen kann, dem empfehle ich wirklich, einen Saft der Wahl etwas dazuzumixen.Sollten noch Veränderungen eintreten, werde ich diese Rezession ergänzen.Für mich hat sich Dietyslim definitiv gelohnt.\nRating: 5,0 von 5 Sternen\nValence: positiv (0.098)\nPolarity: positiv (1.099)\n\nID: p02_007\nDate: 2022-11-14\nTitle: Es geht so\nGamma: 0.301\nReview: Also das Produkt ist wenn man entwässern möchte sehr gut . Bei mir hat es extrem gewirkt musst soviel auf Toilette. Effekt sicherluch ganz gut. Geschmacklich nicht mein Ding könnte man verbessern und ansonsten gab es keine Abnehmerfolge dadurch zum entgiften vielleicht ok\nRating: 3,0 von 5 Sternen\nValence: positiv (0.249)\nPolarity: positiv (1.946)\n\n\n\n\nprint_output_descriptives(gamma_topics[4])\n\nResults for gamma_topic_4 \n## Check for ambiguous topic assignment for review:\nComments: \n- Highest relative gamma: Is gamma_topic_4 the top topic of the abstract?\n- Highest absolute gamma: Is the gamme value for gamma_topic_4 higher than 0.5\n \n                       Highest absolute gamma   No  Yes\nHighest realtive gamma                                 \nNo                                            1195    0\nYes                                            181  354\n\n## Basic descriptive statistics\n\n        var                  label   n NA.prc mean   sd se   md           range\n  gamma_top Highest realtive gamma 535  69.08 0.54 0.10  0 0.54 0.49 (0.3-0.79)\n gamma_main Highest absolute gamma 354  79.54 0.60 0.06  0 0.59 0.3 (0.49-0.79)\n\n\n\nprint_output_abstracts(gamma_topics[4])\n\nID: p01_390\nDate: 2016-06-09\nTitle: Lineavi - Die neue Wunderformel?\nGamma: 0.794\nReview: Im Verlauf der letzten 10 Jahre gelang es mir wiederholt, mein Gewicht unter Zurhilfenahme diverser “Schlankpülverchen” um ca. 10-15kg zu reduzieren. Da ich es jedoch bis heute nicht hinbekomme, mein “Traumgewicht” zu halten, kann sich die Industrie auch weiterhin über einen getreuen Abnehmer -im doppelten Wortsinn- freuen ;o). Um so schöner also, dass auch an der Eiweisspulverfront für reichlich Abwechslung gesorgt ist. Nach Slimfast, Layenberger und Almased nun also Lineavi.Was ist anders?Nun - zum einen wäre da einmal der Geschmack. Da bei diesem Produkt auf die Zugabe von diversen Geschmacksstoffen wie Erdbeere, Aprikose u.s.w. verzichtet wurde, schmeckt Lineavi einfach anders. Eigentlich ein völlig neuer Geschmack, der aber auch zugegebenermaßen polarisiert. Am ehesten vergleichbar mit Almased; nur etwas süßer. Zudem ist die dezente Note von Erbsen deutlich schmeckbar. Überhaupt sind m.E. Parallelen zu Almased durchaus erkennbar. Ähnliche Bestandteile, ähnliche Konzistenz nach der Zubereitung, u.s.w.. Auch haben beide Produkte unter dem Strich etwa den gleichen Brennwert (340/378 kcal), Lineavi sättigt m.E. aber auf Grund des relativ hohen Kohlenhydratgehalts (23,4g/100g) gefühlt etwas nachhaltiger.Wirkt es?Die Frage habe ich im Grunde ja eingangs bereits beantwortet: Ja, wenn man es sinvoll anwendet, kann man ohne große Mühe etwa 1kg pro Woche abnehmen. Das ist Fakt. Dies klappt zugegebenermaßen aber nicht nur mit Lineavi. Mit etwas Disziplin, Bewegung (muß kein Leistungssport sein, ich selbst bin ein absoluter Bewegungsmuffel), ist das wirklich kein Problem)Wem kann man Lineavi denn nun empfehlen?Grob gesagt jedem, der wärend einer Diät einen geeigneten Sattmacher zur Unterstützung benötigt und seinen Körper in dieser Phase mit zusätzlichen Mineralstoffen und Vitaminen versorgen möchte; ohne diesen dabei unnötigerweise eine unüberschaubare Menge an Geschmacks-, Aroma- und Süßstoffen zu zuführen. Hier kann Lineavi ohne Frage punkten. Im Rahmen einer LowCarb Diät erscheinen mir 23,4g Zucker pro 100g allerdings nicht so ganz zielführend.Werde ich es weiterhin verwenden?Ja, das werde ich ganz sicher. Zum einen weil’s mir schmeckt, zum anderen weil es mich doch über einen Zeitraum von 4-5 Stunden sättigt. Und obwohl, um die eingangs gestellte Frage zu beantworten, mit Lineavi das Rad sicher nicht neu erfunden wurde, ist es dennoch eine willkommene Abwechslung zu den üblichen Verdächtigen.\nRating: 4,0 von 5 Sternen\nValence: positiv (0.075)\nPolarity: positiv (1.196)\n\nID: p01_438\nDate: 2016-03-08\nTitle: Süß, aber sättigt\nGamma: 0.771\nReview: DIE REZENSION BEZIEHT SICH AUF: Lineavi Vitalkost – Der gesunde Diät Shake für Ihr Abnehmprogramm + Shaker, 500g (Starterpaket)Rezension eines kostenfreien Testartikels.===FAZIT===Die Mahlzeiten sind ausgewogen und versorgen den Körper mit allen Nährstoffen, Vitaminen und Spurenelementen. Das Beste ist, er macht wirklich anhaltend satt.Die Zusammensetzung hat mit ca. 25 % Kohlenhydraten (bezogen auf die trinkfertige Zubereitung), von den der überwiegende Zucker ist, einen Anteil der mir für einen Diätdrink recht hoch erscheint, speziell am Abend.Der Shake enthält laktosefreie Molke und kann daher aus von laktoseintoleranten Personen eingenommen werden.Der Lineavi Shake ist ungeeignet für Leute, die keine süßen Sachen mögen (1 Stern Abzug). Geschmacklich erinnert mich der Shake an übersüßtes Brot, nicht unbedingt mein Favorit, aber trinkbar.Beim ersten Öffnen ist eine Alu-Folie zu entfernen, die die einzelne Dose versiegelt. Beim Abziehen dieser Siegelfolie hat es einen Teil des sehr feinen Pulvers nach Außen gewirbelt und direkt Arbeitsplatte und Kleidung eingesaut… (1 Stern Abzug).Der Shake lässt sich leider nicht in eine herzhafte Geschmacksrichtung abwandeln, aber mit Obst, Gemüse und sonstigen Zutaten zum Smoothie variieren.Kaufempfehlung: ja, für Freunde süßen Geschmacks.3 Sterne===LIEFERUMFANG STARTERPAKET===Eine Dose mit 500 g Pulver (10 Portionen), Meßlöffel, Shake-Becher mit Siebeinsatz, Anweisung zur Zubereitung und zum Ablauf der Diät oder Entgiftung.===PRAXISTEST===Zunächst hab ich mich Belesen vornehmlich auf der Produktpage des Herstellers. Initial wird empfohlen 4-5 Shakes täglich zu konsumieren, je nach Körpergröße. Im Laufe der Diät werden immer mehr Shakes durch ausgewogene Mahlzeiten ersetzt, es sind einige Vorschläge für Mahlzeiten im Begleitheft vorhanden, mehr finden sich auf der Herstellerseite.Die Viskosität ist mit 5 Messlöffeln sehr hoch, mit 4 Messlöffeln finde ich es angenehmer zu trinken.Wem der Geschmack zu süß ist und das Getränk nach einiger Zeit nicht mehr runterkriegt kann mal versuchen Crushed Ice unterzumischen. Das nimmt etwas der Süße.Der Abnehmerfolg lag bei knapp 150 g/Tag in den ersten Tagen.\nRating: 3,0 von 5 Sternen\nValence: positiv (0.093)\nPolarity: positiv (1.718)\n\nID: p01_422\nDate: 2016-04-12\nTitle: Ablauf / Geschmack / Effekt\nGamma: 0.757\nReview: Da meine Freundin abnehmen möchte, hat Sie den Shake (Starterpaket) ausprobiert und konnte nach ein paar Tagen bereits Erfolge sehen. Ich persönlich hätte nicht gedacht, dass das so gut klappt.### Ablauf:Im Lieferumfang ist eine wirklich sehr gute und verständlich Anleitung enthalten.Kurzfassung: In den ersten Tagen ersetzt man alle Mahlzeiten durch drei Shakes, danach wird die Menge des Shakes reduziert und man kann gewisse Sachen wieder Essen.Da die Menge des benötigen Pulvers von der Körpergröße abhängt, reicht das Starterpaket für ca. 3-4 Tage. Für die Turbo-Diät benötigt man min. 27-30 Shakes. Wenn einem der Shake schmeckt und man weiß man steht die Diät durch, sollte man daher rechtzeitig direkt ein 3er oder 6er Paket nachbestellen.### Geschmack:Mir persönlich hat der Shake gar nicht geschmeckt, meiner Freundin schon. In Worte lässt sich Geschmack immer sehr schlecht fassen. Vergleichbar ist dieser mit „Yokebe Aktivkost Classic“.### Zubereitung:Die Zubereitung geht einfach und schnell. Einfach die empfohlene Menge Pulver in den im Lieferumfang enthaltenen Shaker + kaltes Wasser oder Milch hinzugeben und ordentlich shaken. Das Pulver löst sich sehr gut und recht schnell auf. Sofern man Wasser verwendet, kann man auch einen 1TL Pflanzenöl dazugeben. Hier sollte man ausprobieren was einem am besten geschmeckt. Meine Freundin hat meistens Milch verwendet.### Effekt:Meine Freundin konnte bereits nach den ersten Tagen ein paar Erfolge sehen. Dies liegt vermutlich stark dran, das 3 Shakes nur 630 kcal haben und Ihr Grundbedarf deutlich höher ist. Folge dessen hat Ihr Körper mehr verbraucht als er aufgenommen hat. Da der Shake stark sättigt, hatte Sie kein Hunger in diesen Tagen.Auf eine Angabe von kg verzichte ich hier bewusst, da die Gewichtsreduzierung stark vom Körperbau des Anwenders abhängt. Eine Person mit etwas mehr Körpermasse wird in den ersten Tagen deutlich mehr Gewicht verlieren, als eine Person die bereits ein Normalgewicht hat.### Fazit:Wem der Shake schmeckt und somit die ersten Tage durchhält, der wird auf Dauer sicher den gewünschten Effekt der Gewichtsreduzierung erreichen. Die Kosten ab dem 10. Tag, sind mit ca. EUR 1,70 (beim 3er Paket) für 1 Shake aus meiner Sicht ok.—Ich hoffe meine Rezension ist hilfreich und konnte zur Kaufentscheidung beitragen.Fragen können mir gerne via Kommentar gestellt werden. Mehr über mich in meinem Profil (Klick auf meinen Namen)\nRating: 5,0 von 5 Sternen\nValence: positiv (0.043)\nPolarity: positiv (1.099)\n\nID: p01_101\nDate: 2019-01-30\nTitle: +Schmeckt lecker/-Mogelpackung\nGamma: 0.746\nReview: Ein Shake der schmeckt, bei Bedarf pürriere ich etwas Obst hinein für noch mehr Geschmack, ist aber kein muss!Die Dose ist allerdings eine ziemlich Mogelpackung nur zu 2/3 gefüllt und da man 50g Pulver pro Drink brauch wird sie auch schnell wieder leer sein!\nRating: 4,0 von 5 Sternen\nValence: positiv (0.032)\nPolarity: positiv (0.511)\n\nID: p01_408\nDate: 2016-05-29\nTitle: Geschmack überzeugt\nGamma: 0.744\nReview: LINEAVI ist ein Mahlzeitersatz in Puderform, der mit Proteinen, Vitaminen, Minearalstoffen angereichert ist und versprich,, die Fettverbrannung anzuregen. Mit jeder empfohlenen Portion werden rund 200 Kcal aufgenommen, also in der Regel deutlich weniger, als bei einer gängigen Mahlzeit. Im Zusammenhang mit einer Diät können 1-2 Mahlzeiten problemlos mit einer Portion des Diätprodukts ersetzt werden, ohne Mangelerscheinungen hervor zu rufen.LNEAVI schmeckt etwas fruchtig und etwas nach Vanille. Es enthält Soja und eignet sich daher nicht für Menschen mit einer Sojaallergie.Wer vergleichbare Produkte wie z.B. Almased kennt, findet in diesem Produkt eine Alternative, die einem auch nach der ersten Dose noch schmeckt.Wunder bewirkt LINEAVI ebenso wenig wie die anderen Pulver, deren Zusammensetzung angeblich stets auf den neuesten medizinischen Erkenntnissen beruht. Dennoch ist es möglich mit Hilfe dieses Mahlzeitersatzes gesundes Abnehmen zu unterstützen, wenn man sich bewegt und all das beherzigt, was man bei jeder Diät beherzigen sollte.\nRating: 3,0 von 5 Sternen\nValence: positiv (0.045)\nPolarity: positiv (1.946)\n\n\n\n\nprint_output_descriptives(gamma_topics[5])\n\nResults for gamma_topic_5 \n## Check for ambiguous topic assignment for review:\nComments: \n- Highest relative gamma: Is gamma_topic_5 the top topic of the abstract?\n- Highest absolute gamma: Is the gamme value for gamma_topic_5 higher than 0.5\n \n                       Highest absolute gamma   No  Yes\nHighest realtive gamma                                 \nNo                                            1137    0\nYes                                            151  442\n\n## Basic descriptive statistics\n\n        var                  label   n NA.prc mean   sd   se   md\n  gamma_top Highest realtive gamma 593  65.72 0.59 0.14 0.01 0.61\n gamma_main Highest absolute gamma 442  74.45 0.66 0.09 0.00 0.66\n            range\n  0.6 (0.26-0.86)\n 0.37 (0.49-0.86)\n\n\n\nprint_output_abstracts(gamma_topics[5])\n\nID: p03_239\nDate: 2021-05-27\nTitle: Geld Verschwendung\nGamma: 0.857\nReview: Ich habe es als Ergänzung zur Umstellung der Ernährung um etwas abzunehmen. Leider gar keine Wirkung. Trotz gesunder Ernährung habe ich kein Gramm verloren selbst nach zwei Monaten. Richtig enttäuschend.\nRating: 1,0 von 5 Sternen\nValence: negativ (-0.052)\nPolarity: negativ (-0.251)\n\nID: p03_209\nDate: 2021-06-16\nTitle: Keine Änderung eingetreten. Ohne Wirkung.\nGamma: 0.854\nReview: Bei mir hat sich gar keine Wirkung eingestellt. Normal gegessen und tägliche Bewegung auf dem Fahrrad. Nichts abgenommen, nichts zugenommen ausser die täglichen Schwankungen. Ist auch kein Appetitzügler, Essen hat immer noch geschmeckt. Die Wünsche nach einer besseren Figur sind auch nicht eingetreten. Der Stoffwechsel hat nicht den Rest gemacht. Hier ist bei vielen wohl Einbildung im Spiel.\nRating: 3,0 von 5 Sternen\nValence: positiv (0.004)\nPolarity: positiv (1.099)\n\nID: p03_368\nDate: 2020-11-25\nTitle: KEINE WIRKUNG. ES IST NICHT ZU EMPFEHLEN. Voll enttäuscht.\nGamma: 0.849\nReview: Eigentlich war ich von den vielen positiven Rezensionen überzeugt.Rausgeworfenes Geld!Das Zeug hat keine Wirkung!Schauen sie Zweite Foto - steht hergestellt für TALLIN !Bei Allgemeine Produktinformationen steht:Netto-Gewicht 45 Grammund auf dem Verpackung Füllmenge: 28 Gramm geschrieben ….Die Frage ist - ob die Inhaltszusammenfassung stimmt ?Hab die Tabletten eingenommen und nach 2 Wochen ist nichts passiert.NUR rausgeworfenes Geld!Schade …\nRating: 1,0 von 5 Sternen\nValence: negativ (-0.105)\nPolarity: negativ (-0.511)\n\nID: p03_201\nDate: 2021-06-23\nTitle: Bringt nichts…Geldverschwendung\nGamma: 0.844\nReview: Ich habe das Produkt wegen der vielen positiven Bewertungen bestellt.Ich habe kein Übergewicht, ich wollte lediglich zum Sommer einige Pfunde loswerden. Mein Stoffwechsel sollte angeregt werden, da ich Hashimoto habe. Ich ernähre mich ausgewogen und überwiegend gesund.Meine Packung ist jetzt fast leer. Resonanz: keine Veränderung.Ich werde sie auch noch aufbrauchen, aber erwarte nichts großes.Es ist weder ein Fettburner noch ein Stoffwechsel anreger.Lediglich wird der Appetit ETWAS gezügelt/ doch eher verschoben.Von mir definitiv KEINE Kaufempfehlung.Für das geld habe ich definitiv mehr erwartet.\nRating: 1,0 von 5 Sternen\nValence: negativ (-0.006)\nPolarity: negativ (-0.847)\n\nID: p03_360\nDate: 2020-12-15\nTitle: Leider nix gebracht :-(\nGamma: 0.84\nReview: Habe nach zwei Wochen trotz Umstellen der Ernährung keinen Erfolg gehabt, Gewicht ist gleich geblieben. Schade.\nRating: 1,0 von 5 Sternen\nValence: negativ (-0.036)\nPolarity: negativ (-0.847)"
  },
  {
    "objectID": "content/08-text_as_data/08-slides.html",
    "href": "content/08-text_as_data/08-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you find the slides for the eigth session:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome."
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#seminarplan",
    "href": "content/08-text_as_data/slides/index.html#seminarplan",
    "title": "Text as data",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\nSitzung\nDatum\nThema\nReferent*Innen\n\n\n\n\n1\n26.10.2022\nKick-Off Session\nChristoph Adrian\n\n\n2\n02.11.2022\nDBD: Einführung und Überblick\nChristoph Adrian\n\n\n3\n09.11.2022\nDBD: Datenerhebung\nChristoph Adrian\n\n\n4\n16.11.2022\nAPI-Access (I): Twitter\nFalk\n\n\n5\n23.11.2022\nAPI-Access (II): YouTube\nDenisov\n\n\n6\n30.11.2022\nAPI-Access (II): Reddit\nLandauer\n\n\n7\n07.12.2022\nWebscraping: TikTok\nBrand & Kocher\n\n\n8\n14.12.2022\nText as data\nChristoph Adrian\n\n\n\n\nWEIHNACHTSPAUSE\n\n\n\n9\n11.01.2023\nESM: m-path\nDörr\n\n\n10\n18.01.2023\nData Donations\nHofmann & Wierzbicki\n\n\n11\n25.01.2023\nPUFFER\n\n\n\n12\n02.02.2023\nGuest Lecture: Linking DBD & Survey data\nJohannes Breuer\n\n\n13\n08.02.2023\nSemesterabschluss & Evaluation\nChristoph Adrian\n\n\n\n\n▶️\n⚠️Terminkonflikt"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#nicht-neu-aber-andere-dimension",
    "href": "content/08-text_as_data/slides/index.html#nicht-neu-aber-andere-dimension",
    "title": "Text as data",
    "section": "Nicht neu, aber andere Dimension",
    "text": "Nicht neu, aber andere Dimension\nDas Phänomen Text as data\n\n\n\nLange Tradition der Text- und Inhaltsanalyse (besonders in der Kommunikationswissenschaft)\nNeue Chancen & Herausforderungen durch explosionsartige Vergrößerung des (Text-)Datenaufkommen in den letzten Jahren (Websites, Plattformen & Digitalisierung)\n\n\n\n\n\n\n\n(Salganik, 2018)"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#neue-quellen-neue-methoden-neue-möglichkeiten",
    "href": "content/08-text_as_data/slides/index.html#neue-quellen-neue-methoden-neue-möglichkeiten",
    "title": "Text as data",
    "section": "Neue Quellen, Neue Methoden, neue Möglichkeiten",
    "text": "Neue Quellen, Neue Methoden, neue Möglichkeiten\nVerschiedene Textgrundlagen als Beispiel"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#possibilities-over-possibilities",
    "href": "content/08-text_as_data/slides/index.html#possibilities-over-possibilities",
    "title": "Text as data",
    "section": "Possibilities over possibilities",
    "text": "Possibilities over possibilities\nÜberblick über verschiedene Methoden der Textanalyse"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#im-fokus-diätpillen",
    "href": "content/08-text_as_data/slides/index.html#im-fokus-diätpillen",
    "title": "Text as data",
    "section": "Im Fokus: 💊 Diätpillen",
    "text": "Im Fokus: 💊 Diätpillen\nHintergrund\n\n\n\nReviews von fünf “Diätpillen”\nAutomatisch Scraping via eigener R-Funktion\nDatensatz mit knapp über 2000 Reviews (ohne Bereinigung)\nExemplarische Darstellung folgender Schritte:\n\nText-Processing\nSentiment-Analyse\nTopic Modeling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAusführliche Schritte im Google Colab & Showcase\nHier nur Auszüge"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#quick-scraping-step-by-step",
    "href": "content/08-text_as_data/slides/index.html#quick-scraping-step-by-step",
    "title": "Text as data",
    "section": "Quick scraping step-by-step",
    "text": "Quick scraping step-by-step\n1️⃣ Custom function für Export der html-Elemente\n\nscrape_amazon <- function(page_num, review_url) {\n  url_reviews <- paste0(review_url, \"&pageNumber=\", page_num, \"&sortBy=recent\")\n  doc <- read_html(url_reviews)\n  map_dfr(doc %>% html_elements(\"[id^='customer_review']\"), ~ data.frame(\n    review_title = .x %>% html_element(\".review-title\") %>% html_text2(),\n    review_text = .x %>% html_element(\".review-text-content\") %>% html_text2(),\n    review_star = .x %>% html_element(\".review-rating\") %>% html_text2(),\n    date = .x %>% html_element(\".review-date\") %>% html_text2() %>% gsub(\".*vom \", \"\", .),\n    author = .x %>% html_element(\".a-profile-name\") %>% html_text2(),\n    page = page_num\n  )) %>%\n    as_tibble %>%\n    return()\n}"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#quick-scraping-step-by-step-1",
    "href": "content/08-text_as_data/slides/index.html#quick-scraping-step-by-step-1",
    "title": "Text as data",
    "section": "Quick scraping step-by-step",
    "text": "Quick scraping step-by-step\n2️⃣ Definition von Amazon Review URLs\n\nurl <- list(\n  p01 = \"https://www.amazon.de/LINEAVI-Eiwei%C3%9F-Shake-Kombination-Molkeneiwei%C3%9F-laktosefrei/product-reviews/B018IB02AU/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n  p02 = \"https://www.amazon.de/Detoxkuren%E2%80%A2-Entw%C3%A4sserung-Entschlackung-Stoffwechsel-entschlacken/product-reviews/B072QW5ZN1/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n  p03 = \"https://www.amazon.de/Saint-Nutrition%C2%AE-KETO-BURN-Appetitz%C3%BCgler/product-reviews/B08B67V8G5/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n  p04 = \"https://www.amazon.de/Yokebe-vegetarisch-Mahlzeitersatz-Gewichtsabnahme-hochwertigen/product-reviews/B08GYZ8LRB/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\",\n  p05 = \"https://www.amazon.de/Vihado-Liquid-chlorophyll-drops-alfalfa/product-reviews/B093XNC8QH/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews\"\n)"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#quick-scraping-step-by-step-2",
    "href": "content/08-text_as_data/slides/index.html#quick-scraping-step-by-step-2",
    "title": "Text as data",
    "section": "Quick scraping step-by-step",
    "text": "Quick scraping step-by-step\n3️⃣ Scrape with for-loops\n\namazon <- list()\n# p01 \nfor (i in 1:79) {\n  df <- scrape_amazon(page_num = i, review_url = url$p01)\n  amazon$raw$p01[[i]] <- df\n}\n\n\n\nproduct <- names(url)\n# bind rows for each product\nfor (i in product) {\n  amazon$data$raw[[i]] <- amazon$raw[[i]] %>% \n    bind_rows() %>% \n    rownames_to_column(\"id\") %>% \n    mutate(across(id, as.numeric))\n}\n# bind rows of all products\namazon$data$full <- amazon$data$raw %>% \n  bind_rows(.id = \"src\")"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#quick-scraping-step-by-step-3",
    "href": "content/08-text_as_data/slides/index.html#quick-scraping-step-by-step-3",
    "title": "Text as data",
    "section": "Quick scraping step-by-step",
    "text": "Quick scraping step-by-step\n4️⃣ Endresultat\n\namazon$data$full %>% \n  glimpse()\n\nRows: 2,142\nColumns: 8\n$ src          <chr> \"p01\", \"p01\", \"p01\", \"p01\", \"p01\", \"p01\", \"p01\", \"p01\", \"…\n$ id           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ review_title <chr> \"Schmeckt wie ein angenehmer Milchshake\", \"Nicht zufriede…\n$ review_text  <chr> \"Wie ich damit abgenommen haben kann ich noch nicht sagen…\n$ review_star  <chr> \"4,0 von 5 Sternen\", \"2,0 von 5 Sternen\", \"5,0 von 5 Ster…\n$ date         <chr> \"Kundenrezension aus Deutschland 🇩🇪 am 26. November 2022\"…\n$ author       <chr> \"Rayan Wehbi\", \"Motte\", \"V.K\", \"niw\", \"Murphy\", \"Julia\", …\n$ page         <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, …"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#anzahl-der-reviews-nach-produkt",
    "href": "content/08-text_as_data/slides/index.html#anzahl-der-reviews-nach-produkt",
    "title": "Text as data",
    "section": "Anzahl der Reviews nach Produkt",
    "text": "Anzahl der Reviews nach Produkt\nKennenlernen des Datensatzes\n\namazon$data$de %>% \n  frq(src)\n\nsrc <categorical> \n# total N=1736 valid N=1736 mean=2.88 sd=1.46\n\nValue |   N | Raw % | Valid % | Cum. %\n--------------------------------------\np01   | 438 | 25.23 |   25.23 |  25.23\np02   | 256 | 14.75 |   14.75 |  39.98\np03   | 509 | 29.32 |   29.32 |  69.30\np04   | 143 |  8.24 |    8.24 |  77.53\np05   | 390 | 22.47 |   22.47 | 100.00\n<NA>  |   0 |  0.00 |    <NA> |   <NA>"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#anzahl-der-reviews-nach-produkt-im-zeitverlauf",
    "href": "content/08-text_as_data/slides/index.html#anzahl-der-reviews-nach-produkt-im-zeitverlauf",
    "title": "Text as data",
    "section": "Anzahl der Reviews nach Produkt im Zeitverlauf",
    "text": "Anzahl der Reviews nach Produkt im Zeitverlauf\nKennenlernen des Datensatzes\n\namazon$data$de %>% \n  ggplot(aes(year, fill = src)) +\n  geom_bar() +\n  scale_fill_locuszoom() +\n  theme_pubr()"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#bewertungen-der-produkte-absolute-zahlen",
    "href": "content/08-text_as_data/slides/index.html#bewertungen-der-produkte-absolute-zahlen",
    "title": "Text as data",
    "section": "Bewertungen der Produkte: Absolute Zahlen",
    "text": "Bewertungen der Produkte: Absolute Zahlen\nKennenlernen des Datensatzes\n\namazon$data$de %>% \n  mutate(across(rating, as.factor)) %>% \n  ggplot(aes(src, fill = rating)) + \n  geom_bar() +\n  scale_fill_brewer(palette = \"RdYlGn\") +\n  theme_pubr()"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#bewertungen-der-produkte-kummulierte-anteile",
    "href": "content/08-text_as_data/slides/index.html#bewertungen-der-produkte-kummulierte-anteile",
    "title": "Text as data",
    "section": "Bewertungen der Produkte: Kummulierte Anteile",
    "text": "Bewertungen der Produkte: Kummulierte Anteile\nKennenlernen des Datensatzes\n\namazon$data$de %>% \n  mutate(across(rating, as.factor)) %>% \n  ggplot(aes(src, fill = rating)) + \n  geom_bar(position = \"fill\") +\n  scale_fill_brewer(palette = \"RdYlGn\") +\n  theme_pubr()"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#vom-korpus-bis-zum-model",
    "href": "content/08-text_as_data/slides/index.html#vom-korpus-bis-zum-model",
    "title": "Text as data",
    "section": "Vom Korpus bis zum Model",
    "text": "Vom Korpus bis zum Model\nProzess der Textverarbeitung"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#sätze-token-lemma-pos",
    "href": "content/08-text_as_data/slides/index.html#sätze-token-lemma-pos",
    "title": "Text as data",
    "section": "Sätze – Token – Lemma – POS",
    "text": "Sätze – Token – Lemma – POS\nVorverarbeitungsschritte für Textanalyse\n\n\n\n\n\n\n1. Satzerkennung\n\n\nWas gibt’s in New York zu sehen?\n\n\n\n\n\n\n\n\n\n2. Tokenisierung\n\n\nwas; gibt; `s; in; new; york; zu; sehen; ?\n\n\n\n\n\n\n\n\n\n3. Lemmatisierung\n\n\nwas; geben; `s; in; new; york; zu; sehen; ?\n\n\n\n\n\n\n\n\n\n4. Part-Of-Speech (POS) Tagging\n\n\n>Was/PWS >gibt/VVFIN >’s/PPER >in/APPR >New/NE >York/NE >zu/PTKZU >sehen/VVINF\n\n\n\n\nSatzerkennung: Auflösung der Satzstruktur; Aber: Probleme mit Datumsangaben, Uhrzeit, Abkürzungen, URLS\nTokenisierung: Zerteilung in kleinste Einheiten, Abtrennung von Satzzeichen; Fragen: Umgang mit Zeichen, Symbolen, Zahlen, N-Gramme …\nDefinition Lemmatisierung: Grundform eines Worters, als diejenige Form, unter dem an einen Begriff in einem Nachschlagewerk findet / Rückführung auf die „Vollfrom”\nDefinition POS: Zuordnung von Wörtern und Satzzeichen eines Textes zu Wortarten"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#von-bow-zu-dfm",
    "href": "content/08-text_as_data/slides/index.html#von-bow-zu-dfm",
    "title": "Text as data",
    "section": "Von BOW zu DFM",
    "text": "Von BOW zu DFM\nBag-of-words (BOW) und Document-Feature-Matrix (DFM)\n\n\nBag-of-Words-Modell: es zählt lediglich die Worthäufigkeit je Dokument, die syntaktischen und grammatikalischen Zusammenhänge zwischen einzelnen Wörtern werden ignoriert."
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#the-good-the-bad-and-the-ugly",
    "href": "content/08-text_as_data/slides/index.html#the-good-the-bad-and-the-ugly",
    "title": "Text as data",
    "section": "The good, the bad and the ugly",
    "text": "The good, the bad and the ugly\nGrundidee & Ziele und der Sentimentanalyse\n\nAuf Basis von speziellen Wortlisten bzw. Lexika werden bestimmte Begriffe ausgezählt, denen zuvor entweder ein numerischer Wert (Score) oder eine Kategorien (positiv oder negativ) zugeordnet wurden.\nZiel ist die Bestimmung der Polarität (positive/negative Emotion) eines Textes\nAber: Wie gut ist die Klassifizierung?"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#grundlagen-für-die-analyse",
    "href": "content/08-text_as_data/slides/index.html#grundlagen-für-die-analyse",
    "title": "Text as data",
    "section": "Grundlagen für die Analyse",
    "text": "Grundlagen für die Analyse\nHintergrundinformationen zum Processing & Scoring\n\nDie Verarbeitung und Analyse der Daten erfolgt mit demquanteda bzw. quanteda.textstat Paket und basiert nur auf dem Korpus der deutschen Reviews\nAls Sentiment-Grundlage wird SentiWS genutzt\n\nöffentlich verfügbare deutschsprachige Ressource für die Sentiment Analyse, Opinion Mining und ähnliche Zwecke\npositive und negative Polarität im Intervall [-1; 1] für enthaltene Wörter die angegeben, sowie deren Wortart\nenthält ungefähr 1.650 positive und 1.800 negative Grundformen\n\n\n\n\nund (falls anwendbar) Flexionsvarianten\nso dass, inklusive der verschiedenen Flexionsformen, insgesamt etwa 16.000 positive und 18.000 negative Wortformen enthalten sind. SentiWS enthält nicht nur Adjektive und Adverbien, sondern auch Nomen und Verben die Träger von Sentiment sind"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#erste-ergebnisse",
    "href": "content/08-text_as_data/slides/index.html#erste-ergebnisse",
    "title": "Text as data",
    "section": "Erste Ergebnisse",
    "text": "Erste Ergebnisse\nBewertung der verschiedenen Diätpillen\n\namazon$data$de_senti %>% \n  ggplot(aes(x = src, y = valence)) + \n  ggdist::stat_halfeye(\n    aes(fill = src),\n    adjust = .5, \n    width = .6, \n    .width = 0, \n    justification = -.3, \n    point_colour = NA) + \n  geom_boxplot(\n    aes(color = src),\n    width = .25, \n    outlier.shape = NA\n  ) +\n  geom_point(\n    aes(color = src),\n    size = 1.3,\n    alpha = .3,\n    position = position_jitter(\n      seed = 1, width = .1\n    )\n  ) + \n  coord_cartesian(xlim = c(1.2, NA), clip = \"off\") +\n  scale_color_startrek() +\n  scale_fill_startrek() +\n  theme_pubr()\n\n\n\nValence = average valence of document, based on a dictionary whose values have numeric valence scores."
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#erste-ergebnisse-output",
    "href": "content/08-text_as_data/slides/index.html#erste-ergebnisse-output",
    "title": "Text as data",
    "section": "Erste Ergebnisse",
    "text": "Erste Ergebnisse"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#ein-großes-aber",
    "href": "content/08-text_as_data/slides/index.html#ein-großes-aber",
    "title": "Text as data",
    "section": "Ein großes “… , ABER …”",
    "text": "Ein großes “… , ABER …”\nKritische Anmerkungen zur Sentimentanalyse\n\n\n\namazon$data$de_senti %>% \n  flat_table(rating, val_fct)\n\n       val_fct negativ neutral positiv\nrating                                \n1                  167      40     101\n2                   61      11      37\n3                   47      14      48\n4                   34      10     129\n5                  109      16     912\n\n\n\n\nProbleme der “dictionary method” mit Negationen (“nicht gut”) und “Verstärkern” (“sehr gut”)\nBesser: Codierung bzw. Klassifizierung mit Hilfe von überwachtem maschinellem Lernen"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#grundidee",
    "href": "content/08-text_as_data/slides/index.html#grundidee",
    "title": "Text as data",
    "section": "Grundidee",
    "text": "Grundidee\nQuick Introduction to Topic Modeling\n\nVerfahren des unüberwachten maschinellen Lernens, das sich daher insbesondere zur Exploration und Deskription großer Textmengen eignet\nThemen werden strikt auf Basis von Worthäufigkeiten in den einzelnen Dokumenten vermeintlich objektiv berechnet, ganz ohne subjektive Einschätzungen und damit einhergehenden etwaigen Verzerrungen\nBekanntesten dieser Verfahren sind LDA (Latent Dirichlet Allocation) sowie die darauf aufbauenden CTM (Correlated Topic Models) und STM (Structural Topic Models)"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#annahmen",
    "href": "content/08-text_as_data/slides/index.html#annahmen",
    "title": "Text as data",
    "section": "Annahmen",
    "text": "Annahmen\nQuick Introduction to Topic Modeling\n\nEin Textkorpus besteht aus D Dokumenten und V Wörtern bzw. Terms\nEs wird nun angenommen, dass latente Themen K zu unterschiedlichen Anteilen in den Dokumenten D vorkommen und alle Wörter V mit unterschiedlicher Wahrscheinlichkeit zu den K Themen gehören.\nZiel der Verfahren ist die Berechnung zweier Matrizen D×K und V×K.\n\nDie erste Matrix D×K enthält für jedes einzelne Dokument d und jedes einzelne Thema k die Wahrscheinlichkeit, dass das Thema in diesem Dokument vorkommt (𝚹).\nAnalog enthält V×K für jedes einzelne Wort w und jedes einzelne Thema k die Wahrscheinlichkeit, dass das jeweilige Wort in diesem Thema vorkommt (𝛃).\n\n\n\n\nMit Hilfe dieser Matrizen können die Themen dann beschrieben und interpretiert werden.\n\nSo können aus V×K die wichtigsten Wörter je Thema (d.h., die Wörter mit der höchsten konditionalen Wahrscheinlichkeit, zu einem bestimmten Thema kk zu gehören) abgelesen werden;\nmittels D×K können Themen Dokumenten und umgekehrt zugeordnet werden, z. B. in dem für jedes Dokument dd das Thema kk mit der höchsten konditionalen Wahrscheinlichkeit identifiziert wird.\n\nZur Berechnung dieser Matrizen wird sozusagen der umgekehrte Weg gegangen und die Erzeugung der Dokumente als statistischer Prozess beschrieben: ein Dokument wird demnach erzeugt, in dem zufällig Themen aus der zum Dokument zugehörigen Themenverteilung und Wörter aus der den Themen zugehörigen Wortverteilungen gezogen werden. Hierzu wird das Topic Model zunächst mit zufälligen Themen- und Wortverteilungen initialisiert und dann in einem iterativen, algorithmischen Verfahren nach und nach adaptiert, bis es möglichst gut zu den Daten (dem Textkorpus) passt (d.h. die gemeinsame Likelihood der Themen- und Wortverteilungen maximiert wird).53"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#structual-topic-modeling-im-fokus",
    "href": "content/08-text_as_data/slides/index.html#structual-topic-modeling-im-fokus",
    "title": "Text as data",
    "section": "Structual Topic Modeling im Fokus",
    "text": "Structual Topic Modeling im Fokus\nIdee und Hintergrund von STM\n\n\n𝛴 = Topic covariance\nx = Prevalende Metadata\ngamma = Metadata weights\nsmaller box = N = number of words in a document\nbigger box = M = number of documents in a corpus"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#welches-k",
    "href": "content/08-text_as_data/slides/index.html#welches-k",
    "title": "Text as data",
    "section": "Welches K?",
    "text": "Welches K?\nBestimmung der optimalen Themenanzahl\n\namazon$tpm$model_scores %>% \n  transmute(\n    k,\n    `Lower bound` = lbound,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\")) %>%   \n  pivot_longer(-k, names_to = \"Metric\", values_to = \"Value\") %>% \n  ggplot(\n    aes(k, Value, color = Metric)) +\n  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +\n  facet_wrap(~Metric, scales = \"free_y\") +\n  labs(x = \"K (number of topics)\",\n       y = NULL,\n       title = \"Model diagnostics by number of topics\")"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#welches-k-output",
    "href": "content/08-text_as_data/slides/index.html#welches-k-output",
    "title": "Text as data",
    "section": "Welches K?",
    "text": "Welches K?"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#kombination-aus-kohärenz-exklusivität",
    "href": "content/08-text_as_data/slides/index.html#kombination-aus-kohärenz-exklusivität",
    "title": "Text as data",
    "section": "Kombination aus Kohärenz & Exklusivität",
    "text": "Kombination aus Kohärenz & Exklusivität\nBestimmung der optimalen Themenanzahl\n\namazon$tpm$model_scores %>%\n  select(k, exclusivity, semantic_coherence) %>%\n  filter(k %in% c(5, 7, 11)) %>%\n  unnest(cols = c(exclusivity, semantic_coherence)) %>%\n  mutate(k = as.factor(k)) %>%\n  ggplot(aes(semantic_coherence, exclusivity, color = k)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(x = \"Semantic coherence\",\n       y = \"Exclusivity\",\n       title = \"Comparing exclusivity and semantic coherence\",\n       subtitle = \"Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity\")"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#kombination-aus-kohärenz-exklusivität-output",
    "href": "content/08-text_as_data/slides/index.html#kombination-aus-kohärenz-exklusivität-output",
    "title": "Text as data",
    "section": "Kombination aus Kohärenz & Exklusivität",
    "text": "Kombination aus Kohärenz & Exklusivität"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#top-features-der-themen",
    "href": "content/08-text_as_data/slides/index.html#top-features-der-themen",
    "title": "Text as data",
    "section": "Top Features der Themen",
    "text": "Top Features der Themen\nInterpretation des Models\n\namazon$stm$mdl %>% labelTopics()\n\nTopic 1 Top Words:\n     Highest Prob: tropfen, geschmack, chlorophyll, wasser, wirkung, schmeckt, schon \n     FREX: tropfen, chlorophyll, glas, grün, farbe, flasche, konnten \n     Lift: alfalfa, dosieren, farbe, frisch, geladen, glas, grüne \n     Score: tropfen, chlorophyll, farbe, grün, geladen, medien, wirkung \nTopic 2 Top Words:\n     Highest Prob: produkt, super, nehme, seit, mehr, sagen, wirklich \n     FREX: nehme, gutes, begeistert, seit, wirkt, hilft, überrascht \n     Lift: begeistert, detox, entgiften, entschlacken, gutes, pfirsich, teste \n     Score: produkt, seit, nehme, weiteren, super, gutes, hilft \nTopic 3 Top Words:\n     Highest Prob: gut, dass, tag, tagen, ganz, finde, besser \n     FREX: ganz, finde, möchte, produkte, tagen, anfang, wenig \n     Lift: anfang, soweit, generell, produkte, komisch, versucht, möchte \n     Score: gut, generell, finde, dass, ganz, tag, tagen \nTopic 4 Top Words:\n     Highest Prob: schmeckt, geschmack, shake, lineavi, shaker, pulver, lecker \n     FREX: shake, lineavi, shaker, pulver, lecker, shakes, almased \n     Lift: anleitung, dosen, ersetzt, aktivkost, almased, classic, diätshake \n     Score: lineavi, shaker, shake, almased, pulver, shakes, milch \nTopic 5 Top Words:\n     Highest Prob: schon, wochen, leider, kapseln, abgenommen, sport, abnehmen \n     FREX: kapseln, abgenommen, tabletten, geld, bringt, sport, schlucken \n     Lift: abgenommen, achten, burn, gebracht, gegenteil, geld, geldverschwendung \n     Score: kapseln, tabletten, keto, burn, abgenommen, sport, kilo"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#prevalence-der-themen",
    "href": "content/08-text_as_data/slides/index.html#prevalence-der-themen",
    "title": "Text as data",
    "section": "Prevalence der Themen",
    "text": "Prevalence der Themen\nInterpretation des Models\n\namazon$stm$preval$tgt  %>% \n  ggplot(aes(topic, gamma, label = terms, fill = topic)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(hjust = 1.1, nudge_y = 0.0005, size = 3, color = \"white\") +\n  coord_flip() +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.35), labels = scales::percent) +\n  theme_bw() +\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major = element_blank()) +\n  labs(x = NULL, y = expression(gamma)) +\n  scale_fill_jama()"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#prevalence-der-themen-output",
    "href": "content/08-text_as_data/slides/index.html#prevalence-der-themen-output",
    "title": "Text as data",
    "section": "Prevalence der Themen",
    "text": "Prevalence der Themen"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#validieren-validieren-validieren",
    "href": "content/08-text_as_data/slides/index.html#validieren-validieren-validieren",
    "title": "Text as data",
    "section": "Validieren, Validieren, Validieren",
    "text": "Validieren, Validieren, Validieren\nKritisiche Anmerkungen zum Topic Modeling\n\nAutomated text analysis methods can substantially reduce the costs and time of analyzing massive collections of political texts. When applied to any one problem, however, the output of the models may be misleading or simply wrong. […] What should be avoided, then, is the blind use of any method without a validation step (Grimmer & Stewart, 2013, S. 5).\n\n\nKlassifikationsmodell klassifiziert alle Dokumente, ein Diktionär spuckt für jedes Dokument ein Ergebnis aus, ein Topic Model findet immer die vorgegebene Anzahl an Themen.\nOb es sich dabei auch um inhaltlich sinnvolle Ergebnisse handelt, kann und muss durch manuelle Validierungen festgestellt werden.\nModerne Verfahren (z.B. BERT) potentiell besser geeignet für bestimmte Texte"
  },
  {
    "objectID": "content/08-text_as_data/slides/index.html#literatur",
    "href": "content/08-text_as_data/slides/index.html#literatur",
    "title": "Text as data",
    "section": "Literatur",
    "text": "Literatur\n\n\nSalganik, M. J. (2018). Bit by bit: Social research in the digital age. Princeton University Press.\n\n\n\n\n\nDigital Behavioral Data"
  },
  {
    "objectID": "content/09-esm-m_path/09-slides.html",
    "href": "content/09-esm-m_path/09-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you find the slides for the ninth session:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome."
  },
  {
    "objectID": "content/09-esm-m_path/slides/index.html#seminarplan",
    "href": "content/09-esm-m_path/slides/index.html#seminarplan",
    "title": "Experience Sampling Method",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\nSitzung\nDatum\nThema\nReferent*Innen\n\n\n\n\n1\n26.10.2022\nKick-Off Session\nChristoph Adrian\n\n\n2\n02.11.2022\nDBD: Einführung und Überblick\nChristoph Adrian\n\n\n3\n09.11.2022\nDBD: Datenerhebung\nChristoph Adrian\n\n\n4\n16.11.2022\nAPI-Access (I): Twitter\nFalk\n\n\n5\n23.11.2022\nAPI-Access (II): YouTube\nDenisov\n\n\n6\n30.11.2022\nAPI-Access (II): Reddit\nLandauer\n\n\n7\n07.12.2022\nWebscraping: TikTok\nBrand & Kocher\n\n\n8\n14.12.2022\nText as data\nChristoph Adrian\n\n\n\n\nWEIHNACHTSPAUSE\n\n\n\n9\n11.01.2023\nESM: m-path\nDörr\n\n\n10\n18.01.2023\nData Donations\nHofmann & Wierzbicki\n\n\n11\n25.01.2023\nGuest Lecture: Linking DBD & Survey data\nJohannes Breuer\n\n\n12\n02.02.2023\nBring Your Own Research (Project)\nChristoph Adrian\n\n\n13\n08.02.2023\nSemesterabschluss & Evaluation\nChristoph Adrian\n\n\n\n\n▶️"
  },
  {
    "objectID": "content/09-esm-m_path/slides/index.html#vielen-dank-für-ihre-fragen",
    "href": "content/09-esm-m_path/slides/index.html#vielen-dank-für-ihre-fragen",
    "title": "Experience Sampling Method",
    "section": "Vielen Dank für Ihre Fragen!",
    "text": "Vielen Dank für Ihre Fragen!\n\n\nIch bin nicht sicher, ob ich es richtig verstanden habe, aber ESM ist ja gerade deswegen so besonders, weil es ermöglicht die Teilnehmer in verschiedensten Situationen zu erreichen. Wie wird das sichergestellt? Wenn man nämlich beispielsweise gerade in der U-Bahn ist und eine Frage auf dem Handy erhält (allerdings nicht antworten kann/möchte, weil man gleich aussteigen muss), könnte man sich die Frage ja einfach aufsparen und sie in einem ruhigen Moment Zuhause beantworten, wenn man etwas darüber nachgedacht hat. Das würde allerdings die Vorteile und den Sinn hinter ESM reduzieren, oder?\n\n\n\n\nWas ist mit der älteren Generation, die teilweise keine Handys besitzen oder regelmäßig nutzen wenn eine Untersuchung durch ESM gestützt werden soll, die Informationen älterer und jüngerer Generationen benötigt?\n\n\n\n\nIn dem Paper wird berichtet, dass die Methode gut für Kommunikationswissenschaftliche Forschung geeignet ist. Warum wird sie hier dann noch so selten eingesetzt?"
  },
  {
    "objectID": "content/09-esm-m_path/slides/index.html#lets-discuss",
    "href": "content/09-esm-m_path/slides/index.html#lets-discuss",
    "title": "Experience Sampling Method",
    "section": "Let’s discuss",
    "text": "Let’s discuss\n\n\nDurch die regelmäßige Befragung beim experience sampling kann sich das Verhalten der Probanden verändern/ anpassen (z.B. Befragung zum Medienkonsum > Proband stellt hohen Medienkonsum fest > beginnt Konsum zu reduzieren). In welchen Fällen wäre experience sampling und in welchen Fällen wären reguläre Befragungen besser geeignet? (Diskussionsfrage)\n\n\n\n\nWie kann bei der täglichen Strichprobe der ESM sichergestellt werden, dass sich durch die außergewöhnlich hohe Auseinandersetzung mit persönlichen Faktoren, wie bspw. Gefühlen, keine beeinflussten Ergebnisse entstehen.\n\n\nDie Reaktivität der Teilnehmer vor allem beim zeitbasierten experience sampling führt zur Verzerrung der Ergebnisse. Wie kann dieses Problem gelöst werden?\n\n\n\n\nLaut Paper erfordern ESM-Studien die Teilnahme über einen längeren Zeitraum (1-2 Wochen) und senken dadurch die Bereitschaft der Teilnahme an solchen Studien. Wie könnte dieses Problem umgangen werden?"
  },
  {
    "objectID": "content/09-esm-m_path/slides/index.html#neuer-termin-aktualisertes-thema",
    "href": "content/09-esm-m_path/slides/index.html#neuer-termin-aktualisertes-thema",
    "title": "Experience Sampling Method",
    "section": "Neuer Termin & “aktualisertes” Thema",
    "text": "Neuer Termin & “aktualisertes” Thema\nGastvortrag von Johannes Breuer zum Thema survey linking\n\n\nVerknüpfung von digitalen Spurdaten & Umfragen – Wieso, wie und was ist möglich?” (Digital via Zoom)\nZur Person:\n\nsenior researcher in the team Survey Data Augmentation at GESIS - Leibniz Institute for the Social Sciences (Department Survey Data Curation) in Cologne, Germany\n(co-)lead the team Research Data & Methods at the Center for Advanced Internet Studies (CAIS) in Bochum, Germany.\n\n\n\n\n\n\n\n\n\n\n\nAdd socials"
  },
  {
    "objectID": "content/09-esm-m_path/slides/index.html#bring-your-own-research-project",
    "href": "content/09-esm-m_path/slides/index.html#bring-your-own-research-project",
    "title": "Experience Sampling Method",
    "section": "Bring Your Own Research (Project)",
    "text": "Bring Your Own Research (Project)\nSitzung für offene Fragen & Anliegen\n\n\nGibt es Fragen oder den Wunsch nach weiterer Vertiefung zu\n\neinem Thema,\neiner Methode,\neiner Sitzung oder\neinem Showcase?\n\n\n\n\n\n\nHinweis auf Hausarbeit"
  },
  {
    "objectID": "content/09-esm-m_path/slides/index.html#intensive-longitudinal-design-ild-im-fokus",
    "href": "content/09-esm-m_path/slides/index.html#intensive-longitudinal-design-ild-im-fokus",
    "title": "Experience Sampling Method",
    "section": "Intensive Longitudinal Design (ILD) im Fokus",
    "text": "Intensive Longitudinal Design (ILD) im Fokus\nQuick reminder\n\n“an intensive longitudinal design involves sequential measurements on five or more occasions during which a change process is expected to unfold within each subject (e.g., person or other sampling)”\n\n(Bolger & Laurenceau, 2013)"
  },
  {
    "objectID": "content/09-esm-m_path/slides/index.html#messwiederholung-in-kurzen-abständen",
    "href": "content/09-esm-m_path/slides/index.html#messwiederholung-in-kurzen-abständen",
    "title": "Experience Sampling Method",
    "section": "Messwiederholung in kurzen Abständen",
    "text": "Messwiederholung in kurzen Abständen\nDefinitorischer Kern von ILD\n\n\nWie groß oder klein die zeitlichen Abstände Intervalle ) zwischen den Messungen sind , folgt heoretischen Annahmen über Phänomenfrequenz und Veränderungsprozesse"
  },
  {
    "objectID": "content/09-esm-m_path/slides/index.html#ebenen-über-ebenen",
    "href": "content/09-esm-m_path/slides/index.html#ebenen-über-ebenen",
    "title": "Experience Sampling Method",
    "section": "Ebenen über Ebenen",
    "text": "Ebenen über Ebenen\nILD: Sampling auf zwei geschalteten Ebenen"
  },
  {
    "objectID": "content/09-esm-m_path/slides/index.html#verschiedene-varianten-des-situationssamplings",
    "href": "content/09-esm-m_path/slides/index.html#verschiedene-varianten-des-situationssamplings",
    "title": "Experience Sampling Method",
    "section": "Verschiedene Varianten des Situationssamplings",
    "text": "Verschiedene Varianten des Situationssamplings\nSystematisierung nach (Masur, 2019)"
  },
  {
    "objectID": "content/09-esm-m_path/slides/index.html#personen--undoder-situationsebene",
    "href": "content/09-esm-m_path/slides/index.html#personen--undoder-situationsebene",
    "title": "Experience Sampling Method",
    "section": "Personen- und/oder Situationsebene?",
    "text": "Personen- und/oder Situationsebene?\nArten von Forschungsfragen & Beispiele\n\nFragen auf Personenebene (between-subject): Daten über Messzeitpunkte aggregiert\n\nPersonenmittelwert: Wie ist das durchschnittliche Wohlbefinden (Y) in der Personenstichprobe?\nVarianz der Personenmittelwerte: Welche Unterschiede im durchschnittlichen Wohlbefinden (Y) gibt es zwischen Personen?\nKorrelation auf Personenebene: Hängen Unterschiede im Wohlbefinden (Y) mit Unterschieden in der durchschnittlichen sozialen Interaktion (X) zusammen?\nKausalzusammenhang: Erklärt eine experimentelle Manipulation der sozialen Interaktion (X) die Unterschiede im Wohlbefinden (Y)?\n\nFragen auf Situationsebene (within-subject): Daten mehrerer Messzeitpunkte einer Person\n\nVarianz der Situationswerte: Wie stark weicht das situative Wohlbefinden (Y) vom Durchschnitt einer Person ab?\nKorrelation auf Situationsebene: Hängen diese Abweichungen im situativen Wohlbefinden (Y) mit situativen Unterschieden in der sozialen Interaktion (X) einer Person zusammen?\nGranger Kausalzusammenhang: Erklärt die soziale Interaktion einer Person in der Mitte des Tages (X) die Unterschiede im Wohlbefinden dieser Person am Ende des Tages (Y)?"
  },
  {
    "objectID": "content/09-esm-m_path/slides/index.html#design-your-own-research",
    "href": "content/09-esm-m_path/slides/index.html#design-your-own-research",
    "title": "Experience Sampling Method",
    "section": "Design your own research",
    "text": "Design your own research\nGruppenarbeit (ca 25 Min.) & Ergebnisvorstellung (ca. 15 Min)\n\nTeilen Sie sich in zwei bis drei Gruppen auf\nBearbeiten Sie folgende Aufgaben (ca. 30 Min).:\n\nFormulieren Sie drei unterschiedliche Forschungsfragen, die mit drei unterschiedlichen Kurzzeit-Längsschnittdesigns (siehe Systematik von Masur) beantwortbar sind\nDie Fragen können gerne einen thematischen Bezug zum Marketing haben\nMindestens eine Frage auf Personenebene & mindestens eine Frage auf Situationsebene\nHalten Sie Ihre Fragen & Designideen auf einer PP-Folie fest\n\nErgebnisvorstellung (ca. 20 Min): Präsentation der Fragen & dazugehörigem Design"
  },
  {
    "objectID": "content/09-esm-m_path/slides/index.html#literatur",
    "href": "content/09-esm-m_path/slides/index.html#literatur",
    "title": "Experience Sampling Method",
    "section": "Literatur",
    "text": "Literatur\n\n\nBolger, N., & Laurenceau, J.-P. (2013). Intensive longitudinal methods: An introduction to diary and experience sampling research. Guilford Press.\n\n\nMasur, P. K. (2019). Capturing situational dynamics: Strength and pitfalls of the experience sampling method (P. Müller, S. Geiß, T. K. Naab, & C. Peter, Eds.; Vol. 15). Herbert von Halem Verlag. https://doi.org/10.31219/osf.io/vx5ha\n\n\n\n\n\nDigital Behavioral Data"
  },
  {
    "objectID": "content/10-data_donations/10-slides.html",
    "href": "content/10-data_donations/10-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you find the slides for the tenth session:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome."
  },
  {
    "objectID": "content/10-data_donations/slides/index.html#seminarplan",
    "href": "content/10-data_donations/slides/index.html#seminarplan",
    "title": "Data Donations",
    "section": "Seminarplan",
    "text": "Seminarplan\n\n\n\nSitzung\nDatum\nThema\nReferent*Innen\n\n\n\n\n1\n26.10.2022\nKick-Off Session\nChristoph Adrian\n\n\n2\n02.11.2022\nDBD: Einführung und Überblick\nChristoph Adrian\n\n\n3\n09.11.2022\nDBD: Datenerhebung\nChristoph Adrian\n\n\n4\n16.11.2022\nAPI-Access (I): Twitter\nFalk\n\n\n5\n23.11.2022\nAPI-Access (II): YouTube\nDenisov\n\n\n6\n30.11.2022\nAPI-Access (II): Reddit\nLandauer\n\n\n7\n07.12.2022\nWebscraping: TikTok\nBrand & Kocher\n\n\n8\n14.12.2022\nText as data\nChristoph Adrian\n\n\n\n\nWEIHNACHTSPAUSE\n\n\n\n9\n11.01.2023\nESM: m-path\nDörr\n\n\n10\n18.01.2023\nData Donations\nHofmann & Wierzbicki\n\n\n11\n25.01.2023\nGuest Lecture: Linking DBD & Survey data\nJohannes Breuer\n\n\n12\n02.02.2023\nBring Your Own Research (Project)\nChristoph Adrian\n\n\n13\n08.02.2023\nSemesterabschluss & Evaluation\nChristoph Adrian\n\n\n\n\n▶️"
  },
  {
    "objectID": "content/10-data_donations/slides/index.html#vielen-dank-für-ihre-fragen",
    "href": "content/10-data_donations/slides/index.html#vielen-dank-für-ihre-fragen",
    "title": "Data Donations",
    "section": "Vielen Dank für Ihre Fragen!",
    "text": "Vielen Dank für Ihre Fragen!\n\n\nWas kann unter dem Screenomics Ansatz verstanden werden und wie findet er aktuell Anwendung in der Forschung/Praxis?\n\n\n\n\nBei Data Donations steht die aktive Zustimmung zum Teilen persönlicher Daten im Vordergrund, was die Forschung in diesem Zusammenhang stark erschwert. Wie relevant und häufig ist Forschung mit Data Donations demnach tatsächlich in der Praxis?\n\n\n\n\nIm Paper wird darüber gesprochen, dass bei den DDP’s der Prozess des Teiles vereinfacht werden sollte. Was wäre ein möglicher Weg, wie man dieses Problem lösen könnte?\n\n\n\n\nIn welcher Form könnten Data Donations mit anderen Datenerhebungsmethoden kombiniert werden, um noch bessere und aussagekräftigere Ergebnisse zu erlangen?"
  },
  {
    "objectID": "content/10-data_donations/slides/index.html#lets-discuss",
    "href": "content/10-data_donations/slides/index.html#lets-discuss",
    "title": "Data Donations",
    "section": "Let’s discuss",
    "text": "Let’s discuss\n\n\nData Donations findet zeitlich vor der Analyse der Daten und über einen längeren Zeitraum statt. Ebenfalls ändern sich die Trends und auch die Apps oft sehr schnell. Besteht hier die Gefahr, dass die Daten bis zur Analyse “veralten”?\n\n\n\n\nIst der Anonymisierungsprozess der Daten, hier auch als De-Identifizierung bezeichnet die “perfekte” Lösung zur Gewährleistung des Datenschutzes? Welche Lücken gibt es?\n\n\n\n\nDurch die große Menge an freigegebenen Daten (gesamte SM Aktivität) im Vergleich zu z.B. einer Umfrage, müsste der Bias durch die Selbstselektion der Teilnehmer doch sehr groß sein? Die Belohnung für die Data Donation in der Studie waren 5 Euro. Für welche Belohnung wärt ihr bereit eure gesamten Social-Media-Aktivitäten zur Verfügung zu stellen?"
  },
  {
    "objectID": "content/10-data_donations/slides/index.html#nicht-neu-sondern-anders",
    "href": "content/10-data_donations/slides/index.html#nicht-neu-sondern-anders",
    "title": "Data Donations",
    "section": "Nicht neu, sondern anders",
    "text": "Nicht neu, sondern anders\nData donation of digital traces\n\n\n\n\n\nData donation\n\n\n“the act of an individual actively consenting to donate their personal data for research” (Skatova & Goulding, 2019)\n\n\n\n\n\n\nDigital traces\n\n\n“One’s unique set of digital activities, actions, and communications that leave a data trace on the internet or on a computer or other digital device and can identify the particular user or device.” (Boeschoten, 2022)\n\n\n\n\n\n\n\n©️ https://myshadow.org/tracking-data-traces\n\n\n\n\n\n“Data donation” keine Neuheit –> Vorher Umfragen oder Dokument"
  },
  {
    "objectID": "content/10-data_donations/slides/index.html#verfügbarkeit-durch-veränderte-rahmenbedingungen",
    "href": "content/10-data_donations/slides/index.html#verfügbarkeit-durch-veränderte-rahmenbedingungen",
    "title": "Data Donations",
    "section": "Verfügbarkeit durch veränderte Rahmenbedingungen",
    "text": "Verfügbarkeit durch veränderte Rahmenbedingungen\nGeneral Data Protection Regulation (GDPR)\n\n\n\n\n\n\nArticel 15 - Right of data access\n\n\n\n\n\n\nRight to obtain from a data controller:\n\nConfirmation whether personal data are being processed\nAccess to the personal data\nAccess to information regarding data recipients\nand sources and data derived from your personal data\n\n\n\n\n\n\n\n\n\n\n\n\nArticel 20 - Right of data portability\n\n\n\n\n\n\nGrants data subjects the right to\n\nreceive the personal data in a structured, commonly used and machine-readable format (“Data Download Package”)\ntransmit those data to another data controller"
  },
  {
    "objectID": "content/10-data_donations/slides/index.html#design-your-own-research",
    "href": "content/10-data_donations/slides/index.html#design-your-own-research",
    "title": "Data Donations",
    "section": "Design your own research",
    "text": "Design your own research\nGruppenarbeit (ca 25 Min.) & Ergebnisvorstellung (ca. 15 Min)\n\nTeilen Sie sich in zwei Gruppen auf\nBearbeiten Sie folgende Aufgaben (ca. 30 Min).:\n\nFormulieren Sie eine Forschungsfragen, die Sie mit Hilfe von Datenspenden (DDPS etc.) beantworten möchten.\nBeschreiben Sie die Mechanismen, mit denen Sie eine hohe Compliance sicherstellen würden. Das können sowohl gezielte Maßnahmen (Hinweise bzw. “Aufklärung”) oder größere Konzepte (Data Donation Platform etc) sein.\nHalten Sie Ihre Fragen & Designideen auf einer PP-Folie fest\n\nErgebnisvorstellung (ca. 20 Min): Präsentation der Fragen & dazugehörigem Design\n\n\nData Donations von Instagram DDP’s gibt detaillierteren Einblick in die Social-Media-Nutzung mehrerer Individuen, ist aber hinsichtlich der Analyse sehr komplex. Welche weiteren Anwendungsfälle gibt es für Data Donations noch? In welchem Rahmen lassen sich Data Donations skalieren?\nIch denke dabei beispielsweise an Teilen von Daten einer Smartwatch oder Aufzeichnung von Daten über den Fahrstil für Versicherungen."
  },
  {
    "objectID": "content/10-data_donations/slides/index.html#google-slides-vorlagen",
    "href": "content/10-data_donations/slides/index.html#google-slides-vorlagen",
    "title": "Data Donations",
    "section": "Google Slides Vorlagen",
    "text": "Google Slides Vorlagen\nBitte nutzen Sie die Präsentationsvorlagen\n\n\n\n\n\n\n\n\nGruppe A\n\n\n\n\n\n\nGruppe B"
  },
  {
    "objectID": "content/10-data_donations/slides/index.html#literatur",
    "href": "content/10-data_donations/slides/index.html#literatur",
    "title": "Data Donations",
    "section": "Literatur",
    "text": "Literatur\n\n\nBoeschoten, L. (2022). Data donation: What is it and why is it important? https://hds.sites.uu.nl/2022/01/15/data-donation-day/\n\n\nSkatova, A., & Goulding, J. (2019). Psychology of personal data donation. PLOS ONE, 14(11), e0224240. https://doi.org/10.1371/journal.pone.0224240\n\n\n\n\n\nDigital Behavioral Data"
  },
  {
    "objectID": "content/11-guest_lecture/11-slides.html",
    "href": "content/11-guest_lecture/11-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you find the slides for the tenth session:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digital Behavioral Data",
    "section": "",
    "text": "In this seminar, students will be introduced to working with digital behavioral data (DBD). DBD refers to digital traces of human behavior that are knowingly or unknowingly left in online environments (e.g., social media, messengers, entertainment media, or digital collaboration tools). These rich data are increasingly available to social scientific research in the public interest but can also be used to derive strategic insights for business decisions. Students will learn how to work with DBD alongside the entire research process, from data collection, preprocessing and analysis, to reporting and provision (e.g., via open science tools). Students will first get a comprehensive overview of the ways in which DBD can be collected (e.g., API scraping, usage logging, mock-up virtual environments, or data donations), as well as the requirements for data protection, research ethics, and data quality. Afterwards, students will practice and apply their newly gained knowledge in small projects on use cases from media and communication research. In doing so, they learn about key computational methods via which large digital behavioral datasets (e.g., texts, images, usage behavior logs) can be processed and analyzed. By completing this module, participants will get an up-to-date overview and practical insights into how to harness the potential of observational data traces to better understand media users’ behavior in digital environments."
  },
  {
    "objectID": "index.html#students-will",
    "href": "index.html#students-will",
    "title": "Digital Behavioral Data",
    "section": "Students will",
    "text": "Students will\n\noverview and understand central opportunities of DBD and accompanying challenges for data collection and preprocessing\nevaluate the strengths and weaknesses of different ways of collecting DBD\nget to know and understand central requirements for data protection, research ethics, and data quality\nget to know and overview key computational social science methods to analyze DBD\npractice and apply knowledge on DBD, statistics, and data analysis in small projects of their own"
  },
  {
    "objectID": "index.html#recommended-prerequisites",
    "href": "index.html#recommended-prerequisites",
    "title": "Digital Behavioral Data",
    "section": "Recommended prerequisites",
    "text": "Recommended prerequisites\n\nInterest in social scientific perspectives on media, communication, and digital technologies.\nBasic knowledge of working with statistical software such as Stata, R, Python, or SPSS is required.\nStudents are recommended, but not required, to also visit the lecture Data Science: Foundations, Tools, Applications in Socio-Economics and Marketing."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Digital Behavioral Data",
    "section": "Schedule",
    "text": "Schedule\nBelow is the scheudle for the current semester.\n\n\n\n\n\n\n\n\nSession\nDatum\nTopic\n\n\n\n\n1\n26.10.2022\nKick-Off Session\n\n\n2\n02.11.2022\nDBD: Introduction & Overview\n\n\n3\n09.11.2022\nDBD: Data collection process in focus\n\n\n4\n16.11.2022\nAPI-Access (I): Twitter\n\n\n5\n23.11.2022\nAPI-Access (II): YouTube\n\n\n6\n30.11.2022\nAPI-Access (III): Reddit\n\n\n7\n07.12.2022\nWebscraping: TikTok\n\n\n8\n14.12.2022\nExtra: Text as data\n\n\n-\n-\nCHRISTMAS BREAK\n\n\n9\n11.01.2023\nESM: m-path\n\n\n10\n18.01.2023\nData Donations\n\n\n11\n25.01.2023\nGuest Lecture: Linking DBD & Survey data\n\n\n12\n02.02.2023\nBring Your Own Research (Project)\n\n\n13\n08.02.2023\nClosing Session: Recap, Evaluation & Discussion"
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Digital Behavioral Data",
    "section": "Copyright",
    "text": "Copyright\nThis content is licensed under a GPL-3.0 License."
  }
]